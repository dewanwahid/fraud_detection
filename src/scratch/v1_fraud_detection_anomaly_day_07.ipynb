{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fraud Accounts with Anomaly Detection: Day 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Library import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "# rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "LABELS = [\"Normal\", \"Fraud\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loding data\n",
    "df_day_7 = pd.read_csv(\"/Users/dwahid/Documents/GitHub/fraud_detection/data_clusters/gmm_day_7_k6.tsv\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day_7 = df_day_7.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema = {\n",
    "#     'integer': 'systemid',\n",
    "#     'numeric(20,2)':[col for col in df_day_7.columns if col not in ['systemid', 'Unnamed: 0']]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = df_day_7.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.columns = [\n",
    "#     col.replace('-', '_').replace('/', '_')\n",
    "#     for col in df_test.columns\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_base = [\n",
    "#     'CREATE TABLE data_science.dewan_demo (',\n",
    "#     '    systemid integer,',\n",
    "# ]\n",
    "\n",
    "# list_numeric = [\n",
    "#     '    ' + str(col) +  ' numeric(20, 3),'\n",
    "#     for col in df_test.columns\n",
    "#     if col not in ['systemid', 'Unnamed: 0']\n",
    "# ]\n",
    "\n",
    "# list_base.extend(list_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_query = '\\n'.join(list_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(create_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.to_csv('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['systemid',\n",
       " 'activateestimate',\n",
       " 'activateexpense',\n",
       " 'activateotherincome',\n",
       " 'activatepayment',\n",
       " 'activateproject',\n",
       " 'activaterecurringprofile',\n",
       " 'activatestaff',\n",
       " 'adminde-activation',\n",
       " 'adminonlinepaymentattempt',\n",
       " 'adminpayinvoiceonline-invoice',\n",
       " 'adminpayinvoiceonline-listview',\n",
       " 'archiveclient',\n",
       " 'archiveexpense',\n",
       " 'archiveotherincome',\n",
       " 'archiveproject',\n",
       " 'archivetask',\n",
       " 'autobillpayment',\n",
       " 'banktransferdisabled',\n",
       " 'banktransferenabled',\n",
       " 'bulkimportclientscomplete',\n",
       " 'bulkimportitemsandservicescomplete',\n",
       " 'clientimportcsvsucceeded',\n",
       " 'clientlimitupgradenudge',\n",
       " 'createbankaccount',\n",
       " 'createbanktransaction',\n",
       " 'createbanktransfer',\n",
       " 'createcategory',\n",
       " 'createcontact',\n",
       " 'createcontractor',\n",
       " 'createcreditnote',\n",
       " 'createdexpense',\n",
       " 'createestimate',\n",
       " 'createexpense',\n",
       " 'createitem',\n",
       " 'createotherincome',\n",
       " 'createreceipt',\n",
       " 'createservice',\n",
       " 'creditcardclientaccessgranted',\n",
       " 'creditcardsystemaccessrevoked',\n",
       " 'customemailsignature',\n",
       " 'declinedonlinepaymentnotification',\n",
       " 'deletebusinessaccountant',\n",
       " 'deletebusinesspartner',\n",
       " 'deletecollaborator',\n",
       " 'deletecreditnote',\n",
       " 'deleteestimate',\n",
       " 'deleteexpense',\n",
       " 'deletehours',\n",
       " 'deleteinvoice',\n",
       " 'deleteitem',\n",
       " 'deleteotherincome',\n",
       " 'deleteproject',\n",
       " 'deleterecurringexpense',\n",
       " 'deleteretainerprofile',\n",
       " 'deletestaff',\n",
       " 'deletesystemgateway',\n",
       " 'deletetimeentry',\n",
       " 'deleteuser',\n",
       " 'disconnectbankaccount',\n",
       " 'disconnectpaymentgateway',\n",
       " 'dualstripe/paypalgatewayexperienceinvoicepopupclick',\n",
       " 'emailcreditnote',\n",
       " 'emailestimate',\n",
       " 'emailestimatetoself',\n",
       " 'emailinvoice',\n",
       " 'emailinvoicesample',\n",
       " 'emailreport',\n",
       " 'emailsent',\n",
       " 'enableautobilling',\n",
       " 'enablepaymentgateway',\n",
       " 'expenseimportsucceeded',\n",
       " 'exportjournalentries',\n",
       " 'fbpayenabled',\n",
       " 'fbpayfirstinvoicesent',\n",
       " 'fbpayupdatedacceptedcreditcards',\n",
       " 'fbpayuserconnectedbank',\n",
       " 'generateinvoicefromrecurringprofile',\n",
       " 'hitpaywall-clientlimit',\n",
       " 'hourslogged',\n",
       " 'identitycompletedonboarding',\n",
       " 'login',\n",
       " 'stripepaymentsuccessful',\n",
       " 'successfulonlinepayment',\n",
       " 'surveyquestionanswered',\n",
       " 'updatebusiness',\n",
       " 'updatecategory',\n",
       " 'updateclient',\n",
       " 'updatecompanyprofile',\n",
       " 'updatecontractor',\n",
       " 'updatecreditnote',\n",
       " 'updateestimate',\n",
       " 'updateexpense',\n",
       " 'updateinvoicesample',\n",
       " 'updateitem',\n",
       " 'updateservice',\n",
       " 'is_sales_managed',\n",
       " 'is_freshbooks_account_active',\n",
       " 'is_paying',\n",
       " 'avg_wc_description_day_7',\n",
       " 'avg_wc_notes_day_7',\n",
       " 'avg_wc_terms_day_7',\n",
       " 'avg_wc_address_day_7',\n",
       " 'invoice_count_day_7',\n",
       " 'client_count_day_7',\n",
       " 'cluster_id_k6']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_day_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking null in data\n",
    "df_day_7.isnull().values.any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Filtering Only Import Important Features ###########################################\n",
    "\n",
    "# New Day 7: Importing importing features list\n",
    "important_features = pd.read_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_imp_features_names/model_anoml_important_features_day_7.tsv\", sep=\"\\n,\")\n",
    "\n",
    "# New Day 7: Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])\n",
    "\n",
    "# Filtering only important features \n",
    "df_day_7_imp = df_day_7[df_day_7.columns.intersection(imp_features_list)]\n",
    "\n",
    "# Drop not so important feature columns\n",
    "df_day_7_imp_noid = df_day_7_imp.drop(columns=['systemid',\n",
    "                                 'activateestimate',\n",
    "                                 'activateproject',\n",
    "                                 'activaterecurringprofile',\n",
    "                                 'activatestaff',\n",
    "                                 'banktransferdisabled',\n",
    "                                 'banktransferenabled',\n",
    "                                 'bulkimportitemsandservicescomplete',\n",
    "                                 'creditcardsystemaccessrevoked',\n",
    "                                 'deletetimeentry',\n",
    "                                 'deleterecurringexpense',\n",
    "                                 'deleteretainerprofile',\n",
    "                                 'deletebusinessaccountant',\n",
    "                                 'deletesystemgateway',\n",
    "                                 'exportjournalentries',\n",
    "                                 'generateinvoicefromrecurringprofile',\n",
    "                                 'fbpayuserconnectedbank'\n",
    "                                 \n",
    "                                ], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day_7_imp_noid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarization with standard deviation:  (x-mean)/(std)\n",
    "scaler_anml_day_7 = StandardScaler()\n",
    "scaler_anml_day_7.fit(df_day_7_c1_imp_noid)\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "\n",
    "df_res = pd.DataFrame(scaler_anml_day_7.transform(df_day_7_imp_noid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data into train and test set\n",
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_train = X_train.drop(['cluster_id_k6'], axis=1)\n",
    "\n",
    "y_test = X_test['cluster_id_k6']\n",
    "X_test = X_test.drop(['cluster_id_k6'], axis=1)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model\n",
    "Our Autoencoder uses 4 fully connected layers with 14, 7, 7 and 29 neurons respectively. The first two layers are used for our encoder, the last two go for the decoder. Additionally, L1 regularization will be used during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", \n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model for 100 epochs with a batch size of 32 samples and save the best performing model to a file. The ModelCheckpoint provided by Keras is really handy for such tasks. Additionally, the training progress will be exported in a format that TensorBoard understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 32\n",
    "\n",
    "autoencoder.compile(optimizer='adam', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"model.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test, X_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard]).history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = load_model('model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstruction error on our training and test data seems to converge nicely. Is it low enough? Let's have a closer look at the error distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse,\n",
    "                        'true_class': y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
