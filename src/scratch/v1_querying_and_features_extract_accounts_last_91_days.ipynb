{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Querying & Featuers Extraction\n",
    "\n",
    "## Accounts Singup within Last 91 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ DATABASE CONNECTOR ##############\n",
    "\n",
    "from contextlib import closing\n",
    "\n",
    "import psycopg2\n",
    "import simplejson\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "%load_ext sql\n",
    "%config SqlMagic.displaylimit = 5\n",
    "\n",
    "# Database details\n",
    "DEFAULT_DB = 'data_depot'\n",
    "DEFAULT_HOST = 'freshbooks-data.c8exzn6geij3.us-east-1.redshift.amazonaws.com'\n",
    "DEFAULT_PORT = 5439\n",
    "\n",
    "# Database connector (Ramzy's)\n",
    "from contextlib import closing\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "import simplejson\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "DEFAULT_DB = 'data_depot'\n",
    "DEFAULT_HOST = 'freshbooks-data.c8exzn6geij3.us-east-1.redshift.amazonaws.com'\n",
    "DEFAULT_PORT = 5439\n",
    "\n",
    "\n",
    "class PsycopgConnector:\n",
    "    '''\n",
    "    A database connector that uses Psycopg to connect to Redshift.\n",
    "\n",
    "    How to play:\n",
    "\n",
    "        psy_conn = PsycopgConnector(username, password)\n",
    "        df = psy_conn.run_query(sql=sql, return_data=True)\n",
    "\n",
    "    NOTE: This class commits queries to redshift if return_data=False.\n",
    "    This means INSERT, DROP, TRUNCATE, etc. all work against the DB.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        username=None,\n",
    "        password=None,\n",
    "        db=DEFAULT_DB,\n",
    "        host=DEFAULT_HOST,\n",
    "        port=DEFAULT_PORT,\n",
    "    ):\n",
    "\n",
    "        self.db = DEFAULT_DB\n",
    "        self.host = DEFAULT_HOST\n",
    "        self.port = DEFAULT_PORT\n",
    "\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "\n",
    "    def _get_connection(self):\n",
    "\n",
    "        self.conn = psycopg2.connect(\n",
    "            dbname=self.db,\n",
    "            user=self.username,\n",
    "            password=self.password,\n",
    "            host=self.host,\n",
    "            port=self.port\n",
    "        )\n",
    "\n",
    "        return self.conn\n",
    "\n",
    "    def run_query(self, sql, return_data=False):\n",
    "\n",
    "        with closing(self._get_connection()) as conn:\n",
    "            with conn, conn.cursor() as cur:\n",
    "                if return_data:\n",
    "                    return pd.read_sql(sql=sql, con=conn)\n",
    "                else:\n",
    "                    cur.execute(sql)\n",
    "                    \n",
    "\n",
    "# Read the Redshift's credentials file \n",
    "with open(\"redshift_creds.json.nogit\") as fh:\n",
    "    creds = simplejson.loads(fh.read())\n",
    "    \n",
    "username = creds.get(\"user_name\")\n",
    "password = creds.get(\"password\")\n",
    "\n",
    "\n",
    "# Database connector method\n",
    "pig = PsycopgConnector(username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function: Word Count in a string\n",
    "import re\n",
    "def words_count (strg):\n",
    "    \n",
    "    #print(strg)\n",
    "    \n",
    "    if strg == '' or pd.isnull(strg):\n",
    "        no_of_words = 0\n",
    "        #print('NaN')\n",
    "    else:\n",
    "        strg_words_list = re.findall(r\"[\\w']+\", strg)\n",
    "        no_of_words = len(strg_words_list)\n",
    "\n",
    "        \n",
    "        #print(strg_words_list)\n",
    "    \n",
    "    return no_of_words \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Invoice Data & Extract Avg Word Counts Features \n",
    "\n",
    "## 1.01 Invoice Within 7 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Invoice Data Within 7 Days After Signup_Date ####################\n",
    "\n",
    "# SQL for impoorting all invoices created within 7 days after signup_date\n",
    "sql_invoices_7days_all_accounts = '''WITH invoices_in_a_period AS (\n",
    "    SELECT \n",
    "            systemid, \n",
    "            signup_date, \n",
    "            is_freshbooks_account_active,\n",
    "            is_sales_managed,\n",
    "            admin_email, \n",
    "            signup_ip_address\n",
    "    FROM report_systems rs\n",
    "    WHERE signup_date BETWEEN '2019-08-01' and '2019-10-31'\n",
    "), invoice_created_at AS (\n",
    "    SELECT\n",
    "           pic.systemid,\n",
    "           pic.admin_email, \n",
    "           pic.signup_ip_address,\n",
    "           pic.is_freshbooks_account_active,\n",
    "           pic.is_sales_managed,\n",
    "           pic.signup_date,\n",
    "           inv.invoiceid,\n",
    "           inv.create_date,\n",
    "           inv.created_at,\n",
    "           inv.description,\n",
    "           inv.notes,\n",
    "           inv.terms,\n",
    "           inv.address,\n",
    "           DATEDIFF(days, pic.signup_date, inv.created_at) AS days_to_invoice_creation\n",
    "    FROM invoices_in_a_period AS pic\n",
    "    LEFT JOIN coalesced_live_shards.invoice_stable as inv USING (systemid)\n",
    "    WHERE ((days_to_invoice_creation BETWEEN 0 AND 7) OR days_to_invoice_creation IS NULL)\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM invoice_created_at;'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "# df_invoices_7days_all_accounts = pd.read_sql_query(sql_invoices_7days_all_accounts, connect_to_db)\n",
    "df_invoices_7days_all_accounts = pig.run_query(sql_invoices_7days_all_accounts, return_data=True)\n",
    "\n",
    "\n",
    "# Words count in invoice's description, notes, terms, address\n",
    "df_invoices_7days_all_accounts['avg_wc_description_day_7'] = df_invoices_7days_all_accounts.apply(lambda x: words_count(x['description']), axis=1)\n",
    "df_invoices_7days_all_accounts['avg_wc_notes_day_7'] = df_invoices_7days_all_accounts.apply(lambda x: words_count(x['notes']), axis=1)\n",
    "df_invoices_7days_all_accounts['avg_wc_terms_day_7'] = df_invoices_7days_all_accounts.apply(lambda x: words_count(x['terms']), axis=1)\n",
    "df_invoices_7days_all_accounts['avg_wc_address_day_7'] = df_invoices_7days_all_accounts.apply(lambda x: words_count(x['address']), axis=1)\n",
    "\n",
    "                                                                                                                   \n",
    "# Filters the text columns from the dataframe\n",
    "df_invoices_7days_all_accounts_fil = df_invoices_7days_all_accounts.filter(['systemid', 'invoiceid', \n",
    "                                                                            'signup_date', \n",
    "                                                                            'create_date', \n",
    "                                                                            'created_at',\n",
    "                                                                            'days_to_invoice_creation', \n",
    "                                                                            'avg_wc_description_day_7', \n",
    "                                                                            'avg_wc_notes_day_7', \n",
    "                                                                            'avg_wc_terms_day_7',\n",
    "                                                                            'avg_wc_address_day_7'])  \n",
    "                                                                                                                   \n",
    "# Summing (grouping) all invoices for a 'systemid'\n",
    "df_word_count_7days_all_accounts_total = df_invoices_7days_all_accounts_fil.groupby('systemid').mean()  \n",
    "\n",
    "# Final word count table\n",
    "df_word_count_7days_all_accounts_final = df_word_count_7days_all_accounts_total.filter(['systemid',\n",
    "                                                                            'avg_wc_description_day_7', \n",
    "                                                                            'avg_wc_notes_day_7', \n",
    "                                                                            'avg_wc_terms_day_7',\n",
    "                                                                            'avg_wc_address_day_7'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.02 Invoice Within Day 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Invoice Data Within 14 Days After Signup_Date ####################\n",
    "\n",
    "# SQL for impoorting all invoices created within 14 days after signup_date\n",
    "sql_invoices_14days_all_accounts = '''WITH invoices_in_a_period AS (\n",
    "    SELECT systemid, signup_date\n",
    "    FROM report_systems rs\n",
    "    WHERE signup_date BETWEEN '2019-08-01' and '2019-10-31'\n",
    "), invoice_created_at AS (\n",
    "    SELECT\n",
    "           pic.systemid,\n",
    "           inv.invoiceid,\n",
    "           pic.signup_date,\n",
    "           inv.create_date,\n",
    "           inv.created_at,\n",
    "           inv.description,\n",
    "           inv.notes,\n",
    "           inv.terms,\n",
    "           inv.address,\n",
    "           DATEDIFF(days, pic.signup_date, inv.created_at) AS days_to_invoice_creation\n",
    "    FROM invoices_in_a_period AS pic\n",
    "    LEFT JOIN coalesced_live_shards.invoice_stable as inv USING (systemid)\n",
    "    WHERE ((days_to_invoice_creation BETWEEN 0 AND 14) OR days_to_invoice_creation IS NULL)\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM invoice_created_at;'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "# df_invoices_14days_all_accounts = pd.read_sql_query(sql_invoices_14days_all_accounts, connect_to_db)\n",
    "df_invoices_14days_all_accounts = pig.run_query(sql_invoices_14days_all_accounts, return_data=True)\n",
    "\n",
    "\n",
    "# Words count in invoice's description, notes, terms, address\n",
    "df_invoices_14days_all_accounts['avg_wc_description_day_14'] = df_invoices_14days_all_accounts.apply(lambda x: words_count(x['description']), axis=1)\n",
    "df_invoices_14days_all_accounts['avg_wc_notes_day_14'] = df_invoices_14days_all_accounts.apply(lambda x: words_count(x['notes']), axis=1)\n",
    "df_invoices_14days_all_accounts['avg_wc_terms_day_14'] = df_invoices_14days_all_accounts.apply(lambda x: words_count(x['terms']), axis=1)\n",
    "df_invoices_14days_all_accounts['avg_wc_address_day_14'] = df_invoices_14days_all_accounts.apply(lambda x: words_count(x['address']), axis=1)\n",
    "\n",
    "                                                                                                                   \n",
    "# Filters the text columns from the dataframe\n",
    "df_invoices_14days_all_accounts_fil = df_invoices_14days_all_accounts.filter(['systemid', 'invoiceid', \n",
    "                                                                            'signup_date', \n",
    "                                                                            'create_date', \n",
    "                                                                            'created_at',\n",
    "                                                                            'days_to_invoice_creation', \n",
    "                                                                            'avg_wc_description_day_14', \n",
    "                                                                            'avg_wc_notes_day_14', \n",
    "                                                                            'avg_wc_terms_day_14',\n",
    "                                                                            'avg_wc_address_day_14'])  \n",
    "                                                                                                                   \n",
    "# Summing (grouping) all invoices for a 'systemid'\n",
    "df_word_count_14days_all_accounts_total = df_invoices_14days_all_accounts_fil.groupby('systemid').mean()  \n",
    "\n",
    "# Final word count table\n",
    "df_word_count_14days_all_accounts_final = df_word_count_14days_all_accounts_total.filter(['systemid',\n",
    "                                                                            'avg_wc_description_day_14', \n",
    "                                                                            'avg_wc_notes_day_14', \n",
    "                                                                            'avg_wc_terms_day_14',\n",
    "                                                                            'avg_wc_address_day_14'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.03 Invoice Within 21 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Invoice Data Within 21 Days After Signup_Date ####################\n",
    "\n",
    "# SQL for impoorting all invoices created within 21 days after signup_date\n",
    "sql_invoices_21days_all_accounts = '''WITH invoices_in_a_period AS (\n",
    "    SELECT systemid, signup_date\n",
    "    FROM report_systems rs\n",
    "    WHERE signup_date BETWEEN '2019-08-01' and '2019-10-31'\n",
    "), invoice_created_at AS (\n",
    "    SELECT\n",
    "           pic.systemid,\n",
    "           inv.invoiceid,\n",
    "           pic.signup_date,\n",
    "           inv.create_date,\n",
    "           inv.created_at,\n",
    "           inv.description,\n",
    "           inv.notes,\n",
    "           inv.terms,\n",
    "           inv.address,\n",
    "           DATEDIFF(days, pic.signup_date, inv.created_at) AS days_to_invoice_creation\n",
    "    FROM invoices_in_a_period AS pic\n",
    "    LEFT JOIN coalesced_live_shards.invoice_stable as inv USING (systemid)\n",
    "    WHERE ((days_to_invoice_creation BETWEEN 0 AND 21) OR days_to_invoice_creation IS NULL)\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM invoice_created_at;'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_invoices_21days_all_accounts = pig.run_query(sql_invoices_21days_all_accounts, return_data=True)\n",
    "\n",
    "\n",
    "# Words count in invoice's description, notes, terms, address\n",
    "df_invoices_21days_all_accounts['avg_wc_description_day_21'] = df_invoices_21days_all_accounts.apply(lambda x: words_count(x['description']), axis=1)\n",
    "df_invoices_21days_all_accounts['avg_wc_notes_day_21'] = df_invoices_21days_all_accounts.apply(lambda x: words_count(x['notes']), axis=1)\n",
    "df_invoices_21days_all_accounts['avg_wc_terms_day_21'] = df_invoices_21days_all_accounts.apply(lambda x: words_count(x['terms']), axis=1)\n",
    "df_invoices_21days_all_accounts['avg_wc_address_day_21'] = df_invoices_21days_all_accounts.apply(lambda x: words_count(x['address']), axis=1)\n",
    "\n",
    "                                                                                                                   \n",
    "# Filters the text columns from the dataframe\n",
    "df_invoices_21days_all_accounts_fil = df_invoices_21days_all_accounts.filter(['systemid', 'invoiceid', \n",
    "                                                                            'signup_date', \n",
    "                                                                            'create_date', \n",
    "                                                                            'created_at',\n",
    "                                                                            'days_to_invoice_creation', \n",
    "                                                                            'avg_wc_description_day_21', \n",
    "                                                                            'avg_wc_notes_day_21', \n",
    "                                                                            'avg_wc_terms_day_21',\n",
    "                                                                            'avg_wc_address_day_21'])  \n",
    "                                                                                                                   \n",
    "# Summing (grouping) all invoices for a 'systemid'\n",
    "df_word_count_21days_all_accounts_total = df_invoices_21days_all_accounts_fil.groupby('systemid').mean()  \n",
    "\n",
    "# Final word count table\n",
    "df_word_count_21days_all_accounts_final = df_word_count_21days_all_accounts_total.filter(['systemid',\n",
    "                                                                            'avg_wc_description_day_21', \n",
    "                                                                            'avg_wc_notes_day_21', \n",
    "                                                                            'avg_wc_terms_day_21',\n",
    "                                                                            'avg_wc_address_day_21'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.04 Invoice Within 28 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Invoice Data Within 28 Days After Signup_Date ####################\n",
    "\n",
    "# SQL for impoorting all invoices created within 28 days after signup_date\n",
    "sql_invoices_28days_all_accounts = '''WITH invoices_in_a_period AS (\n",
    "    SELECT systemid, signup_date\n",
    "    FROM report_systems rs\n",
    "    WHERE signup_date BETWEEN '2019-08-01' and '2019-10-31'\n",
    "), invoice_created_at AS (\n",
    "    SELECT\n",
    "           pic.systemid,\n",
    "           inv.invoiceid,\n",
    "           pic.signup_date,\n",
    "           inv.create_date,\n",
    "           inv.created_at,\n",
    "           inv.description,\n",
    "           inv.notes,\n",
    "           inv.terms,\n",
    "           inv.address,\n",
    "           DATEDIFF(days, pic.signup_date, inv.created_at) AS days_to_invoice_creation\n",
    "    FROM invoices_in_a_period AS pic\n",
    "    LEFT JOIN coalesced_live_shards.invoice_stable as inv USING (systemid)\n",
    "    WHERE ((days_to_invoice_creation BETWEEN 0 AND 28) OR days_to_invoice_creation IS NULL)\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM invoice_created_at;'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_invoices_28days_all_accounts = pig.run_query(sql_invoices_28days_all_accounts, return_data=True)\n",
    "\n",
    "# Words count in invoice's description, notes, terms, address\n",
    "df_invoices_28days_all_accounts['avg_wc_description_day_28'] = df_invoices_28days_all_accounts.apply(lambda x: words_count(x['description']), axis=1)\n",
    "df_invoices_28days_all_accounts['avg_wc_notes_day_28'] = df_invoices_28days_all_accounts.apply(lambda x: words_count(x['notes']), axis=1)\n",
    "df_invoices_28days_all_accounts['avg_wc_terms_day_28'] = df_invoices_28days_all_accounts.apply(lambda x: words_count(x['terms']), axis=1)\n",
    "df_invoices_28days_all_accounts['avg_wc_address_day_28'] = df_invoices_28days_all_accounts.apply(lambda x: words_count(x['address']), axis=1)\n",
    "\n",
    "                                                                                                                   \n",
    "# Filters the text columns from the dataframe\n",
    "df_invoices_28days_all_accounts_fil = df_invoices_28days_all_accounts.filter(['systemid', 'invoiceid', \n",
    "                                                                            'signup_date', \n",
    "                                                                            'create_date', \n",
    "                                                                            'created_at',\n",
    "                                                                            'days_to_invoice_creation', \n",
    "                                                                            'avg_wc_description_day_28', \n",
    "                                                                            'avg_wc_notes_day_28', \n",
    "                                                                            'avg_wc_terms_day_28',\n",
    "                                                                            'avg_wc_address_day_28'])  \n",
    "                                                                                                                   \n",
    "# Summing (grouping) all invoices for a 'systemid'\n",
    "df_word_count_28days_all_accounts_total = df_invoices_28days_all_accounts_fil.groupby('systemid').mean()  \n",
    "\n",
    "# Final word count table\n",
    "df_word_count_28days_all_accounts_final = df_word_count_28days_all_accounts_total.filter(['systemid',\n",
    "                                                                            'avg_wc_description_day_28', \n",
    "                                                                            'avg_wc_notes_day_28', \n",
    "                                                                            'avg_wc_terms_day_28',\n",
    "                                                                            'avg_wc_address_day_28'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.05 Invoice Within 35 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Invoice Data Within 35 Days After Signup_Date ####################\n",
    "\n",
    "# SQL for impoorting all invoices created within 35 days after signup_date\n",
    "sql_invoices_35days_all_accounts = '''WITH invoices_in_a_period AS (\n",
    "    SELECT systemid, signup_date\n",
    "    FROM report_systems rs\n",
    "    WHERE signup_date BETWEEN '2019-08-01' and '2019-10-31'\n",
    "), invoice_created_at AS (\n",
    "    SELECT\n",
    "           pic.systemid,\n",
    "           inv.invoiceid,\n",
    "           pic.signup_date,\n",
    "           inv.create_date,\n",
    "           inv.created_at,\n",
    "           inv.description,\n",
    "           inv.notes,\n",
    "           inv.terms,\n",
    "           inv.address,\n",
    "           DATEDIFF(days, pic.signup_date, inv.created_at) AS days_to_invoice_creation\n",
    "    FROM invoices_in_a_period AS pic\n",
    "    LEFT JOIN coalesced_live_shards.invoice_stable as inv USING (systemid)\n",
    "    WHERE ((days_to_invoice_creation BETWEEN 0 AND 35) OR days_to_invoice_creation IS NULL)\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM invoice_created_at;'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_invoices_35days_all_accounts = pig.run_query(sql_invoices_35days_all_accounts, return_data=True)\n",
    "\n",
    "# Words count in invoice's description, notes, terms, address\n",
    "df_invoices_35days_all_accounts['avg_wc_description_day_35'] = df_invoices_35days_all_accounts.apply(lambda x: words_count(x['description']), axis=1)\n",
    "df_invoices_35days_all_accounts['avg_wc_notes_day_35'] = df_invoices_35days_all_accounts.apply(lambda x: words_count(x['notes']), axis=1)\n",
    "df_invoices_35days_all_accounts['avg_wc_terms_day_35'] = df_invoices_35days_all_accounts.apply(lambda x: words_count(x['terms']), axis=1)\n",
    "df_invoices_35days_all_accounts['avg_wc_address_day_35'] = df_invoices_35days_all_accounts.apply(lambda x: words_count(x['address']), axis=1)\n",
    "\n",
    "                                                                                                                   \n",
    "# Filters the text columns from the dataframe\n",
    "df_invoices_35days_all_accounts_fil = df_invoices_35days_all_accounts.filter(['systemid', 'invoiceid', \n",
    "                                                                            'signup_date', \n",
    "                                                                            'create_date', \n",
    "                                                                            'created_at',\n",
    "                                                                            'days_to_invoice_creation', \n",
    "                                                                            'avg_wc_description_day_35', \n",
    "                                                                            'avg_wc_notes_day_35', \n",
    "                                                                            'avg_wc_terms_day_35',\n",
    "                                                                            'avg_wc_address_day_35'])  \n",
    "                                                                                                                   \n",
    "# Summing (grouping) all invoices for a 'systemid'\n",
    "df_word_count_35days_all_accounts_total = df_invoices_35days_all_accounts_fil.groupby('systemid').mean()  \n",
    "\n",
    "# Final word count table\n",
    "df_word_count_35days_all_accounts_final = df_word_count_35days_all_accounts_total.filter(['systemid',\n",
    "                                                                            'avg_wc_description_day_35', \n",
    "                                                                            'avg_wc_notes_day_35', \n",
    "                                                                            'avg_wc_terms_day_35',\n",
    "                                                                            'avg_wc_address_day_35'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.06 Invoice Within 42 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Invoice Data Within 42 Days After Signup_Date ####################\n",
    "\n",
    "# SQL for impoorting all invoices created within 42 days after signup_date\n",
    "sql_invoices_42days_all_accounts = '''WITH invoices_in_a_period AS (\n",
    "    SELECT systemid, signup_date\n",
    "    FROM report_systems rs\n",
    "    WHERE signup_date BETWEEN '2019-08-01' and '2019-10-31'\n",
    "), invoice_created_at AS (\n",
    "    SELECT\n",
    "           pic.systemid,\n",
    "           inv.invoiceid,\n",
    "           pic.signup_date,\n",
    "           inv.create_date,\n",
    "           inv.created_at,\n",
    "           inv.description,\n",
    "           inv.notes,\n",
    "           inv.terms,\n",
    "           inv.address,\n",
    "           DATEDIFF(days, pic.signup_date, inv.created_at) AS days_to_invoice_creation\n",
    "    FROM invoices_in_a_period AS pic\n",
    "    LEFT JOIN coalesced_live_shards.invoice_stable as inv USING (systemid)\n",
    "    WHERE ((days_to_invoice_creation BETWEEN 0 AND 42) OR days_to_invoice_creation IS NULL)\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM invoice_created_at;'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_invoices_42days_all_accounts = pig.run_query(sql_invoices_42days_all_accounts, return_data=True)\n",
    "\n",
    "# Words count in invoice's description, notes, terms, address\n",
    "df_invoices_42days_all_accounts['avg_wc_description_day_42'] = df_invoices_42days_all_accounts.apply(lambda x: words_count(x['description']), axis=1)\n",
    "df_invoices_42days_all_accounts['avg_wc_notes_day_42'] = df_invoices_42days_all_accounts.apply(lambda x: words_count(x['notes']), axis=1)\n",
    "df_invoices_42days_all_accounts['avg_wc_terms_day_42'] = df_invoices_42days_all_accounts.apply(lambda x: words_count(x['terms']), axis=1)\n",
    "df_invoices_42days_all_accounts['avg_wc_address_day_42'] = df_invoices_42days_all_accounts.apply(lambda x: words_count(x['address']), axis=1)\n",
    "\n",
    "                                                                                                                   \n",
    "# Filters the text columns from the dataframe\n",
    "df_invoices_42days_all_accounts_fil = df_invoices_42days_all_accounts.filter(['systemid', 'invoiceid', \n",
    "                                                                            'signup_date', \n",
    "                                                                            'create_date', \n",
    "                                                                            'created_at',\n",
    "                                                                            'days_to_invoice_creation', \n",
    "                                                                            'avg_wc_description_day_42', \n",
    "                                                                            'avg_wc_notes_day_42', \n",
    "                                                                            'avg_wc_terms_day_42',\n",
    "                                                                            'avg_wc_address_day_42'])  \n",
    "                                                                                                                   \n",
    "# Summing (grouping) all invoices for a 'systemid'\n",
    "df_word_count_42days_all_accounts_total = df_invoices_42days_all_accounts_fil.groupby('systemid').mean()  \n",
    "\n",
    "# Final word count table\n",
    "df_word_count_42days_all_accounts_final = df_word_count_42days_all_accounts_total.filter(['systemid',\n",
    "                                                                            'avg_wc_description_day_42', \n",
    "                                                                            'avg_wc_notes_day_42', \n",
    "                                                                            'avg_wc_terms_day_42',\n",
    "                                                                            'avg_wc_address_day_42'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.07 Invoice Within 49 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Invoice Data Within 49 Days After Signup_Date ####################\n",
    "\n",
    "# SQL for impoorting all invoices created within 49 days after signup_date\n",
    "sql_invoices_49days_all_accounts = '''WITH invoices_in_a_period AS (\n",
    "    SELECT systemid, signup_date\n",
    "    FROM report_systems rs\n",
    "    WHERE signup_date BETWEEN '2019-08-01' and '2019-10-31'\n",
    "), invoice_created_at AS (\n",
    "    SELECT\n",
    "           pic.systemid,\n",
    "           inv.invoiceid,\n",
    "           pic.signup_date,\n",
    "           inv.create_date,\n",
    "           inv.created_at,\n",
    "           inv.description,\n",
    "           inv.notes,\n",
    "           inv.terms,\n",
    "           inv.address,\n",
    "           DATEDIFF(days, pic.signup_date, inv.created_at) AS days_to_invoice_creation\n",
    "    FROM invoices_in_a_period AS pic\n",
    "    LEFT JOIN coalesced_live_shards.invoice_stable as inv USING (systemid)\n",
    "    WHERE ((days_to_invoice_creation BETWEEN 0 AND 49) OR days_to_invoice_creation IS NULL)\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM invoice_created_at;'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_invoices_49days_all_accounts = pig.run_query(sql_invoices_49days_all_accounts, return_data=True)\n",
    "\n",
    "# Words count in invoice's description, notes, terms, address\n",
    "df_invoices_49days_all_accounts['avg_wc_description_day_49'] = df_invoices_49days_all_accounts.apply(lambda x: words_count(x['description']), axis=1)\n",
    "df_invoices_49days_all_accounts['avg_wc_notes_day_49'] = df_invoices_49days_all_accounts.apply(lambda x: words_count(x['notes']), axis=1)\n",
    "df_invoices_49days_all_accounts['avg_wc_terms_day_49'] = df_invoices_49days_all_accounts.apply(lambda x: words_count(x['terms']), axis=1)\n",
    "df_invoices_49days_all_accounts['avg_wc_address_day_49'] = df_invoices_49days_all_accounts.apply(lambda x: words_count(x['address']), axis=1)\n",
    "\n",
    "                                                                                                                   \n",
    "# Filters the text columns from the dataframe\n",
    "df_invoices_49days_all_accounts_fil = df_invoices_49days_all_accounts.filter(['systemid', 'invoiceid', \n",
    "                                                                            'signup_date', \n",
    "                                                                            'create_date', \n",
    "                                                                            'created_at',\n",
    "                                                                            'days_to_invoice_creation', \n",
    "                                                                            'avg_wc_description_day_49', \n",
    "                                                                            'avg_wc_notes_day_49', \n",
    "                                                                            'avg_wc_terms_day_49',\n",
    "                                                                            'avg_wc_address_day_49'])  \n",
    "                                                                                                                   \n",
    "# Summing (grouping) all invoices for a 'systemid'\n",
    "df_word_count_49days_all_accounts_total = df_invoices_49days_all_accounts_fil.groupby('systemid').mean()  \n",
    "\n",
    "# Final word count table\n",
    "df_word_count_49days_all_accounts_final = df_word_count_49days_all_accounts_total.filter(['systemid',\n",
    "                                                                            'avg_wc_description_day_49', \n",
    "                                                                            'avg_wc_notes_day_49', \n",
    "                                                                            'avg_wc_terms_day_49',\n",
    "                                                                            'avg_wc_address_day_49'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.08 Invoice Within 56 Days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Invoice Data Within 56 Days After Signup_Date ####################\n",
    "\n",
    "# SQL for impoorting all invoices created within 56 days after signup_date\n",
    "sql_invoices_56days_all_accounts = '''WITH invoices_in_a_period AS (\n",
    "    SELECT systemid, signup_date\n",
    "    FROM report_systems rs\n",
    "    WHERE signup_date BETWEEN '2019-08-01' and '2019-10-31'\n",
    "), invoice_created_at AS (\n",
    "    SELECT\n",
    "           pic.systemid,\n",
    "           inv.invoiceid,\n",
    "           pic.signup_date,\n",
    "           inv.create_date,\n",
    "           inv.created_at,\n",
    "           inv.description,\n",
    "           inv.notes,\n",
    "           inv.terms,\n",
    "           inv.address,\n",
    "           DATEDIFF(days, pic.signup_date, inv.created_at) AS days_to_invoice_creation\n",
    "    FROM invoices_in_a_period AS pic\n",
    "    LEFT JOIN coalesced_live_shards.invoice_stable as inv USING (systemid)\n",
    "    WHERE ((days_to_invoice_creation BETWEEN 0 AND 56) OR days_to_invoice_creation IS NULL)\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM invoice_created_at;'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_invoices_56days_all_accounts = pig.run_query(sql_invoices_56days_all_accounts, return_data=True)\n",
    "\n",
    "# Words count in invoice's description, notes, terms, address\n",
    "df_invoices_56days_all_accounts['avg_wc_description_day_56'] = df_invoices_56days_all_accounts.apply(lambda x: words_count(x['description']), axis=1)\n",
    "df_invoices_56days_all_accounts['avg_wc_notes_day_56'] = df_invoices_56days_all_accounts.apply(lambda x: words_count(x['notes']), axis=1)\n",
    "df_invoices_56days_all_accounts['avg_wc_terms_day_56'] = df_invoices_56days_all_accounts.apply(lambda x: words_count(x['terms']), axis=1)\n",
    "df_invoices_56days_all_accounts['avg_wc_address_day_56'] = df_invoices_56days_all_accounts.apply(lambda x: words_count(x['address']), axis=1)\n",
    "\n",
    "                                                                                                                   \n",
    "# Filters the text columns from the dataframe\n",
    "df_invoices_56days_all_accounts_fil = df_invoices_56days_all_accounts.filter(['systemid', 'invoiceid', \n",
    "                                                                            'signup_date', \n",
    "                                                                            'create_date', \n",
    "                                                                            'created_at',\n",
    "                                                                            'days_to_invoice_creation', \n",
    "                                                                            'avg_wc_description_day_56', \n",
    "                                                                            'avg_wc_notes_day_56', \n",
    "                                                                            'avg_wc_terms_day_56',\n",
    "                                                                            'avg_wc_address_day_56'])  \n",
    "                                                                                                                   \n",
    "# Summing (grouping) all invoices for a 'systemid'\n",
    "df_word_count_56days_all_accounts_total = df_invoices_56days_all_accounts_fil.groupby('systemid').mean()  \n",
    "\n",
    "# Final word count table\n",
    "df_word_count_56days_all_accounts_final = df_word_count_56days_all_accounts_total.filter(['systemid',\n",
    "                                                                            'avg_wc_description_day_56', \n",
    "                                                                            'avg_wc_notes_day_56', \n",
    "                                                                            'avg_wc_terms_day_56',\n",
    "                                                                            'avg_wc_address_day_56'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.09 Invoice Within 63 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Invoice Data Within 63 Days After Signup_Date ####################\n",
    "\n",
    "# SQL for impoorting all invoices created within 63 days after signup_date\n",
    "sql_invoices_63days_all_accounts = '''WITH invoices_in_a_period AS (\n",
    "    SELECT systemid, signup_date\n",
    "    FROM report_systems rs\n",
    "    WHERE signup_date BETWEEN '2019-08-01' and '2019-10-31'\n",
    "), invoice_created_at AS (\n",
    "    SELECT\n",
    "           pic.systemid,\n",
    "           inv.invoiceid,\n",
    "           pic.signup_date,\n",
    "           inv.create_date,\n",
    "           inv.created_at,\n",
    "           inv.description,\n",
    "           inv.notes,\n",
    "           inv.terms,\n",
    "           inv.address,\n",
    "           DATEDIFF(days, pic.signup_date, inv.created_at) AS days_to_invoice_creation\n",
    "    FROM invoices_in_a_period AS pic\n",
    "    LEFT JOIN coalesced_live_shards.invoice_stable as inv USING (systemid)\n",
    "    WHERE ((days_to_invoice_creation BETWEEN 0 AND 63) OR days_to_invoice_creation IS NULL)\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM invoice_created_at;'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_invoices_63days_all_accounts = pig.run_query(sql_invoices_63days_all_accounts, return_data=True)\n",
    "\n",
    "# Words count in invoice's description, notes, terms, address\n",
    "df_invoices_63days_all_accounts['avg_wc_description_day_63'] = df_invoices_63days_all_accounts.apply(lambda x: words_count(x['description']), axis=1)\n",
    "df_invoices_63days_all_accounts['avg_wc_notes_day_63'] = df_invoices_63days_all_accounts.apply(lambda x: words_count(x['notes']), axis=1)\n",
    "df_invoices_63days_all_accounts['avg_wc_terms_day_63'] = df_invoices_63days_all_accounts.apply(lambda x: words_count(x['terms']), axis=1)\n",
    "df_invoices_63days_all_accounts['avg_wc_address_day_63'] = df_invoices_63days_all_accounts.apply(lambda x: words_count(x['address']), axis=1)\n",
    "\n",
    "                                                                                                                   \n",
    "# Filters the text columns from the dataframe\n",
    "df_invoices_63days_all_accounts_fil = df_invoices_63days_all_accounts.filter(['systemid', 'invoiceid', \n",
    "                                                                            'signup_date', \n",
    "                                                                            'create_date', \n",
    "                                                                            'created_at',\n",
    "                                                                            'days_to_invoice_creation', \n",
    "                                                                            'avg_wc_description_day_63', \n",
    "                                                                            'avg_wc_notes_day_63', \n",
    "                                                                            'avg_wc_terms_day_63',\n",
    "                                                                            'avg_wc_address_day_63'])  \n",
    "                                                                                                                   \n",
    "# Summing (grouping) all invoices for a 'systemid'\n",
    "df_word_count_63days_all_accounts_total = df_invoices_63days_all_accounts_fil.groupby('systemid').mean()  \n",
    "\n",
    "# Final word count table\n",
    "df_word_count_63days_all_accounts_final = df_word_count_63days_all_accounts_total.filter(['systemid',\n",
    "                                                                            'avg_wc_description_day_63', \n",
    "                                                                            'avg_wc_notes_day_63', \n",
    "                                                                            'avg_wc_terms_day_63',\n",
    "                                                                            'avg_wc_address_day_63'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 Invoice Within 70 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Invoice Data Within 70 Days After Signup_Date ####################\n",
    "\n",
    "# SQL for impoorting all invoices created within 70 days after signup_date\n",
    "sql_invoices_70days_all_accounts = '''WITH invoices_in_a_period AS (\n",
    "    SELECT systemid, signup_date\n",
    "    FROM report_systems rs\n",
    "    WHERE signup_date BETWEEN '2019-08-01' and '2019-10-31'\n",
    "), invoice_created_at AS (\n",
    "    SELECT\n",
    "           pic.systemid,\n",
    "           inv.invoiceid,\n",
    "           pic.signup_date,\n",
    "           inv.create_date,\n",
    "           inv.created_at,\n",
    "           inv.description,\n",
    "           inv.notes,\n",
    "           inv.terms,\n",
    "           inv.address,\n",
    "           DATEDIFF(days, pic.signup_date, inv.created_at) AS days_to_invoice_creation\n",
    "    FROM invoices_in_a_period AS pic\n",
    "    LEFT JOIN coalesced_live_shards.invoice_stable as inv USING (systemid)\n",
    "    WHERE ((days_to_invoice_creation BETWEEN 0 AND 70) OR days_to_invoice_creation IS NULL)\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM invoice_created_at;'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_invoices_70days_all_accounts = pig.run_query(sql_invoices_70days_all_accounts, return_data=True)\n",
    "\n",
    "# Words count in invoice's description, notes, terms, address\n",
    "df_invoices_70days_all_accounts['avg_wc_description_day_70'] = df_invoices_70days_all_accounts.apply(lambda x: words_count(x['description']), axis=1)\n",
    "df_invoices_70days_all_accounts['avg_wc_notes_day_70'] = df_invoices_70days_all_accounts.apply(lambda x: words_count(x['notes']), axis=1)\n",
    "df_invoices_70days_all_accounts['avg_wc_terms_day_70'] = df_invoices_70days_all_accounts.apply(lambda x: words_count(x['terms']), axis=1)\n",
    "df_invoices_70days_all_accounts['avg_wc_address_day_70'] = df_invoices_70days_all_accounts.apply(lambda x: words_count(x['address']), axis=1)\n",
    "\n",
    "                                                                                                                   \n",
    "# Filters the text columns from the dataframe\n",
    "df_invoices_70days_all_accounts_fil = df_invoices_70days_all_accounts.filter(['systemid', 'invoiceid', \n",
    "                                                                            'signup_date', \n",
    "                                                                            'create_date', \n",
    "                                                                            'created_at',\n",
    "                                                                            'days_to_invoice_creation', \n",
    "                                                                            'avg_wc_description_day_70', \n",
    "                                                                            'avg_wc_notes_day_70', \n",
    "                                                                            'avg_wc_terms_day_70',\n",
    "                                                                            'avg_wc_address_day_70'])  \n",
    "                                                                                                                   \n",
    "# Summing (grouping) all invoices for a 'systemid'\n",
    "df_word_count_70days_all_accounts_total = df_invoices_70days_all_accounts_fil.groupby('systemid').mean()  \n",
    "\n",
    "# Final word count table\n",
    "df_word_count_70days_all_accounts_final = df_word_count_70days_all_accounts_total.filter(['systemid',\n",
    "                                                                            'avg_wc_description_day_70', \n",
    "                                                                            'avg_wc_notes_day_70', \n",
    "                                                                            'avg_wc_terms_day_70',\n",
    "                                                                            'avg_wc_address_day_70'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Invoice Within 77 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Invoice Data Within 70 Days After Signup_Date ####################\n",
    "\n",
    "# SQL for impoorting all invoices created within 77 days after signup_date\n",
    "sql_invoices_77days_all_accounts = '''WITH invoices_in_a_period AS (\n",
    "    SELECT systemid, signup_date\n",
    "    FROM report_systems rs\n",
    "    WHERE signup_date BETWEEN '2019-08-01' and '2019-10-31'\n",
    "), invoice_created_at AS (\n",
    "    SELECT\n",
    "           pic.systemid,\n",
    "           inv.invoiceid,\n",
    "           pic.signup_date,\n",
    "           inv.create_date,\n",
    "           inv.created_at,\n",
    "           inv.description,\n",
    "           inv.notes,\n",
    "           inv.terms,\n",
    "           inv.address,\n",
    "           DATEDIFF(days, pic.signup_date, inv.created_at) AS days_to_invoice_creation\n",
    "    FROM invoices_in_a_period AS pic\n",
    "    LEFT JOIN coalesced_live_shards.invoice_stable as inv USING (systemid)\n",
    "    WHERE ((days_to_invoice_creation BETWEEN 0 AND 77) OR days_to_invoice_creation IS NULL)\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM invoice_created_at;'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_invoices_77days_all_accounts = pig.run_query(sql_invoices_77days_all_accounts, return_data=True)\n",
    "\n",
    "# Words count in invoice's description, notes, terms, address\n",
    "df_invoices_77days_all_accounts['avg_wc_description_day_77'] = df_invoices_77days_all_accounts.apply(lambda x: words_count(x['description']), axis=1)\n",
    "df_invoices_77days_all_accounts['avg_wc_notes_day_77'] = df_invoices_77days_all_accounts.apply(lambda x: words_count(x['notes']), axis=1)\n",
    "df_invoices_77days_all_accounts['avg_wc_terms_day_77'] = df_invoices_77days_all_accounts.apply(lambda x: words_count(x['terms']), axis=1)\n",
    "df_invoices_77days_all_accounts['avg_wc_address_day_77'] = df_invoices_77days_all_accounts.apply(lambda x: words_count(x['address']), axis=1)\n",
    "\n",
    "                                                                                                                   \n",
    "# Filters the text columns from the dataframe\n",
    "df_invoices_77days_all_accounts_fil = df_invoices_77days_all_accounts.filter(['systemid', 'invoiceid', \n",
    "                                                                            'signup_date', \n",
    "                                                                            'create_date', \n",
    "                                                                            'created_at',\n",
    "                                                                            'days_to_invoice_creation', \n",
    "                                                                            'avg_wc_description_day_77', \n",
    "                                                                            'avg_wc_notes_day_77', \n",
    "                                                                            'avg_wc_terms_day_77',\n",
    "                                                                            'avg_wc_address_day_77'])  \n",
    "                                                                                                                   \n",
    "# Summing (grouping) all invoices for a 'systemid'\n",
    "df_word_count_77days_all_accounts_total = df_invoices_77days_all_accounts_fil.groupby('systemid').mean()  \n",
    "\n",
    "# Final word count table\n",
    "df_word_count_77days_all_accounts_final = df_word_count_77days_all_accounts_total.filter(['systemid',\n",
    "                                                                            'avg_wc_description_day_77', \n",
    "                                                                            'avg_wc_notes_day_77', \n",
    "                                                                            'avg_wc_terms_day_77',\n",
    "                                                                            'avg_wc_address_day_77'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.12 Invoice Within 84 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Invoice Data Within 84 Days After Signup_Date ####################\n",
    "\n",
    "# SQL for impoorting all invoices created within 84 days after signup_date\n",
    "sql_invoices_84days_all_accounts = '''WITH invoices_in_a_period AS (\n",
    "    SELECT systemid, signup_date\n",
    "    FROM report_systems rs\n",
    "    WHERE signup_date BETWEEN '2019-08-01' and '2019-10-31'\n",
    "), invoice_created_at AS (\n",
    "    SELECT\n",
    "           pic.systemid,\n",
    "           inv.invoiceid,\n",
    "           pic.signup_date,\n",
    "           inv.create_date,\n",
    "           inv.created_at,\n",
    "           inv.description,\n",
    "           inv.notes,\n",
    "           inv.terms,\n",
    "           inv.address,\n",
    "           DATEDIFF(days, pic.signup_date, inv.created_at) AS days_to_invoice_creation\n",
    "    FROM invoices_in_a_period AS pic\n",
    "    LEFT JOIN coalesced_live_shards.invoice_stable as inv USING (systemid)\n",
    "    WHERE ((days_to_invoice_creation BETWEEN 0 AND 84) OR days_to_invoice_creation IS NULL)\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM invoice_created_at;'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_invoices_84days_all_accounts = pig.run_query(sql_invoices_84days_all_accounts, return_data=True)\n",
    "\n",
    "# Words count in invoice's description, notes, terms, address\n",
    "df_invoices_84days_all_accounts['avg_wc_description_day_84'] = df_invoices_84days_all_accounts.apply(lambda x: words_count(x['description']), axis=1)\n",
    "df_invoices_84days_all_accounts['avg_wc_notes_day_84'] = df_invoices_84days_all_accounts.apply(lambda x: words_count(x['notes']), axis=1)\n",
    "df_invoices_84days_all_accounts['avg_wc_terms_day_84'] = df_invoices_84days_all_accounts.apply(lambda x: words_count(x['terms']), axis=1)\n",
    "df_invoices_84days_all_accounts['avg_wc_address_day_84'] = df_invoices_84days_all_accounts.apply(lambda x: words_count(x['address']), axis=1)\n",
    "\n",
    "                                                                                                                   \n",
    "# Filters the text columns from the dataframe\n",
    "df_invoices_84days_all_accounts_fil = df_invoices_84days_all_accounts.filter(['systemid', 'invoiceid', \n",
    "                                                                            'signup_date', \n",
    "                                                                            'create_date', \n",
    "                                                                            'created_at',\n",
    "                                                                            'days_to_invoice_creation', \n",
    "                                                                            'avg_wc_description_day_84', \n",
    "                                                                            'avg_wc_notes_day_84', \n",
    "                                                                            'avg_wc_terms_day_84',\n",
    "                                                                            'avg_wc_address_day_84'])  \n",
    "                                                                                                                   \n",
    "# Summing (grouping) all invoices for a 'systemid'\n",
    "df_word_count_84days_all_accounts_total = df_invoices_84days_all_accounts_fil.groupby('systemid').mean()  \n",
    "\n",
    "# Final word count table\n",
    "df_word_count_84days_all_accounts_final = df_word_count_84days_all_accounts_total.filter(['systemid',\n",
    "                                                                            'avg_wc_description_day_84', \n",
    "                                                                            'avg_wc_notes_day_84', \n",
    "                                                                            'avg_wc_terms_day_84',\n",
    "                                                                            'avg_wc_address_day_84'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.13 Invoice Within 91 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Invoice Data Within 91 Days After Signup_Date ####################\n",
    "\n",
    "# SQL for impoorting all invoices created within 91 days after signup_date\n",
    "sql_invoices_91days_all_accounts = '''WITH invoices_in_a_period AS (\n",
    "    SELECT systemid, signup_date\n",
    "    FROM report_systems rs\n",
    "    WHERE signup_date BETWEEN '2019-08-01' and '2019-10-31'\n",
    "), invoice_created_at AS (\n",
    "    SELECT\n",
    "           pic.systemid,\n",
    "           inv.invoiceid,\n",
    "           pic.signup_date,\n",
    "           inv.create_date,\n",
    "           inv.created_at,\n",
    "           inv.description,\n",
    "           inv.notes,\n",
    "           inv.terms,\n",
    "           inv.address,\n",
    "           DATEDIFF(days, pic.signup_date, inv.created_at) AS days_to_invoice_creation\n",
    "    FROM invoices_in_a_period AS pic\n",
    "    LEFT JOIN coalesced_live_shards.invoice_stable as inv USING (systemid)\n",
    "    WHERE ((days_to_invoice_creation BETWEEN 0 AND 91) OR days_to_invoice_creation IS NULL)\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM invoice_created_at;'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_invoices_91days_all_accounts = pig.run_query(sql_invoices_91days_all_accounts, return_data=True)\n",
    "\n",
    "# Words count in invoice's description, notes, terms, address\n",
    "df_invoices_91days_all_accounts['avg_wc_description_day_91'] = df_invoices_91days_all_accounts.apply(lambda x: words_count(x['description']), axis=1)\n",
    "df_invoices_91days_all_accounts['avg_wc_notes_day_91'] = df_invoices_91days_all_accounts.apply(lambda x: words_count(x['notes']), axis=1)\n",
    "df_invoices_91days_all_accounts['avg_wc_terms_day_91'] = df_invoices_91days_all_accounts.apply(lambda x: words_count(x['terms']), axis=1)\n",
    "df_invoices_91days_all_accounts['avg_wc_address_day_91'] = df_invoices_91days_all_accounts.apply(lambda x: words_count(x['address']), axis=1)\n",
    "\n",
    "                                                                                                                   \n",
    "# Filters the text columns from the dataframe\n",
    "df_invoices_91days_all_accounts_fil = df_invoices_91days_all_accounts.filter(['systemid', 'invoiceid', \n",
    "                                                                            'signup_date', \n",
    "                                                                            'create_date', \n",
    "                                                                            'created_at',\n",
    "                                                                            'days_to_invoice_creation', \n",
    "                                                                            'avg_wc_description_day_91', \n",
    "                                                                            'avg_wc_notes_day_91', \n",
    "                                                                            'avg_wc_terms_day_91',\n",
    "                                                                            'avg_wc_address_day_91'])  \n",
    "                                                                                                                   \n",
    "# Summing (grouping) all invoices for a 'systemid'\n",
    "df_word_count_91days_all_accounts_total = df_invoices_91days_all_accounts_fil.groupby('systemid').mean()  \n",
    "\n",
    "# Final word count table\n",
    "df_word_count_91days_all_accounts_final = df_word_count_91days_all_accounts_total.filter(['systemid',\n",
    "                                                                            'avg_wc_description_day_91', \n",
    "                                                                            'avg_wc_notes_day_91', \n",
    "                                                                            'avg_wc_terms_day_91',\n",
    "                                                                            'avg_wc_address_day_91'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking\n",
    "# df_word_count_91days_all_accounts_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.14 Join All Periodic Invoice Word Counts Features Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Join All Periodic Invoice Features Data #######################\n",
    "\n",
    "# Joininig day 7 and day 14 th dataframes\n",
    "df_avg_invoice_word_count = pd.merge(df_word_count_7days_all_accounts_final, df_word_count_14days_all_accounts_final,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# left join day 21 \n",
    "df_avg_invoice_word_count = pd.merge(df_avg_invoice_word_count, df_word_count_21days_all_accounts_final,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# left join day 28\n",
    "df_avg_invoice_word_count = pd.merge(df_avg_invoice_word_count, df_word_count_28days_all_accounts_final,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# left join day 35\n",
    "df_avg_invoice_word_count = pd.merge(df_avg_invoice_word_count, df_word_count_35days_all_accounts_final,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# left join day 42\n",
    "df_avg_invoice_word_count = pd.merge(df_avg_invoice_word_count, df_word_count_42days_all_accounts_final,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# left join day 49\n",
    "df_avg_invoice_word_count = pd.merge(df_avg_invoice_word_count, df_word_count_49days_all_accounts_final,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# left join day 56\n",
    "df_avg_invoice_word_count = pd.merge(df_avg_invoice_word_count, df_word_count_56days_all_accounts_final,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# left join day 63\n",
    "df_avg_invoice_word_count = pd.merge(df_avg_invoice_word_count, df_word_count_63days_all_accounts_final,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# left join day 70\n",
    "df_avg_invoice_word_count = pd.merge(df_avg_invoice_word_count, df_word_count_70days_all_accounts_final,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# left join day 77\n",
    "df_avg_invoice_word_count = pd.merge(df_avg_invoice_word_count, df_word_count_77days_all_accounts_final,\n",
    "                                     on='systemid', how='left')\n",
    "# left join day 84\n",
    "df_avg_invoice_word_count = pd.merge(df_avg_invoice_word_count, df_word_count_84days_all_accounts_final,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# left join day 91\n",
    "df_avg_invoice_word_count = pd.merge(df_avg_invoice_word_count, df_word_count_91days_all_accounts_final,\n",
    "                                     on='systemid', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking\n",
    "# df_avg_invoice_word_count.shape\n",
    "# df_avg_invoice_word_count.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import RSystems, Invoice & Client Counts Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Import RSystems, Periodic Invoices & Client Counts Data ###############\n",
    "\n",
    "# SQL query \n",
    "sql_rs_invoices_clients_activities_all_accounts = '''WITH periodic_report_system_activities AS (\n",
    "    SELECT\n",
    "        systemid,\n",
    "        signup_date,\n",
    "        admin_email,\n",
    "        is_sales_managed,\n",
    "        is_freshbooks_account_active,\n",
    "        is_paying,\n",
    "        signup_ip_address\n",
    "    FROM report_systems rs\n",
    "    WHERE signup_date BETWEEN '2019-08-01' and '2019-10-31'\n",
    "), invoice_create_date AS (\n",
    "    SELECT\n",
    "           pic.systemid,\n",
    "           inv.invoiceid,\n",
    "           inv.create_date,\n",
    "           inv.created_at,\n",
    "           DATEDIFF(days, pic.signup_date, inv.created_at) AS days_to_invoice_creation\n",
    "    FROM periodic_report_system_activities AS pic\n",
    "    LEFT JOIN coalesced_live_shards.invoice_stable as inv USING (systemid)\n",
    "), invoice_grouping AS (\n",
    "    SELECT\n",
    "           systemid,\n",
    "           COUNT(invoiceid) as invoice_count,\n",
    "           SUM(CASE WHEN days_to_invoice_creation BETWEEN 0 AND 7 THEN 1 ELSE 0 END) AS invoice_count_day_7,\n",
    "           SUM(CASE WHEN days_to_invoice_creation BETWEEN 0 AND 14 THEN 1 ELSE 0 END) AS invoice_count_day_14,\n",
    "           SUM(CASE WHEN days_to_invoice_creation BETWEEN 0 AND 21 THEN 1 ELSE 0 END) AS invoice_count_day_21,\n",
    "           SUM(CASE WHEN days_to_invoice_creation BETWEEN 0 AND 28 THEN 1 ELSE 0 END) AS invoice_count_day_28,\n",
    "           SUM(CASE WHEN days_to_invoice_creation BETWEEN 0 AND 35 THEN 1 ELSE 0 END) AS invoice_count_day_35,\n",
    "           SUM(CASE WHEN days_to_invoice_creation BETWEEN 0 AND 42 THEN 1 ELSE 0 END) AS invoice_count_day_42,\n",
    "           SUM(CASE WHEN days_to_invoice_creation BETWEEN 0 AND 49 THEN 1 ELSE 0 END) AS invoice_count_day_49,\n",
    "           SUM(CASE WHEN days_to_invoice_creation BETWEEN 0 AND 56 THEN 1 ELSE 0 END) AS invoice_count_day_56,\n",
    "           SUM(CASE WHEN days_to_invoice_creation BETWEEN 0 AND 63 THEN 1 ELSE 0 END) AS invoice_count_day_63,\n",
    "           SUM(CASE WHEN days_to_invoice_creation BETWEEN 0 AND 70 THEN 1 ELSE 0 END) AS invoice_count_day_70,\n",
    "           SUM(CASE WHEN days_to_invoice_creation BETWEEN 0 AND 77 THEN 1 ELSE 0 END) AS invoice_count_day_77,\n",
    "           SUM(CASE WHEN days_to_invoice_creation BETWEEN 0 AND 84 THEN 1 ELSE 0 END) AS invoice_count_day_84,\n",
    "           SUM(CASE WHEN days_to_invoice_creation BETWEEN 0 AND 91 THEN 1 ELSE 0 END) AS invoice_count_day_91\n",
    "    FROM invoice_create_date\n",
    "    GROUP BY systemid\n",
    "), client_crate_date AS (\n",
    "     SELECT\n",
    "            pic.systemid,\n",
    "            usr.userid,\n",
    "            usr.signup_date,\n",
    "            DATEDIFF(days, pic.signup_date, usr.signup_date) AS days_to_client_creation\n",
    "    FROM periodic_report_system_activities  AS pic\n",
    "    LEFT JOIN coalesced_live_shards.\"user\" as usr USING (systemid)\n",
    "), client_grouping AS (\n",
    "    SELECT\n",
    "           systemid,\n",
    "           count(userid) AS client_count,\n",
    "           SUM(CASE WHEN days_to_client_creation BETWEEN 0 AND 7 THEN 1 ELSE 0 END) AS client_count_day_7,\n",
    "           SUM(CASE WHEN days_to_client_creation BETWEEN 0 AND 14 THEN 1 ELSE 0 END) AS client_count_day_14,\n",
    "           SUM(CASE WHEN days_to_client_creation BETWEEN 0 AND 21 THEN 1 ELSE 0 END) AS client_count_day_21,\n",
    "           SUM(CASE WHEN days_to_client_creation BETWEEN 0 AND 28 THEN 1 ELSE 0 END) AS client_count_day_28,\n",
    "           SUM(CASE WHEN days_to_client_creation BETWEEN 0 AND 35 THEN 1 ELSE 0 END) AS client_count_day_35,\n",
    "           SUM(CASE WHEN days_to_client_creation BETWEEN 0 AND 42 THEN 1 ELSE 0 END) AS client_count_day_42,\n",
    "           SUM(CASE WHEN days_to_client_creation BETWEEN 0 AND 49 THEN 1 ELSE 0 END) AS client_count_day_49,\n",
    "           SUM(CASE WHEN days_to_client_creation BETWEEN 0 AND 56 THEN 1 ELSE 0 END) AS client_count_day_56,\n",
    "           SUM(CASE WHEN days_to_client_creation BETWEEN 0 AND 63 THEN 1 ELSE 0 END) AS client_count_day_63,\n",
    "           SUM(CASE WHEN days_to_client_creation BETWEEN 0 AND 70 THEN 1 ELSE 0 END) AS client_count_day_70,\n",
    "           SUM(CASE WHEN days_to_client_creation BETWEEN 0 AND 77 THEN 1 ELSE 0 END) AS client_count_day_77,\n",
    "           SUM(CASE WHEN days_to_client_creation BETWEEN 0 AND 84 THEN 1 ELSE 0 END) AS client_count_day_84,\n",
    "           SUM(CASE WHEN days_to_client_creation BETWEEN 0 AND 91 THEN 1 ELSE 0 END) AS client_count_day_91\n",
    "    FROM  client_crate_date\n",
    "    GROUP BY systemid\n",
    ")\n",
    "\n",
    "SELECT\n",
    "       systemid,\n",
    "       signup_date,\n",
    "       admin_email,\n",
    "       is_sales_managed,\n",
    "       is_freshbooks_account_active,\n",
    "       is_paying,\n",
    "       signup_ip_address,\n",
    "       inv_gr.invoice_count,\n",
    "       inv_gr.invoice_count_day_7,\n",
    "       inv_gr.invoice_count_day_14,\n",
    "       inv_gr.invoice_count_day_21,\n",
    "       inv_gr.invoice_count_day_28,\n",
    "       inv_gr.invoice_count_day_35,\n",
    "       inv_gr.invoice_count_day_42,\n",
    "       inv_gr.invoice_count_day_49,\n",
    "       inv_gr.invoice_count_day_56,\n",
    "       inv_gr.invoice_count_day_63,\n",
    "       inv_gr.invoice_count_day_70,\n",
    "       inv_gr.invoice_count_day_77,\n",
    "       inv_gr.invoice_count_day_84,\n",
    "       inv_gr.invoice_count_day_91,\n",
    "       cl_gr.client_count,\n",
    "       cl_gr.client_count_day_7,\n",
    "       cl_gr.client_count_day_14,\n",
    "       cl_gr.client_count_day_21,\n",
    "       cl_gr.client_count_day_28,\n",
    "       cl_gr.client_count_day_35,\n",
    "       cl_gr.client_count_day_42,\n",
    "       cl_gr.client_count_day_49,\n",
    "       cl_gr.client_count_day_56,\n",
    "       cl_gr.client_count_day_63,\n",
    "       cl_gr.client_count_day_70,\n",
    "       cl_gr.client_count_day_77,\n",
    "       cl_gr.client_count_day_84,\n",
    "       cl_gr.client_count_day_91\n",
    "FROM periodic_report_system_activities\n",
    "LEFT JOIN invoice_grouping as inv_gr USING (systemid)\n",
    "LEFT JOIN client_grouping AS cl_gr USING (systemid);\n",
    "'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_rs_invoices_clients_activities_all_accounts = pig.run_query(sql_rs_invoices_clients_activities_all_accounts, return_data=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Join Avg Word Counts and Invoice & Client Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Join Avg Word Counts and Invoice & Client Counts ########################\n",
    "\n",
    "# left join invoices' average periodic word counts (description, notes, terms, address) with the invices & client counts\n",
    "df_periodic_invoice_all_counts = pd.merge(df_avg_invoice_word_count, df_rs_invoices_clients_activities_all_accounts,\n",
    "                                     on='systemid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check \n",
    "# df_periodic_invoice_all_counts.shape\n",
    "# df_periodic_invoice_all_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as csv file\n",
    "df_periodic_invoice_all_counts.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/periodic_invoice_all_counts_accounts_last_3_months.tsv\", \n",
    "                                      sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV file\n",
    "# df_periodic_invoice_all_counts = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/periodic_invoice_all_counts_accounts_last_3_months.tsv\", \n",
    "#                                       sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Import and Exact Features from Events Data\n",
    "\n",
    "## 4.1 Event data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Event Features Extraction ################################\n",
    "\n",
    "# SQL for events \n",
    "sql_events = '''WITH selected_accounts_events AS (\n",
    "    SELECT systemid,\n",
    "           signup_date,\n",
    "           signup_datetime\n",
    "    FROM report_systems\n",
    "    WHERE signup_date BETWEEN '2019-08-01' and '2019-10-31'\n",
    "), events_activities AS (\n",
    "    SELECT sae.systemid,\n",
    "           signup_date,\n",
    "           dd.date,\n",
    "           datediff(days, signup_date, dd.date) as days_to_event,\n",
    "           lower(e.event) as event,\n",
    "           ec.count\n",
    "    FROM selected_accounts_events AS sae\n",
    "    LEFT JOIN event_counts AS ec USING (systemid)\n",
    "    LEFT JOIN d_date AS dd USING (date_key)\n",
    "    LEFT JOIN d_event e on ec.event_key = e.event_key\n",
    "), event_groupings AS (\n",
    "    SELECT distinct  ea.systemid,\n",
    "                    ea.signup_date,\n",
    "                    ea.date,\n",
    "                    ea.event,\n",
    "                    ea.count,\n",
    "                    (CASE WHEN days_to_event BETWEEN 0 AND 7 THEN ea.count END) AS day_7_event,\n",
    "                    (CASE WHEN days_to_event BETWEEN 0 AND 14 THEN ea.count END) AS day_14_event,\n",
    "                    (CASE WHEN days_to_event BETWEEN 0 AND 21 THEN ea.count END) AS day_21_event,\n",
    "                    (CASE WHEN days_to_event BETWEEN 0 AND 28 THEN ea.count END) AS day_28_event,\n",
    "                    (CASE WHEN days_to_event BETWEEN 0 AND 35 THEN ea.count END) AS day_35_event,\n",
    "                    (CASE WHEN days_to_event BETWEEN 0 AND 42 THEN ea.count END) AS day_42_event,\n",
    "                    (CASE WHEN days_to_event BETWEEN 0 AND 49 THEN ea.count END) AS day_49_event,\n",
    "                    (CASE WHEN days_to_event BETWEEN 0 AND 56 THEN ea.count END) AS day_56_event,\n",
    "                    (CASE WHEN days_to_event BETWEEN 0 AND 63 THEN ea.count END) AS day_63_event,\n",
    "                    (CASE WHEN days_to_event BETWEEN 0 AND 70 THEN ea.count END) AS day_70_event,\n",
    "                    (CASE WHEN days_to_event BETWEEN 0 AND 77 THEN ea.count END) AS day_77_event,\n",
    "                    (CASE WHEN days_to_event BETWEEN 0 AND 84 THEN ea.count END) AS day_84_event,\n",
    "                    (CASE WHEN days_to_event BETWEEN 0 AND 91 THEN ea.count END) AS day_91_event\n",
    "    FROM events_activities AS ea\n",
    ")\n",
    "SELECT systemid,\n",
    "       signup_date,\n",
    "       date,\n",
    "       event,\n",
    "       count,\n",
    "       sum(day_7_event) AS event_count_day_7,\n",
    "       sum(day_14_event) AS event_count_day_14,\n",
    "       sum(day_21_event) AS event_count_day_21,\n",
    "       sum(day_28_event) AS event_count_day_28,\n",
    "       sum(day_35_event) AS event_count_day_35,\n",
    "       sum(day_42_event) AS event_count_day_42,\n",
    "       sum(day_49_event) AS event_count_day_49,\n",
    "       sum(day_56_event) AS event_count_day_56,\n",
    "       sum(day_63_event) AS event_count_day_63,\n",
    "       sum(day_70_event) AS event_count_day_70,\n",
    "       sum(day_77_event) AS event_count_day_77,\n",
    "       sum(day_84_event) AS event_count_day_84,\n",
    "       sum(day_91_event) AS event_count_day_91\n",
    "From event_groupings\n",
    "GROUP BY systemid, signup_date, date, event, count\n",
    "ORDER BY systemid, count DESC;'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_events_all_accounts = pig.run_query(sql_events, return_data=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "# df_events_all_accounts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Removing whitespace from the event string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Removing Whitespace From the Event String ######################\n",
    "\n",
    "# Removing row if there is 'None' the event cell\n",
    "df_events_all_accounts = df_events_all_accounts[~df_events_all_accounts.astype(str).eq('None').any(1)]\n",
    "\n",
    "# Replace the 'NaN' cell by zero\n",
    "df_events_all_accounts.fillna(0, inplace=True)\n",
    "\n",
    "# Using lambda function to remove the white space in the event string name\n",
    "df_events_all_accounts['event_name'] = df_events_all_accounts.apply(lambda x: x['event'].replace(' ', ''), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Final Features Extraction: Day 7\n",
    "\n",
    "## 5.1 Filter only events for day 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Final Features Extraction: Day 7 ##############################\n",
    "\n",
    "# Filtered the events columns for day 7 period\n",
    "df_events_all_accounts_day_7 = df_events_all_accounts[['systemid', 'event_count_day_7', 'event_name']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Pivote the day events (each unique event become a column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pivote the Day 7 Events (Each Unique Event Become a Column)###\n",
    "# Pivot table based on the unique column value in 'event_name'\n",
    "df_events_all_accounts_day_7 = df_events_all_accounts_day_7.pivot_table(values='event_count_day_7', columns='event_name', index='systemid', aggfunc=np.sum,  fill_value=0)\n",
    "\n",
    "# Drop the old column name\n",
    "df_events_all_accounts_day_7.columns.name = None\n",
    "\n",
    "# Reset the index\n",
    "df_events_all_accounts_day_7 = df_events_all_accounts_day_7.reset_index()\n",
    "\n",
    "# Replace 'NaN' with zero\n",
    "df_events_all_accounts_day_7.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV export \n",
    "df_events_all_accounts_day_7.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_7.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_events_all_accounts_day_7 = pd.read_csv(\n",
    "#   \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_7.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Get only important features from day 7 events data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Adding missing features columns in the event features dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing importing features list\n",
    "important_features = pd.read_csv( \n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/important_features.csv\", sep=\"\\n,\")\n",
    "\n",
    "# Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking\n",
    "# important_features.head()\n",
    "# imp_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding missing important feature column \n",
    "for i in range(len(imp_features_list)):\n",
    "    if imp_features_list[i] in df_events_all_accounts_day_7.columns:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")\n",
    "        df_events_all_accounts_day_7[imp_features_list[i]] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking\n",
    "# df_events_all_accounts_day_7.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Filtering only important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering only important features \n",
    "df_events_imp_features_all_accounts_day_7 = \\\n",
    "            df_events_all_accounts_day_7.loc[:, df_events_all_accounts_day_7.columns.str.contains('|'.join(imp_features_list))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking\n",
    "# df_events_imp_features_all_accounts_day_7.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Filtering avgerage word counts features from invoice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filtering average word counts features from the invoice data\n",
    "# Invoice features at day 7\n",
    "df_invoice_features_all_accounts_day_7 = df_periodic_invoice_all_counts[[\n",
    "        'systemid',\n",
    "        'admin_email',\n",
    "        'admin_email', \n",
    "        'is_sales_managed', \n",
    "        'is_freshbooks_account_active'\n",
    "        'is_paying',\n",
    "        'avg_wc_description_day_7',\n",
    "        'avg_wc_notes_day_7',\n",
    "        'avg_wc_terms_day_7',\n",
    "        'avg_wc_address_day_7',\n",
    "        'invoice_count_day_7',\n",
    "        'client_count_day_7'\n",
    "        ]]\n",
    "\n",
    "# CSV export\n",
    "df_invoice_features_all_accounts_day_7.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_7.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_invoice_features_all_accounts_day_7 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_7.tsv\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112640, 11)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking\n",
    "# df_invoice_features_all_accounts_day_7.head()\n",
    "# df_invoice_features_all_accounts_day_7.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Merging events' and invoice features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging events' and invoice features\n",
    "df_final_features_day_7 = pd.merge(df_events_imp_features_all_accounts_day_7, \n",
    "                                   df_invoice_features_all_accounts_day_7,\n",
    "                                     on='systemid', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114908, 242)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking\n",
    "# df_final_features_day_7.head()\n",
    "# df_final_features_day_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV export \n",
    "df_final_features_day_7.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_7.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_final_features_day_7 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_7.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Features Extractions: Day 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Final Features Extraction: Day 14 ##############################\n",
    "\n",
    "# Filtered the events columns for day 14 period\n",
    "df_events_all_accounts_day_14 = df_events_all_accounts[['systemid', 'event_count_day_14', 'event_name']]\n",
    "\n",
    "### Pivote the Day 14 Events (Each Unique Event Become a Column)###\n",
    "# Pivot table based on the unique column value in 'event_name'\n",
    "df_events_all_accounts_day_14 = df_events_all_accounts_day_14.pivot_table(values='event_count_day_14', columns='event_name', index='systemid', aggfunc=np.sum,  fill_value=0)\n",
    "\n",
    "# Drop the old column name\n",
    "df_events_all_accounts_day_14.columns.name = None\n",
    "\n",
    "# Reset the index\n",
    "df_events_all_accounts_day_14 = df_events_all_accounts_day_14.reset_index()\n",
    "\n",
    "# Replace 'NaN' with zero\n",
    "df_events_all_accounts_day_14.fillna(0, inplace=True)\n",
    "\n",
    "# CSV export \n",
    "df_events_all_accounts_day_14.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_14.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_events_all_accounts_day_14 = pd.read_csv(\n",
    "#   \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_14.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Importing importing features list\n",
    "important_features = pd.read_csv( \n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/important_features.csv\", sep=\"\\n,\")\n",
    "\n",
    "# Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])\n",
    "\n",
    "# Adding missing important feature column \n",
    "for i in range(len(imp_features_list)):\n",
    "    if imp_features_list[i] in df_events_all_accounts_day_14.columns:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")\n",
    "        df_events_all_accounts_day_14[imp_features_list[i]] = 0\n",
    "\n",
    "\n",
    "# Filtering only important features \n",
    "df_events_imp_features_all_accounts_day_14 = \\\n",
    "            df_events_all_accounts_day_14.loc[:, df_events_all_accounts_day_14.columns.str.contains('|'.join(imp_features_list))]\n",
    "\n",
    "\n",
    "### Filtering average word counts features from the invoice data\n",
    "# Invoice features at day 14\n",
    "df_invoice_features_all_accounts_day_14 = df_periodic_invoice_all_counts[[\n",
    "        'systemid',\n",
    "        'avg_wc_description_day_14',\n",
    "        'avg_wc_notes_day_14',\n",
    "        'avg_wc_terms_day_14',\n",
    "        'avg_wc_address_day_14',\n",
    "        'invoice_count_day_14',\n",
    "        'client_count_day_14',\n",
    "        'is_freshbooks_account_active',\n",
    "        'is_paying',\n",
    "        'base_subscription_amount_first_upgrade',\n",
    "        'upgrade_ever'\n",
    "        ]]\n",
    "\n",
    "# CSV export\n",
    "df_invoice_features_all_accounts_day_14.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_14.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_invoice_features_all_accounts_day_14 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_14.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Merging events' and invoice features\n",
    "df_final_features_day_14 = pd.merge(df_events_imp_features_all_accounts_day_14, \n",
    "                                   df_invoice_features_all_accounts_day_14,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# CSV export \n",
    "df_final_features_day_14.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_14.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_final_features_day_14 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_14.tsv\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Feature Extraction: Day 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Final Features Extraction: Day 21 ##############################\n",
    "\n",
    "# Filtered the events columns for day 21 period\n",
    "df_events_all_accounts_day_21 = df_events_all_accounts[['systemid', 'event_count_day_21', 'event_name']]\n",
    "\n",
    "### Pivote the Day 21 Events (Each Unique Event Become a Column)###\n",
    "# Pivot table based on the unique column value in 'event_name'\n",
    "df_events_all_accounts_day_21 = df_events_all_accounts_day_21.pivot_table(values='event_count_day_21', columns='event_name', index='systemid', aggfunc=np.sum,  fill_value=0)\n",
    "\n",
    "# Drop the old column name\n",
    "df_events_all_accounts_day_21.columns.name = None\n",
    "\n",
    "# Reset the index\n",
    "df_events_all_accounts_day_21 = df_events_all_accounts_day_21.reset_index()\n",
    "\n",
    "# Replace 'NaN' with zero\n",
    "df_events_all_accounts_day_21.fillna(0, inplace=True)\n",
    "\n",
    "# CSV export \n",
    "df_events_all_accounts_day_21.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_21.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_events_all_accounts_day_21 = pd.read_csv(\n",
    "#   \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_21.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Importing importing features list\n",
    "important_features = pd.read_csv( \n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/important_features.csv\", sep=\"\\n,\")\n",
    "\n",
    "# Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])\n",
    "\n",
    "# Adding missing important feature column \n",
    "for i in range(len(imp_features_list)):\n",
    "    if imp_features_list[i] in df_events_all_accounts_day_21.columns:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")\n",
    "        df_events_all_accounts_day_21[imp_features_list[i]] = 0\n",
    "\n",
    "\n",
    "# Filtering only important features \n",
    "df_events_imp_features_all_accounts_day_21 = \\\n",
    "            df_events_all_accounts_day_21.loc[:, df_events_all_accounts_day_21.columns.str.contains('|'.join(imp_features_list))]\n",
    "\n",
    "\n",
    "### Filtering average word counts features from the invoice data\n",
    "# Invoice features at day 21\n",
    "df_invoice_features_all_accounts_day_21 = df_periodic_invoice_all_counts[[\n",
    "        'systemid',\n",
    "        'avg_wc_description_day_21',\n",
    "        'avg_wc_notes_day_21',\n",
    "        'avg_wc_terms_day_21',\n",
    "        'avg_wc_address_day_21',\n",
    "        'invoice_count_day_21',\n",
    "        'client_count_day_21',\n",
    "        'is_freshbooks_account_active',\n",
    "        'is_paying',\n",
    "        'base_subscription_amount_first_upgrade',\n",
    "        'upgrade_ever'\n",
    "        ]]\n",
    "\n",
    "# CSV export\n",
    "df_invoice_features_all_accounts_day_21.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_21.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_invoice_features_all_accounts_day_21 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_21.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Merging events' and invoice features\n",
    "df_final_features_day_21 = pd.merge(df_events_imp_features_all_accounts_day_21, \n",
    "                                   df_invoice_features_all_accounts_day_21,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# CSV export \n",
    "df_final_features_day_21.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_21.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_final_features_day_21 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_21.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Final Features Extraction: Day 28 ##############################\n",
    "\n",
    "# Filtered the events columns for day 28 period\n",
    "df_events_all_accounts_day_28 = df_events_all_accounts[['systemid', 'event_count_day_28', 'event_name']]\n",
    "\n",
    "### Pivote the Day 28 Events (Each Unique Event Become a Column)###\n",
    "# Pivot table based on the unique column value in 'event_name'\n",
    "df_events_all_accounts_day_28 = df_events_all_accounts_day_28.pivot_table(values='event_count_day_28', columns='event_name', index='systemid', aggfunc=np.sum,  fill_value=0)\n",
    "\n",
    "# Drop the old column name\n",
    "df_events_all_accounts_day_28.columns.name = None\n",
    "\n",
    "# Reset the index\n",
    "df_events_all_accounts_day_28 = df_events_all_accounts_day_28.reset_index()\n",
    "\n",
    "# Replace 'NaN' with zero\n",
    "df_events_all_accounts_day_28.fillna(0, inplace=True)\n",
    "\n",
    "# CSV export \n",
    "df_events_all_accounts_day_28.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_28.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_events_all_accounts_day_28 = pd.read_csv(\n",
    "#   \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_28.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Importing importing features list\n",
    "important_features = pd.read_csv( \n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/important_features.csv\", sep=\"\\n,\")\n",
    "\n",
    "# Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])\n",
    "\n",
    "# Adding missing important feature column \n",
    "for i in range(len(imp_features_list)):\n",
    "    if imp_features_list[i] in df_events_all_accounts_day_28.columns:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")\n",
    "        df_events_all_accounts_day_28[imp_features_list[i]] = 0\n",
    "\n",
    "\n",
    "# Filtering only important features \n",
    "df_events_imp_features_all_accounts_day_28 = \\\n",
    "            df_events_all_accounts_day_28.loc[:, df_events_all_accounts_day_28.columns.str.contains('|'.join(imp_features_list))]\n",
    "\n",
    "\n",
    "### Filtering average word counts features from the invoice data\n",
    "# Invoice features at day 28\n",
    "df_invoice_features_all_accounts_day_28 = df_periodic_invoice_all_counts[[\n",
    "        'systemid',\n",
    "        'avg_wc_description_day_28',\n",
    "        'avg_wc_notes_day_28',\n",
    "        'avg_wc_terms_day_28',\n",
    "        'avg_wc_address_day_28',\n",
    "        'invoice_count_day_28',\n",
    "        'client_count_day_28',\n",
    "        'is_freshbooks_account_active',\n",
    "        'is_paying',\n",
    "        'base_subscription_amount_first_upgrade',\n",
    "        'upgrade_ever'\n",
    "        ]]\n",
    "\n",
    "# CSV export\n",
    "df_invoice_features_all_accounts_day_28.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_28.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_invoice_features_all_accounts_day_28 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_28.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Merging events' and invoice features\n",
    "df_final_features_day_28 = pd.merge(df_events_imp_features_all_accounts_day_28, \n",
    "                                   df_invoice_features_all_accounts_day_28,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# CSV export \n",
    "df_final_features_day_28.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_28.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_final_features_day_28 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_28.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Final Features Extraction: Day 35 ##############################\n",
    "\n",
    "# Filtered the events columns for day 35 period\n",
    "df_events_all_accounts_day_35 = df_events_all_accounts[['systemid', 'event_count_day_35', 'event_name']]\n",
    "\n",
    "### Pivote the Day 35 Events (Each Unique Event Become a Column)###\n",
    "# Pivot table based on the unique column value in 'event_name'\n",
    "df_events_all_accounts_day_35 = df_events_all_accounts_day_35.pivot_table(values='event_count_day_35', columns='event_name', index='systemid', aggfunc=np.sum,  fill_value=0)\n",
    "\n",
    "# Drop the old column name\n",
    "df_events_all_accounts_day_35.columns.name = None\n",
    "\n",
    "# Reset the index\n",
    "df_events_all_accounts_day_35 = df_events_all_accounts_day_35.reset_index()\n",
    "\n",
    "# Replace 'NaN' with zero\n",
    "df_events_all_accounts_day_35.fillna(0, inplace=True)\n",
    "\n",
    "# CSV export \n",
    "df_events_all_accounts_day_35.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_35.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_events_all_accounts_day_35 = pd.read_csv(\n",
    "#   \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_35.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Importing importing features list\n",
    "important_features = pd.read_csv( \n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/important_features.csv\", sep=\"\\n,\")\n",
    "\n",
    "# Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])\n",
    "\n",
    "# Adding missing important feature column \n",
    "for i in range(len(imp_features_list)):\n",
    "    if imp_features_list[i] in df_events_all_accounts_day_35.columns:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")\n",
    "        df_events_all_accounts_day_35[imp_features_list[i]] = 0\n",
    "\n",
    "\n",
    "# Filtering only important features \n",
    "df_events_imp_features_all_accounts_day_35 = \\\n",
    "            df_events_all_accounts_day_35.loc[:, df_events_all_accounts_day_35.columns.str.contains('|'.join(imp_features_list))]\n",
    "\n",
    "\n",
    "### Filtering average word counts features from the invoice data\n",
    "# Invoice features at day 35\n",
    "df_invoice_features_all_accounts_day_35 = df_periodic_invoice_all_counts[[\n",
    "        'systemid',\n",
    "        'avg_wc_description_day_35',\n",
    "        'avg_wc_notes_day_35',\n",
    "        'avg_wc_terms_day_35',\n",
    "        'avg_wc_address_day_35',\n",
    "        'invoice_count_day_35',\n",
    "        'client_count_day_35',\n",
    "        'is_freshbooks_account_active',\n",
    "        'is_paying',\n",
    "        'base_subscription_amount_first_upgrade',\n",
    "        'upgrade_ever'\n",
    "        ]]\n",
    "\n",
    "# CSV export\n",
    "df_invoice_features_all_accounts_day_35.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_35.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_invoice_features_all_accounts_day_35 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_35.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Merging events' and invoice features\n",
    "df_final_features_day_35 = pd.merge(df_events_imp_features_all_accounts_day_35, \n",
    "                                   df_invoice_features_all_accounts_day_35,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# CSV export \n",
    "df_final_features_day_35.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_35.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_final_features_day_35 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_35.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Final Features Extraction: Day 42 ##############################\n",
    "\n",
    "# Filtered the events columns for day 42 period\n",
    "df_events_all_accounts_day_42 = df_events_all_accounts[['systemid', 'event_count_day_42', 'event_name']]\n",
    "\n",
    "### Pivote the Day 42 Events (Each Unique Event Become a Column)###\n",
    "# Pivot table based on the unique column value in 'event_name'\n",
    "df_events_all_accounts_day_42 = df_events_all_accounts_day_42.pivot_table(values='event_count_day_42', columns='event_name', index='systemid', aggfunc=np.sum,  fill_value=0)\n",
    "\n",
    "# Drop the old column name\n",
    "df_events_all_accounts_day_42.columns.name = None\n",
    "\n",
    "# Reset the index\n",
    "df_events_all_accounts_day_42 = df_events_all_accounts_day_42.reset_index()\n",
    "\n",
    "# Replace 'NaN' with zero\n",
    "df_events_all_accounts_day_42.fillna(0, inplace=True)\n",
    "\n",
    "# CSV export \n",
    "df_events_all_accounts_day_42.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_42.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_events_all_accounts_day_42 = pd.read_csv(\n",
    "#   \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_42.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Importing importing features list\n",
    "important_features = pd.read_csv( \n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/important_features.csv\", sep=\"\\n,\")\n",
    "\n",
    "# Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])\n",
    "\n",
    "# Adding missing important feature column \n",
    "for i in range(len(imp_features_list)):\n",
    "    if imp_features_list[i] in df_events_all_accounts_day_42.columns:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")\n",
    "        df_events_all_accounts_day_42[imp_features_list[i]] = 0\n",
    "\n",
    "\n",
    "# Filtering only important features \n",
    "df_events_imp_features_all_accounts_day_42 = \\\n",
    "            df_events_all_accounts_day_42.loc[:, df_events_all_accounts_day_42.columns.str.contains('|'.join(imp_features_list))]\n",
    "\n",
    "\n",
    "### Filtering average word counts features from the invoice data\n",
    "# Invoice features at day 42\n",
    "df_invoice_features_all_accounts_day_42 = df_periodic_invoice_all_counts[[\n",
    "        'systemid',\n",
    "        'avg_wc_description_day_42',\n",
    "        'avg_wc_notes_day_42',\n",
    "        'avg_wc_terms_day_42',\n",
    "        'avg_wc_address_day_42',\n",
    "        'invoice_count_day_42',\n",
    "        'client_count_day_42',\n",
    "        'is_freshbooks_account_active',\n",
    "        'is_paying',\n",
    "        'base_subscription_amount_first_upgrade',\n",
    "        'upgrade_ever'\n",
    "        ]]\n",
    "\n",
    "# CSV export\n",
    "df_invoice_features_all_accounts_day_42.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_42.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_invoice_features_all_accounts_day_42 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_42.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Merging events' and invoice features\n",
    "df_final_features_day_42 = pd.merge(df_events_imp_features_all_accounts_day_42, \n",
    "                                   df_invoice_features_all_accounts_day_42,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# CSV export \n",
    "df_final_features_day_42.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_42.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_final_features_day_42 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_42.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Final Features Extraction: Day 49 ##############################\n",
    "\n",
    "# Filtered the events columns for day 49 period\n",
    "df_events_all_accounts_day_49 = df_events_all_accounts[['systemid', 'event_count_day_49', 'event_name']]\n",
    "\n",
    "### Pivote the Day 49 Events (Each Unique Event Become a Column)###\n",
    "# Pivot table based on the unique column value in 'event_name'\n",
    "df_events_all_accounts_day_49 = df_events_all_accounts_day_49.pivot_table(values='event_count_day_49', columns='event_name', index='systemid', aggfunc=np.sum,  fill_value=0)\n",
    "\n",
    "# Drop the old column name\n",
    "df_events_all_accounts_day_49.columns.name = None\n",
    "\n",
    "# Reset the index\n",
    "df_events_all_accounts_day_49 = df_events_all_accounts_day_49.reset_index()\n",
    "\n",
    "# Replace 'NaN' with zero\n",
    "df_events_all_accounts_day_49.fillna(0, inplace=True)\n",
    "\n",
    "# CSV export \n",
    "df_events_all_accounts_day_49.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_49.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_events_all_accounts_day_49 = pd.read_csv(\n",
    "#   \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_49.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Importing importing features list\n",
    "important_features = pd.read_csv( \n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/important_features.csv\", sep=\"\\n,\")\n",
    "\n",
    "# Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])\n",
    "\n",
    "# Adding missing important feature column \n",
    "for i in range(len(imp_features_list)):\n",
    "    if imp_features_list[i] in df_events_all_accounts_day_49.columns:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")\n",
    "        df_events_all_accounts_day_49[imp_features_list[i]] = 0\n",
    "\n",
    "\n",
    "# Filtering only important features \n",
    "df_events_imp_features_all_accounts_day_49 = \\\n",
    "            df_events_all_accounts_day_49.loc[:, df_events_all_accounts_day_49.columns.str.contains('|'.join(imp_features_list))]\n",
    "\n",
    "\n",
    "### Filtering average word counts features from the invoice data\n",
    "# Invoice features at day 49\n",
    "df_invoice_features_all_accounts_day_49 = df_periodic_invoice_all_counts[[\n",
    "        'systemid',\n",
    "        'avg_wc_description_day_49',\n",
    "        'avg_wc_notes_day_49',\n",
    "        'avg_wc_terms_day_49',\n",
    "        'avg_wc_address_day_49',\n",
    "        'invoice_count_day_49',\n",
    "        'client_count_day_49',\n",
    "        'is_freshbooks_account_active',\n",
    "        'is_paying',\n",
    "        'base_subscription_amount_first_upgrade',\n",
    "        'upgrade_ever'\n",
    "        ]]\n",
    "\n",
    "# CSV export\n",
    "df_invoice_features_all_accounts_day_49.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_49.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_invoice_features_all_accounts_day_49 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_49.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Merging events' and invoice features\n",
    "df_final_features_day_49 = pd.merge(df_events_imp_features_all_accounts_day_49, \n",
    "                                   df_invoice_features_all_accounts_day_49,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# CSV export \n",
    "df_final_features_day_49.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_49.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_final_features_day_49 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_49.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Final Features Extraction: Day 56 ##############################\n",
    "\n",
    "# Filtered the events columns for day 56 period\n",
    "df_events_all_accounts_day_56 = df_events_all_accounts[['systemid', 'event_count_day_56', 'event_name']]\n",
    "\n",
    "### Pivote the Day 56 Events (Each Unique Event Become a Column)###\n",
    "# Pivot table based on the unique column value in 'event_name'\n",
    "df_events_all_accounts_day_56 = df_events_all_accounts_day_56.pivot_table(values='event_count_day_56', columns='event_name', index='systemid', aggfunc=np.sum,  fill_value=0)\n",
    "\n",
    "# Drop the old column name\n",
    "df_events_all_accounts_day_56.columns.name = None\n",
    "\n",
    "# Reset the index\n",
    "df_events_all_accounts_day_56 = df_events_all_accounts_day_56.reset_index()\n",
    "\n",
    "# Replace 'NaN' with zero\n",
    "df_events_all_accounts_day_56.fillna(0, inplace=True)\n",
    "\n",
    "# CSV export \n",
    "df_events_all_accounts_day_56.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_56.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_events_all_accounts_day_56 = pd.read_csv(\n",
    "#   \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_56.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Importing importing features list\n",
    "important_features = pd.read_csv( \n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/important_features.csv\", sep=\"\\n,\")\n",
    "\n",
    "# Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])\n",
    "\n",
    "# Adding missing important feature column \n",
    "for i in range(len(imp_features_list)):\n",
    "    if imp_features_list[i] in df_events_all_accounts_day_56.columns:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")\n",
    "        df_events_all_accounts_day_56[imp_features_list[i]] = 0\n",
    "\n",
    "\n",
    "# Filtering only important features \n",
    "df_events_imp_features_all_accounts_day_56 = \\\n",
    "            df_events_all_accounts_day_56.loc[:, df_events_all_accounts_day_56.columns.str.contains('|'.join(imp_features_list))]\n",
    "\n",
    "\n",
    "### Filtering average word counts features from the invoice data\n",
    "# Invoice features at day 56\n",
    "df_invoice_features_all_accounts_day_56 = df_periodic_invoice_all_counts[[\n",
    "        'systemid',\n",
    "        'avg_wc_description_day_56',\n",
    "        'avg_wc_notes_day_56',\n",
    "        'avg_wc_terms_day_56',\n",
    "        'avg_wc_address_day_56',\n",
    "        'invoice_count_day_56',\n",
    "        'client_count_day_56',\n",
    "        'is_freshbooks_account_active',\n",
    "        'is_paying',\n",
    "        'base_subscription_amount_first_upgrade',\n",
    "        'upgrade_ever'\n",
    "        ]]\n",
    "\n",
    "# CSV export\n",
    "df_invoice_features_all_accounts_day_56.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_56.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_invoice_features_all_accounts_day_56 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_56.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Merging events' and invoice features\n",
    "df_final_features_day_56 = pd.merge(df_events_imp_features_all_accounts_day_56, \n",
    "                                   df_invoice_features_all_accounts_day_56,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# CSV export \n",
    "df_final_features_day_56.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_56.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_final_features_day_56 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_56.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Final Features Extraction: Day 63 ##############################\n",
    "\n",
    "# Filtered the events columns for day 63 period\n",
    "df_events_all_accounts_day_63 = df_events_all_accounts[['systemid', 'event_count_day_63', 'event_name']]\n",
    "\n",
    "### Pivote the Day 63 Events (Each Unique Event Become a Column)###\n",
    "# Pivot table based on the unique column value in 'event_name'\n",
    "df_events_all_accounts_day_63 = df_events_all_accounts_day_63.pivot_table(values='event_count_day_63', columns='event_name', index='systemid', aggfunc=np.sum,  fill_value=0)\n",
    "\n",
    "# Drop the old column name\n",
    "df_events_all_accounts_day_63.columns.name = None\n",
    "\n",
    "# Reset the index\n",
    "df_events_all_accounts_day_63 = df_events_all_accounts_day_63.reset_index()\n",
    "\n",
    "# Replace 'NaN' with zero\n",
    "df_events_all_accounts_day_63.fillna(0, inplace=True)\n",
    "\n",
    "# CSV export \n",
    "df_events_all_accounts_day_63.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_63.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_events_all_accounts_day_63 = pd.read_csv(\n",
    "#   \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_63.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Importing importing features list\n",
    "important_features = pd.read_csv( \n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/important_features.csv\", sep=\"\\n,\")\n",
    "\n",
    "# Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])\n",
    "\n",
    "# Adding missing important feature column \n",
    "for i in range(len(imp_features_list)):\n",
    "    if imp_features_list[i] in df_events_all_accounts_day_63.columns:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")\n",
    "        df_events_all_accounts_day_63[imp_features_list[i]] = 0\n",
    "\n",
    "\n",
    "# Filtering only important features \n",
    "df_events_imp_features_all_accounts_day_63 = \\\n",
    "            df_events_all_accounts_day_63.loc[:, df_events_all_accounts_day_63.columns.str.contains('|'.join(imp_features_list))]\n",
    "\n",
    "\n",
    "### Filtering average word counts features from the invoice data\n",
    "# Invoice features at day 63\n",
    "df_invoice_features_all_accounts_day_63 = df_periodic_invoice_all_counts[[\n",
    "        'systemid',\n",
    "        'avg_wc_description_day_63',\n",
    "        'avg_wc_notes_day_63',\n",
    "        'avg_wc_terms_day_63',\n",
    "        'avg_wc_address_day_63',\n",
    "        'invoice_count_day_63',\n",
    "        'client_count_day_63',\n",
    "        'is_freshbooks_account_active',\n",
    "        'is_paying',\n",
    "        'base_subscription_amount_first_upgrade',\n",
    "        'upgrade_ever'\n",
    "        ]]\n",
    "\n",
    "# CSV export\n",
    "df_invoice_features_all_accounts_day_63.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_63.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_invoice_features_all_accounts_day_63 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_63.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Merging events' and invoice features\n",
    "df_final_features_day_63 = pd.merge(df_events_imp_features_all_accounts_day_63, \n",
    "                                   df_invoice_features_all_accounts_day_63,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# CSV export \n",
    "df_final_features_day_63.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_63.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_final_features_day_63 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_63.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Final Features Extraction: Day 70 ##############################\n",
    "\n",
    "# Filtered the events columns for day 70 period\n",
    "df_events_all_accounts_day_70 = df_events_all_accounts[['systemid', 'event_count_day_70', 'event_name']]\n",
    "\n",
    "### Pivote the Day 70 Events (Each Unique Event Become a Column)###\n",
    "# Pivot table based on the unique column value in 'event_name'\n",
    "df_events_all_accounts_day_70 = df_events_all_accounts_day_70.pivot_table(values='event_count_day_70', columns='event_name', index='systemid', aggfunc=np.sum,  fill_value=0)\n",
    "\n",
    "# Drop the old column name\n",
    "df_events_all_accounts_day_70.columns.name = None\n",
    "\n",
    "# Reset the index\n",
    "df_events_all_accounts_day_70 = df_events_all_accounts_day_70.reset_index()\n",
    "\n",
    "# Replace 'NaN' with zero\n",
    "df_events_all_accounts_day_70.fillna(0, inplace=True)\n",
    "\n",
    "# CSV export \n",
    "df_events_all_accounts_day_70.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_70.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_events_all_accounts_day_70 = pd.read_csv(\n",
    "#   \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_70.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Importing importing features list\n",
    "important_features = pd.read_csv( \n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/important_features.csv\", sep=\"\\n,\")\n",
    "\n",
    "# Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])\n",
    "\n",
    "# Adding missing important feature column \n",
    "for i in range(len(imp_features_list)):\n",
    "    if imp_features_list[i] in df_events_all_accounts_day_70.columns:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")\n",
    "        df_events_all_accounts_day_70[imp_features_list[i]] = 0\n",
    "\n",
    "\n",
    "# Filtering only important features \n",
    "df_events_imp_features_all_accounts_day_70 = \\\n",
    "            df_events_all_accounts_day_70.loc[:, df_events_all_accounts_day_70.columns.str.contains('|'.join(imp_features_list))]\n",
    "\n",
    "\n",
    "### Filtering average word counts features from the invoice data\n",
    "# Invoice features at day 70\n",
    "df_invoice_features_all_accounts_day_70 = df_periodic_invoice_all_counts[[\n",
    "        'systemid',\n",
    "        'avg_wc_description_day_70',\n",
    "        'avg_wc_notes_day_70',\n",
    "        'avg_wc_terms_day_70',\n",
    "        'avg_wc_address_day_70',\n",
    "        'invoice_count_day_70',\n",
    "        'client_count_day_70',\n",
    "        'is_freshbooks_account_active',\n",
    "        'is_paying',\n",
    "        'base_subscription_amount_first_upgrade',\n",
    "        'upgrade_ever'\n",
    "        ]]\n",
    "\n",
    "# CSV export\n",
    "df_invoice_features_all_accounts_day_70.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_70.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_invoice_features_all_accounts_day_70 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_70.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Merging events' and invoice features\n",
    "df_final_features_day_70 = pd.merge(df_events_imp_features_all_accounts_day_70, \n",
    "                                   df_invoice_features_all_accounts_day_70,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# CSV export \n",
    "df_final_features_day_70.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_70.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_final_features_day_70 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_70.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Final Features Extraction: Day 77 ##############################\n",
    "\n",
    "# Filtered the events columns for day 77 period\n",
    "df_events_all_accounts_day_77 = df_events_all_accounts[['systemid', 'event_count_day_77', 'event_name']]\n",
    "\n",
    "### Pivote the Day 77 Events (Each Unique Event Become a Column)###\n",
    "# Pivot table based on the unique column value in 'event_name'\n",
    "df_events_all_accounts_day_77 = df_events_all_accounts_day_77.pivot_table(values='event_count_day_77', columns='event_name', index='systemid', aggfunc=np.sum,  fill_value=0)\n",
    "\n",
    "# Drop the old column name\n",
    "df_events_all_accounts_day_77.columns.name = None\n",
    "\n",
    "# Reset the index\n",
    "df_events_all_accounts_day_77 = df_events_all_accounts_day_77.reset_index()\n",
    "\n",
    "# Replace 'NaN' with zero\n",
    "df_events_all_accounts_day_77.fillna(0, inplace=True)\n",
    "\n",
    "# CSV export \n",
    "df_events_all_accounts_day_77.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_77.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_events_all_accounts_day_77 = pd.read_csv(\n",
    "#   \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_77.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Importing importing features list\n",
    "important_features = pd.read_csv( \n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/important_features.csv\", sep=\"\\n,\")\n",
    "\n",
    "# Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])\n",
    "\n",
    "# Adding missing important feature column \n",
    "for i in range(len(imp_features_list)):\n",
    "    if imp_features_list[i] in df_events_all_accounts_day_77.columns:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")\n",
    "        df_events_all_accounts_day_77[imp_features_list[i]] = 0\n",
    "\n",
    "\n",
    "# Filtering only important features \n",
    "df_events_imp_features_all_accounts_day_77 = \\\n",
    "            df_events_all_accounts_day_77.loc[:, df_events_all_accounts_day_77.columns.str.contains('|'.join(imp_features_list))]\n",
    "\n",
    "\n",
    "### Filtering average word counts features from the invoice data\n",
    "# Invoice features at day 77\n",
    "df_invoice_features_all_accounts_day_77 = df_periodic_invoice_all_counts[[\n",
    "        'systemid',\n",
    "        'avg_wc_description_day_77',\n",
    "        'avg_wc_notes_day_77',\n",
    "        'avg_wc_terms_day_77',\n",
    "        'avg_wc_address_day_77',\n",
    "        'invoice_count_day_77',\n",
    "        'client_count_day_77',\n",
    "        'is_freshbooks_account_active',\n",
    "        'is_paying',\n",
    "        'base_subscription_amount_first_upgrade',\n",
    "        'upgrade_ever'\n",
    "        ]]\n",
    "\n",
    "# CSV export\n",
    "df_invoice_features_all_accounts_day_77.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_77.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_invoice_features_all_accounts_day_77 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_77.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Merging events' and invoice features\n",
    "df_final_features_day_77 = pd.merge(df_events_imp_features_all_accounts_day_77, \n",
    "                                   df_invoice_features_all_accounts_day_77,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# CSV export \n",
    "df_final_features_day_77.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_77.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_final_features_day_77 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_77.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Final Features Extraction: Day 84 ##############################\n",
    "\n",
    "# Filtered the events columns for day 84 period\n",
    "df_events_all_accounts_day_84 = df_events_all_accounts[['systemid', 'event_count_day_84', 'event_name']]\n",
    "\n",
    "### Pivote the Day 84 Events (Each Unique Event Become a Column)###\n",
    "# Pivot table based on the unique column value in 'event_name'\n",
    "df_events_all_accounts_day_84 = df_events_all_accounts_day_84.pivot_table(values='event_count_day_84', columns='event_name', index='systemid', aggfunc=np.sum,  fill_value=0)\n",
    "\n",
    "# Drop the old column name\n",
    "df_events_all_accounts_day_84.columns.name = None\n",
    "\n",
    "# Reset the index\n",
    "df_events_all_accounts_day_84 = df_events_all_accounts_day_84.reset_index()\n",
    "\n",
    "# Replace 'NaN' with zero\n",
    "df_events_all_accounts_day_84.fillna(0, inplace=True)\n",
    "\n",
    "# CSV export \n",
    "df_events_all_accounts_day_84.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_84.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_events_all_accounts_day_84 = pd.read_csv(\n",
    "#   \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_84.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Importing importing features list\n",
    "important_features = pd.read_csv( \n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/important_features.csv\", sep=\"\\n,\")\n",
    "\n",
    "# Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])\n",
    "\n",
    "# Adding missing important feature column \n",
    "for i in range(len(imp_features_list)):\n",
    "    if imp_features_list[i] in df_events_all_accounts_day_84.columns:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")\n",
    "        df_events_all_accounts_day_84[imp_features_list[i]] = 0\n",
    "\n",
    "\n",
    "# Filtering only important features \n",
    "df_events_imp_features_all_accounts_day_84 = \\\n",
    "            df_events_all_accounts_day_84.loc[:, df_events_all_accounts_day_84.columns.str.contains('|'.join(imp_features_list))]\n",
    "\n",
    "\n",
    "### Filtering average word counts features from the invoice data\n",
    "# Invoice features at day 84\n",
    "df_invoice_features_all_accounts_day_84 = df_periodic_invoice_all_counts[[\n",
    "        'systemid',\n",
    "        'avg_wc_description_day_84',\n",
    "        'avg_wc_notes_day_84',\n",
    "        'avg_wc_terms_day_84',\n",
    "        'avg_wc_address_day_84',\n",
    "        'invoice_count_day_84',\n",
    "        'client_count_day_84',\n",
    "        'is_freshbooks_account_active',\n",
    "        'is_paying',\n",
    "        'base_subscription_amount_first_upgrade',\n",
    "        'upgrade_ever'\n",
    "        ]]\n",
    "\n",
    "# CSV export\n",
    "df_invoice_features_all_accounts_day_84.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_84.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_invoice_features_all_accounts_day_84 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_84.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Merging events' and invoice features\n",
    "df_final_features_day_84 = pd.merge(df_events_imp_features_all_accounts_day_84, \n",
    "                                   df_invoice_features_all_accounts_day_84,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# CSV export \n",
    "df_final_features_day_84.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_84.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_final_features_day_84 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_84.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Final Features Extraction: Day 91 ##############################\n",
    "\n",
    "# Filtered the events columns for day 91 period\n",
    "df_events_all_accounts_day_91 = df_events_all_accounts[['systemid', 'event_count_day_91', 'event_name']]\n",
    "\n",
    "### Pivote the Day 91 Events (Each Unique Event Become a Column)###\n",
    "# Pivot table based on the unique column value in 'event_name'\n",
    "df_events_all_accounts_day_91 = df_events_all_accounts_day_91.pivot_table(values='event_count_day_91', columns='event_name', index='systemid', aggfunc=np.sum,  fill_value=0)\n",
    "\n",
    "# Drop the old column name\n",
    "df_events_all_accounts_day_91.columns.name = None\n",
    "\n",
    "# Reset the index\n",
    "df_events_all_accounts_day_91 = df_events_all_accounts_day_91.reset_index()\n",
    "\n",
    "# Replace 'NaN' with zero\n",
    "df_events_all_accounts_day_91.fillna(0, inplace=True)\n",
    "\n",
    "# CSV export \n",
    "df_events_all_accounts_day_91.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_91.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_events_all_accounts_day_91 = pd.read_csv(\n",
    "#   \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/events_accounts_last_3_months_day_91.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Importing importing features list\n",
    "important_features = pd.read_csv( \n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/important_features.csv\", sep=\"\\n,\")\n",
    "\n",
    "# Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])\n",
    "\n",
    "# Adding missing important feature column \n",
    "for i in range(len(imp_features_list)):\n",
    "    if imp_features_list[i] in df_events_all_accounts_day_91.columns:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")\n",
    "        df_events_all_accounts_day_91[imp_features_list[i]] = 0\n",
    "\n",
    "\n",
    "# Filtering only important features \n",
    "df_events_imp_features_all_accounts_day_91 = \\\n",
    "            df_events_all_accounts_day_91.loc[:, df_events_all_accounts_day_91.columns.str.contains('|'.join(imp_features_list))]\n",
    "\n",
    "\n",
    "### Filtering average word counts features from the invoice data\n",
    "# Invoice features at day 91\n",
    "df_invoice_features_all_accounts_day_91 = df_periodic_invoice_all_counts[[\n",
    "        'systemid',\n",
    "        'avg_wc_description_day_91',\n",
    "        'avg_wc_notes_day_91',\n",
    "        'avg_wc_terms_day_91',\n",
    "        'avg_wc_address_day_91',\n",
    "        'invoice_count_day_91',\n",
    "        'client_count_day_91',\n",
    "        'is_freshbooks_account_active',\n",
    "        'is_paying',\n",
    "        'base_subscription_amount_first_upgrade',\n",
    "        'upgrade_ever'\n",
    "        ]]\n",
    "\n",
    "# CSV export\n",
    "df_invoice_features_all_accounts_day_91.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_91.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_invoice_features_all_accounts_day_91 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_v1/invoice_features_accounts_last_3_months_day_91.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Merging events' and invoice features\n",
    "df_final_features_day_91 = pd.merge(df_events_imp_features_all_accounts_day_91, \n",
    "                                   df_invoice_features_all_accounts_day_91,\n",
    "                                     on='systemid', how='left')\n",
    "\n",
    "# CSV export \n",
    "df_final_features_day_91.to_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_91.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Import CSV\n",
    "# df_final_features_day_91 = pd.read_csv(\n",
    "#     \"/Users/dwahid/Documents/GitHub/fraud_detection/data_final/final_features_accounts_last_3_months_day_91.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
