{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Fraud Risk Score For New Accounts\n",
    "\n",
    "In this script, we will import and compute the Fraud Risk Score (FRC) for any new accounts signup in last 91 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "from scipy import stats\n",
    "get_ipython().magic(u'config IPCompleter.greedy=True')\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import mixture\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Import New Accounts Information ########################################\n",
    "\n",
    "## Needs to run this for each WSD day model\n",
    "%run ./wsm_x_days_test.ipynb   # Day 0X\n",
    "\n",
    "## New user features data file path and details\n",
    "path = \"file-path\"\n",
    "file_name = \"new_users_features_\"\n",
    "today = str(date.today())\n",
    "\n",
    "## User data file names and paths\n",
    "d07 = path + file_name + \"day_07_\" + today + \".tsv\"\n",
    "d14 = path + file_name + \"day_14_\" + today + \".tsv\"\n",
    "d21 = path + file_name + \"day_21_\" + today + \".tsv\"\n",
    "d28 = path + file_name + \"day_28_\" + today + \".tsv\"\n",
    "d35 = path + file_name + \"day_35_\" + today + \".tsv\"\n",
    "d42 = path + file_name + \"day_42_\" + today + \".tsv\"\n",
    "d49 = path + file_name + \"day_49_\" + today + \".tsv\"\n",
    "d56 = path + file_name + \"day_56_\" + today + \".tsv\"\n",
    "d70 = path + file_name + \"day_70_\" + today + \".tsv\"\n",
    "d77 = path + file_name + \"day_77_\" + today + \".tsv\"\n",
    "d84 = path + file_name + \"day_84_\" + today + \".tsv\"\n",
    "d91 = path + file_name + \"day_91_\" + today + \".tsv\"\n",
    "\n",
    "## Import data from the local disk\n",
    "df_07 = pd.read_csv(d07, sep=\"\\t\")\n",
    "df_14 = pd.read_csv(d14, sep=\"\\t\")\n",
    "df_21 = pd.read_csv(d21, sep=\"\\t\")\n",
    "df_28 = pd.read_csv(d28, sep=\"\\t\")\n",
    "df_35 = pd.read_csv(d35, sep=\"\\t\")\n",
    "\n",
    "## Drop rows with NaN\n",
    "df_X = df_X.dropna()\n",
    "\n",
    "## Sorting columns\n",
    "df_X_sort = df_X.reindex(sorted(df_X.columns), axis=1)\n",
    "\n",
    "## Create a copy of the dataframe for analysis\n",
    "df_X_ana = df_X_sort.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data standarization: Mini-Max Scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 07: Data Standarizationå ##############################################\n",
    "\n",
    "# Load standarization parameter from the disk\n",
    "path_minimax_scaler_day_07 = 'mn_saved_d07.sav'\n",
    "min_max_scaler_day_07 = pickle.load(open(path_minimax_scaler_day_07, 'rb'))\n",
    "\n",
    "# List of columns to normalize\n",
    "column_names_to_not_normalize = ['features list']\n",
    "column_names_to_normalize = [x for x in list(df_07_ana) if x not in column_names_to_not_normalize ]\n",
    "\n",
    "# Normalized all features columns except the 'id'\n",
    "x_day_07 = df_07_ana[column_names_to_normalize].values\n",
    "x_scaled_day_07 = min_max_scaler_day_07.fit_transform(x_day_07)\n",
    "df_07_scaled = pd.DataFrame(x_scaled_day_07, columns=column_names_to_normalize, index = df_07_ana.index)\n",
    "df_07_ana[column_names_to_normalize] = df_07_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 14: Data Standarizationå ##############################################\n",
    "\n",
    "# Load standarization parameter from the disk\n",
    "path_minimax_scaler_day_14 = 'mn_saved_d14.sav'\n",
    "min_max_scaler_day_14 = pickle.load(open(path_minimax_scaler_day_14, 'rb'))\n",
    "\n",
    "# List of columns to normalize\n",
    "column_names_to_not_normalize = ['features list']\n",
    "column_names_to_normalize = [x for x in list(df_14_ana) if x not in column_names_to_not_normalize ]\n",
    "\n",
    "# Normalized all features columns except the 'id'\n",
    "x_day_14 = df_14_ana[column_names_to_normalize].values\n",
    "x_scaled_day_14 = min_max_scaler_day_14.fit_transform(x_day_14)\n",
    "df_14_scaled = pd.DataFrame(x_scaled_day_14, columns=column_names_to_normalize, index = df_14_ana.index)\n",
    "df_14_ana[column_names_to_normalize] = df_14_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Get the column index\n",
    "# col_names = list(df_14_scaled)\n",
    "# L = len(col_names)\n",
    "\n",
    "# for i in range(0, L):\n",
    "#     print i, col_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 21: Data Standarizationå ##############################################\n",
    "\n",
    "# Load standarization parameter from the disk\n",
<<<<<<< HEAD
    "path_minimax_scaler_day_21 = '/fraud_detection/src/saved_models/mn_saved_d21.sav'\n",
=======
    "path_minimax_scaler_day_21 = '/saved_models/mn_saved_d21.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "min_max_scaler_day_21 = pickle.load(open(path_minimax_scaler_day_21, 'rb'))\n",
    "\n",
    "# List of columns to normalize\n",
    "column_names_to_not_normalize = ['admin_email', 'days_on_platform', 'effective_date', \n",
    "                                 'is_sales_managed', 'signup_date', 'id']\n",
    "column_names_to_normalize = [x for x in list(df_21_ana) if x not in column_names_to_not_normalize ]\n",
    "\n",
    "# Normalized all features columns except the 'id'\n",
    "x_day_21 = df_21_ana[column_names_to_normalize].values\n",
    "x_scaled_day_21 = min_max_scaler_day_21.fit_transform(x_day_21)\n",
    "df_21_scaled = pd.DataFrame(x_scaled_day_21, columns=column_names_to_normalize, index = df_21_ana.index)\n",
    "df_21_ana[column_names_to_normalize] = df_21_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 28: Data Standarizationå ##############################################\n",
    "\n",
    "# Load standarization parameter from the disk\n",
<<<<<<< HEAD
    "path_minimax_scaler_day_28 = '/fraud_detection/src/saved_models/mn_saved_d28.sav'\n",
=======
    "path_minimax_scaler_day_28 = '/saved_models/mn_saved_d28.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "min_max_scaler_day_28 = pickle.load(open(path_minimax_scaler_day_28, 'rb'))\n",
    "\n",
    "# List of columns to normalize\n",
    "column_names_to_not_normalize = ['admin_email', 'days_on_platform', 'effective_date', \n",
    "                                 'is_sales_managed', 'signup_date', 'id']\n",
    "column_names_to_normalize = [x for x in list(df_28_ana) if x not in column_names_to_not_normalize ]\n",
    "\n",
    "# Normalized all features columns except the 'id'\n",
    "x_day_28 = df_28_ana[column_names_to_normalize].values\n",
    "x_scaled_day_28 = min_max_scaler_day_28.fit_transform(x_day_28)\n",
    "df_28_scaled = pd.DataFrame(x_scaled_day_28, columns=column_names_to_normalize, index = df_28_ana.index)\n",
    "df_28_ana[column_names_to_normalize] = df_28_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the column index\n",
    "# col_names = list(df_28_scaled)\n",
    "# L = len(col_names)\n",
    "\n",
    "# for i in range(0, L):\n",
    "#     print i, col_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 35: Data Standarizationå ##############################################\n",
    "\n",
    "# Load standarization parameter from the disk\n",
<<<<<<< HEAD
    "path_minimax_scaler_day_35 = '/fraud_detection/src/saved_models/mn_saved_d35.sav'\n",
=======
    "path_minimax_scaler_day_35 = '/saved_models/mn_saved_d35.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "min_max_scaler_day_35 = pickle.load(open(path_minimax_scaler_day_35, 'rb'))\n",
    "\n",
    "# List of columns to normalize\n",
    "column_names_to_not_normalize = ['admin_email', 'days_on_platform', 'effective_date', \n",
    "                                 'is_sales_managed', 'signup_date', 'id']\n",
    "column_names_to_normalize = [x for x in list(df_35_ana) if x not in column_names_to_not_normalize ]\n",
    "\n",
    "# Normalized all features columns except the 'id'\n",
    "x_day_35 = df_35_ana[column_names_to_normalize].values\n",
    "x_scaled_day_35 = min_max_scaler_day_35.fit_transform(x_day_35)\n",
    "df_35_scaled = pd.DataFrame(x_scaled_day_35, columns=column_names_to_normalize, index = df_35_ana.index)\n",
    "df_35_ana[column_names_to_normalize] = df_35_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the column index\n",
    "# col_names = list(df_35_scaled)\n",
    "# L = len(col_names)\n",
    "\n",
    "# for i in range(0, L):\n",
    "#     print i, col_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 42: Data Standarizationå ##############################################\n",
    "\n",
    "# Load standarization parameter from the disk\n",
<<<<<<< HEAD
    "path_minimax_scaler_day_42 = '/fraud_detection/src/saved_models/mn_saved_d42.sav'\n",
=======
    "path_minimax_scaler_day_42 = '/saved_models/mn_saved_d42.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "min_max_scaler_day_42 = pickle.load(open(path_minimax_scaler_day_42, 'rb'))\n",
    "\n",
    "# List of columns to normalize\n",
    "column_names_to_not_normalize = ['admin_email', 'days_on_platform', 'effective_date', \n",
    "                                 'is_sales_managed', 'signup_date', 'id']\n",
    "column_names_to_normalize = [x for x in list(df_42_ana) if x not in column_names_to_not_normalize ]\n",
    "\n",
    "# Normalized all features columns except the 'id'\n",
    "x_day_42 = df_42_ana[column_names_to_normalize].values\n",
    "x_scaled_day_42 = min_max_scaler_day_42.fit_transform(x_day_42)\n",
    "df_42_scaled = pd.DataFrame(x_scaled_day_42, columns=column_names_to_normalize, index = df_42_ana.index)\n",
    "df_42_ana[column_names_to_normalize] = df_42_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the column index\n",
    "# col_names = list(df_42_scaled)\n",
    "# L = len(col_names)\n",
    "\n",
    "# for i in range(0, L):\n",
    "#     print i, col_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 49: Data Standarizationå ##############################################\n",
    "\n",
    "# Load standarization parameter from the disk\n",
<<<<<<< HEAD
    "path_minimax_scaler_day_49 = '/fraud_detection/src/saved_models/mn_saved_d49.sav'\n",
=======
    "path_minimax_scaler_day_49 = '/saved_models/mn_saved_d49.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "min_max_scaler_day_49 = pickle.load(open(path_minimax_scaler_day_49, 'rb'))\n",
    "\n",
    "# List of columns to normalize\n",
    "column_names_to_not_normalize = ['admin_email', 'days_on_platform', 'effective_date', \n",
    "                                 'is_sales_managed', 'signup_date', 'id']\n",
    "column_names_to_normalize = [x for x in list(df_49_ana) if x not in column_names_to_not_normalize ]\n",
    "\n",
    "# Normalized all features columns except the 'id'\n",
    "x_day_49 = df_49_ana[column_names_to_normalize].values\n",
    "x_scaled_day_49 = min_max_scaler_day_49.fit_transform(x_day_49)\n",
    "df_49_scaled = pd.DataFrame(x_scaled_day_49, columns=column_names_to_normalize, index = df_49_ana.index)\n",
    "df_49_ana[column_names_to_normalize] = df_49_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 56: Data Standarizationå ##############################################\n",
    "\n",
    "# Load standarization parameter from the disk\n",
<<<<<<< HEAD
    "path_minimax_scaler_day_56 = '/fraud_detection/src/saved_models/mn_saved_d56.sav'\n",
=======
    "path_minimax_scaler_day_56 = '/saved_models/mn_saved_d56.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "min_max_scaler_day_56 = pickle.load(open(path_minimax_scaler_day_56, 'rb'))\n",
    "\n",
    "# List of columns to normalize\n",
    "column_names_to_not_normalize = ['admin_email', 'days_on_platform', 'effective_date', \n",
    "                                 'is_sales_managed', 'signup_date', 'id']\n",
    "column_names_to_normalize = [x for x in list(df_56_ana) if x not in column_names_to_not_normalize ]\n",
    "\n",
    "# Normalized all features columns except the 'id'\n",
    "x_day_56 = df_56_ana[column_names_to_normalize].values\n",
    "x_scaled_day_56 = min_max_scaler_day_56.fit_transform(x_day_56)\n",
    "df_56_scaled = pd.DataFrame(x_scaled_day_56, columns=column_names_to_normalize, index = df_56_ana.index)\n",
    "df_56_ana[column_names_to_normalize] = df_56_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 63: Data Standarizationå ##############################################\n",
    "\n",
    "# Load standarization parameter from the disk\n",
<<<<<<< HEAD
    "path_minimax_scaler_day_63 = '/fraud_detection/src/saved_models/mn_saved_d63.sav'\n",
=======
    "path_minimax_scaler_day_63 = '/saved_models/mn_saved_d63.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "min_max_scaler_day_63 = pickle.load(open(path_minimax_scaler_day_63, 'rb'))\n",
    "\n",
    "# List of columns to normalize\n",
    "column_names_to_not_normalize = ['admin_email', 'days_on_platform', 'effective_date', \n",
    "                                 'is_sales_managed', 'signup_date', 'id']\n",
    "column_names_to_normalize = [x for x in list(df_63_ana) if x not in column_names_to_not_normalize ]\n",
    "\n",
    "# Normalized all features columns except the 'id'\n",
    "x_day_63 = df_63_ana[column_names_to_normalize].values\n",
    "x_scaled_day_63 = min_max_scaler_day_63.fit_transform(x_day_63)\n",
    "df_63_scaled = pd.DataFrame(x_scaled_day_63, columns=column_names_to_normalize, index = df_63_ana.index)\n",
    "df_63_ana[column_names_to_normalize] = df_63_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 70: Data Standarizationå ##############################################\n",
    "\n",
    "# Load standarization parameter from the disk\n",
<<<<<<< HEAD
    "path_minimax_scaler_day_70 = '/fraud_detection/src/saved_models/mn_saved_d70.sav'\n",
=======
    "path_minimax_scaler_day_70 = '/saved_models/mn_saved_d70.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "min_max_scaler_day_70 = pickle.load(open(path_minimax_scaler_day_70, 'rb'))\n",
    "\n",
    "# List of columns to normalize\n",
    "column_names_to_not_normalize = ['admin_email', 'days_on_platform', 'effective_date', \n",
    "                                 'is_sales_managed', 'signup_date', 'id']\n",
    "column_names_to_normalize = [x for x in list(df_70_ana) if x not in column_names_to_not_normalize ]\n",
    "\n",
    "# Normalized all features columns except the 'id'\n",
    "x_day_70 = df_70_ana[column_names_to_normalize].values\n",
    "x_scaled_day_70 = min_max_scaler_day_70.fit_transform(x_day_70)\n",
    "df_70_scaled = pd.DataFrame(x_scaled_day_70, columns=column_names_to_normalize, index = df_70_ana.index)\n",
    "df_70_ana[column_names_to_normalize] = df_70_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the column index\n",
    "# col_names = list(df_70_scaled)\n",
    "# L = len(col_names)\n",
    "\n",
    "# for i in range(0, L):\n",
    "#     print i, col_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 77: Data Standarizationå ##############################################\n",
    "\n",
    "# Load standarization parameter from the disk\n",
<<<<<<< HEAD
    "path_minimax_scaler_day_77 = '/fraud_detection/src/saved_models/mn_saved_d77.sav'\n",
=======
    "path_minimax_scaler_day_77 = '/saved_models/mn_saved_d77.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "min_max_scaler_day_77 = pickle.load(open(path_minimax_scaler_day_77, 'rb'))\n",
    "\n",
    "# List of columns to normalize\n",
    "column_names_to_not_normalize = ['admin_email', 'days_on_platform', 'effective_date', \n",
    "                                 'is_sales_managed', 'signup_date', 'id']\n",
    "column_names_to_normalize = [x for x in list(df_77_ana) if x not in column_names_to_not_normalize ]\n",
    "\n",
    "# Normalized all features columns except the 'id'\n",
    "x_day_77 = df_77_ana[column_names_to_normalize].values\n",
    "x_scaled_day_77 = min_max_scaler_day_77.fit_transform(x_day_77)\n",
    "df_77_scaled = pd.DataFrame(x_scaled_day_77, columns=column_names_to_normalize, index = df_77_ana.index)\n",
    "df_77_ana[column_names_to_normalize] = df_77_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 84: Data Standarizationå ##############################################\n",
    "\n",
    "# Load standarization parameter from the disk\n",
<<<<<<< HEAD
    "path_minimax_scaler_day_84 = '/fraud_detection/src/saved_models/mn_saved_d84.sav'\n",
=======
    "path_minimax_scaler_day_84 = '/saved_models/mn_saved_d84.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "min_max_scaler_day_84 = pickle.load(open(path_minimax_scaler_day_84, 'rb'))\n",
    "\n",
    "# List of columns to normalize\n",
    "column_names_to_not_normalize = ['admin_email', 'days_on_platform', 'effective_date', \n",
    "                                 'is_sales_managed', 'signup_date', 'id']\n",
    "column_names_to_normalize = [x for x in list(df_84_ana) if x not in column_names_to_not_normalize ]\n",
    "\n",
    "# Normalized all features columns except the 'id'\n",
    "x_day_84 = df_84_ana[column_names_to_normalize].values\n",
    "x_scaled_day_84 = min_max_scaler_day_84.fit_transform(x_day_84)\n",
    "df_84_scaled = pd.DataFrame(x_scaled_day_84, columns=column_names_to_normalize, index = df_84_ana.index)\n",
    "df_84_ana[column_names_to_normalize] = df_84_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the column index\n",
    "# col_names = list(df_84_scaled)\n",
    "# L = len(col_names)\n",
    "\n",
    "# for i in range(0, L):\n",
    "#     print i, col_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 91: Data Standarizationå ##############################################\n",
    "\n",
    "# Load standarization parameter from the disk\n",
<<<<<<< HEAD
    "path_minimax_scaler_day_91 = '/fraud_detection/src/saved_models/mn_saved_d91.sav'\n",
=======
    "path_minimax_scaler_day_91 = '/saved_models/mn_saved_d91.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "min_max_scaler_day_91 = pickle.load(open(path_minimax_scaler_day_91, 'rb'))\n",
    "\n",
    "# List of columns to normalize\n",
    "column_names_to_not_normalize = ['admin_email', 'days_on_platform', 'effective_date', \n",
    "                                 'is_sales_managed', 'signup_date', 'id']\n",
    "column_names_to_normalize = [x for x in list(df_91_ana) if x not in column_names_to_not_normalize ]\n",
    "\n",
    "# Normalized all features columns except the 'id'\n",
    "x_day_91 = df_91_ana[column_names_to_normalize].values\n",
    "x_scaled_day_91 = min_max_scaler_day_91.fit_transform(x_day_91)\n",
    "df_91_scaled = pd.DataFrame(x_scaled_day_91, columns=column_names_to_normalize, index = df_91_ana.index)\n",
    "df_91_ana[column_names_to_normalize] = df_91_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the column index\n",
    "# col_names = list(df_91_scaled)\n",
    "# L = len(col_names)\n",
    "\n",
    "# for i in range(0, L):\n",
    "#     print i, col_names[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised (GMM) Clustering\n",
    "### Predicting clusters\n",
    "Now, we will predict clusters corresponding to each new users accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 07: Predicting Clusters ##############################################\n",
    "\n",
    "# Load the trained GMM models from disk\n",
<<<<<<< HEAD
    "path_gmm_day_07 = '/fraud_detection/src/saved_models/fraud_detection_clustering_day_07_k7_model.sav'\n",
=======
    "path_gmm_day_07 = '/fraud_detection_clustering_day_07_k7_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_gmm_day_07 = pickle.load(open(path_gmm_day_07, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "gmm_cluster_id_day_07 = model_gmm_day_07.predict(df_07_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_07['gmm_cluster_id_day_07'] = gmm_cluster_id_day_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the column index\n",
    "# col_names = list(df_07)\n",
    "# L = len(col_names)\n",
    "\n",
    "# for i in range(0, L):\n",
    "#     print i, col_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 14: Predicting Clusters ##############################################\n",
    "\n",
    "# Load the trained GMM models from disk\n",
<<<<<<< HEAD
    "path_gmm_day_14 = '/fraud_detection/src/saved_models/fraud_detection_clustering_day_14_k8_model.sav'\n",
=======
    "path_gmm_day_14 = '/fraud_detection_clustering_day_14_k8_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_gmm_day_14 = pickle.load(open(path_gmm_day_14, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "gmm_cluster_id_day_14 = model_gmm_day_14.predict(df_14_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_14['gmm_cluster_id_day_14'] = gmm_cluster_id_day_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the column index\n",
    "# col_names = list(df_14)\n",
    "# L = len(col_names)\n",
    "\n",
    "# for i in range(0, L):\n",
    "#     print i, col_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 21: Predicting Clusters ##############################################\n",
    "\n",
    "# Load the trained GMM models from disk\n",
<<<<<<< HEAD
    "path_gmm_day_21 = '/fraud_detection/src/saved_models/fraud_detection_clustering_day_21_k6_model.sav'\n",
=======
    "path_gmm_day_21 = '/fraud_detection_clustering_day_21_k6_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_gmm_day_21 = pickle.load(open(path_gmm_day_21, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "gmm_cluster_id_day_21 = model_gmm_day_21.predict(df_21_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_21['gmm_cluster_id_day_21'] = gmm_cluster_id_day_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the column index\n",
    "# col_names = list(df_21)\n",
    "# L = len(col_names)\n",
    "\n",
    "# for i in range(0, L):\n",
    "#     print i, col_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 28: Predicting Clusters ##############################################\n",
    "\n",
    "# Load the trained GMM models from disk\n",
<<<<<<< HEAD
    "path_gmm_day_28 = '/fraud_detection/src/saved_models/fraud_detection_clustering_day_28_k6_model.sav'\n",
=======
    "path_gmm_day_28 = '/fraud_detection_clustering_day_28_k6_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_gmm_day_28 = pickle.load(open(path_gmm_day_28, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "gmm_cluster_id_day_28 = model_gmm_day_28.predict(df_28_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_28['gmm_cluster_id_day_28'] = gmm_cluster_id_day_28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the column index\n",
    "# col_names = list(df_28)\n",
    "# L = len(col_names)\n",
    "\n",
    "# for i in range(0, L):\n",
    "#     print i, col_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 35: Predicting Clusters ##############################################\n",
    "\n",
    "# Load the trained GMM models from disk\n",
<<<<<<< HEAD
    "path_gmm_day_35 = '/fraud_detection/src/saved_models/fraud_detection_clustering_day_35_k8_model.sav'\n",
=======
    "path_gmm_day_35 = '/fraud_detection_clustering_day_35_k8_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_gmm_day_35 = pickle.load(open(path_gmm_day_35, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "gmm_cluster_id_day_35 = model_gmm_day_35.predict(df_35_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_35['gmm_cluster_id_day_35'] = gmm_cluster_id_day_35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 42: Predicting Clusters ##############################################\n",
    "\n",
    "# Load the trained GMM models from disk\n",
<<<<<<< HEAD
    "path_gmm_day_42 = '/fraud_detection/src/saved_models/fraud_detection_clustering_day_42_k7_model.sav'\n",
=======
    "path_gmm_day_42 = '/fraud_detection_clustering_day_42_k7_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_gmm_day_42 = pickle.load(open(path_gmm_day_42, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "gmm_cluster_id_day_42 = model_gmm_day_42.predict(df_42_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_42['gmm_cluster_id_day_42'] = gmm_cluster_id_day_42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 49: Predicting Clusters ##############################################\n",
    "\n",
    "# Load the trained GMM models from disk\n",
<<<<<<< HEAD
    "path_gmm_day_49 = '/fraud_detection/src/saved_models/fraud_detection_clustering_day_49_k6_model.sav'\n",
=======
    "path_gmm_day_49 = '/fraud_detection_clustering_day_49_k6_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_gmm_day_49 = pickle.load(open(path_gmm_day_49, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "gmm_cluster_id_day_49 = model_gmm_day_49.predict(df_49_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_49['gmm_cluster_id_day_49'] = gmm_cluster_id_day_49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 56: Predicting Clusters ##############################################\n",
    "\n",
    "# Load the trained GMM models from disk\n",
<<<<<<< HEAD
    "path_gmm_day_56 = '/fraud_detection/src/saved_models/fraud_detection_clustering_day_56_k8_model.sav'\n",
=======
    "path_gmm_day_56 = '/fraud_detection_clustering_day_56_k8_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_gmm_day_56 = pickle.load(open(path_gmm_day_56, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "gmm_cluster_id_day_56 = model_gmm_day_56.predict(df_56_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_56['gmm_cluster_id_day_56'] = gmm_cluster_id_day_56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the column index\n",
    "# col_names = list(df_56_sort)\n",
    "# L = len(col_names)\n",
    "\n",
    "# for i in range(0, L):\n",
    "#     print i, col_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 63: Predicting Clusters ##############################################\n",
    "\n",
    "# Load the trained GMM models from disk\n",
<<<<<<< HEAD
    "path_gmm_day_63 = '/fraud_detection/src/saved_models/fraud_detection_clustering_day_63_k6_model.sav'\n",
=======
    "path_gmm_day_63 = '/fraud_detection_clustering_day_63_k6_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_gmm_day_63 = pickle.load(open(path_gmm_day_63, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "gmm_cluster_id_day_63 = model_gmm_day_63.predict(df_63_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_63['gmm_cluster_id_day_63'] = gmm_cluster_id_day_63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the column index\n",
    "# col_names = list(df_63)\n",
    "# L = len(col_names)\n",
    "\n",
    "# for i in range(0, L):\n",
    "#     print i, col_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 70: Predicting Clusters ##############################################\n",
    "\n",
    "# Load the trained GMM models from disk\n",
<<<<<<< HEAD
    "path_gmm_day_70 = '/fraud_detection/src/saved_models/fraud_detection_clustering_day_70_k8_model.sav'\n",
=======
    "path_gmm_day_70 = '/fraud_detection_clustering_day_70_k8_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_gmm_day_70 = pickle.load(open(path_gmm_day_70, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "gmm_cluster_id_day_70 = model_gmm_day_70.predict(df_70_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_70['gmm_cluster_id_day_70'] = gmm_cluster_id_day_70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 77: Predicting Clusters ##############################################\n",
    "\n",
    "# Load the trained GMM models from disk\n",
<<<<<<< HEAD
    "path_gmm_day_77 = '/fraud_detection/src/saved_models/fraud_detection_clustering_day_77_k6_model.sav'\n",
=======
    "path_gmm_day_77 = '/fraud_detection_clustering_day_77_k6_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_gmm_day_77 = pickle.load(open(path_gmm_day_77, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "gmm_cluster_id_day_77 = model_gmm_day_77.predict(df_77_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_77['gmm_cluster_id_day_77'] = gmm_cluster_id_day_77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 84: Predicting Clusters ##############################################\n",
    "\n",
    "# Load the trained GMM models from disk\n",
<<<<<<< HEAD
    "path_gmm_day_84 = '/fraud_detection/src/saved_models/fraud_detection_clustering_day_84_k8_model.sav'\n",
=======
    "path_gmm_day_84 = '/fraud_detection_clustering_day_84_k8_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_gmm_day_84 = pickle.load(open(path_gmm_day_84, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "gmm_cluster_id_day_84 = model_gmm_day_84.predict(df_84_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_84['gmm_cluster_id_day_84'] = gmm_cluster_id_day_84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 91: Predicting Clusters ##############################################\n",
    "\n",
    "# Load the trained GMM models from disk\n",
<<<<<<< HEAD
    "path_gmm_day_91 = '/fraud_detection/src/saved_models/fraud_detection_clustering_day_91_k7_model.sav'\n",
=======
    "path_gmm_day_91 = '/fraud_detection_clustering_day_91_k7_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_gmm_day_91 = pickle.load(open(path_gmm_day_91, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "gmm_cluster_id_day_91 = model_gmm_day_91.predict(df_91_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_91['gmm_cluster_id_day_91'] = gmm_cluster_id_day_91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hideOutput": true
   },
   "source": [
    "## Filtering users accounts associated with the fraud clusters\n",
    "\n",
    "From the analysis on the training data, our observation on the fraud risk clusters are in the follwing:\n",
    "- **Day 07 (week 01):** Number of clusters - 6; Fraud risk cluster - C01\n",
    "- **Day 14 (week 02):** Number of clusters - 6; Fraud risk cluster - C01\n",
    "- **Day 21 (week 03):** Number of clusters - 8; Fraud risk cluster - C05\n",
    "- **Day 28 (week 04):** Number of clusters - 8; Fraud risk cluster - C05\n",
    "- **Day 35 (week 05):** Number of clusters - 7; Fraud risk cluster - C05\n",
    "- **Day 42 (week 06):** Number of clusters - 6; Fraud risk cluster - C01\n",
    "- **Day 49 (week 07):** Number of clusters - 8; Fraud risk cluster - C02\n",
    "- **Day 56 (week 08):** Number of clusters - 7; Fraud risk cluster - C05\n",
    "- **Day 63 (week 09):** Number of clusters - 7; Fraud risk cluster - C05\n",
    "- **Day 70 (week 10):** Number of clusters - 8; Fraud risk cluster - C07\n",
    "- **Day 77 (week 11):** Number of clusters - 6; Fraud risk cluster - C01\n",
    "- **Day 84 (week 12):** Number of clusters - 6; Fraud risk cluster - C01\n",
    "- **Day 91 (week 13):** Number of clusters - 7; Fraud risk cluster - C05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Day 07: Filtering Users Accounts Associated With Fraud Clusters #############\n",
    "\n",
    "# The only the accounts labeled as cluster c01 (fraud cluster)\n",
    "df_07_gmm_c01 = df_07[df_07.gmm_cluster_id_day_07 == 1]\n",
    "\n",
    "df_07_gmm_c01 = df_07_gmm_c01.reset_index() # resetting index\n",
    "df_07_gmm_c01 = df_07_gmm_c01.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_07_gmm_c01.head()\n",
    "# df_07_gmm_c01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Day 14: Filtering Users Accounts Associated With Fraud Clusters #############\n",
    "\n",
    "\n",
    "# The only the accounts labeled as cluster c07 (fraud cluster)\n",
    "df_14_gmm_c07 = df_14[df_14.gmm_cluster_id_day_14 == 7]\n",
    "\n",
    "df_14_gmm_c07 = df_14_gmm_c07.reset_index() # resetting index\n",
    "df_14_gmm_c07 = df_14_gmm_c07.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_14_gmm_c07.head()\n",
    "# df_14_gmm_c07.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Day 21: Filtering Users Accounts Associated With Fraud Clusters #############\n",
    "\n",
    "# The only the accounts labeled as cluster c05 (fraud cluster)\n",
    "df_21_gmm_c05 = df_21[df_21.gmm_cluster_id_day_21 == 5]\n",
    "\n",
    "df_21_gmm_c05 = df_21_gmm_c05.reset_index() # resetting index\n",
    "df_21_gmm_c05 = df_21_gmm_c05.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_21_gmm_c05.head()\n",
    "# df_21_gmm_c05.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Day 28: Filtering Users Accounts Associated With Fraud Clusters #############\n",
    "\n",
    "# The only the accounts labeled as cluster c05 (fraud cluster)\n",
    "df_28_gmm_c05 = df_28[df_28.gmm_cluster_id_day_28 == 5]\n",
    "\n",
    "df_28_gmm_c05 = df_28_gmm_c05.reset_index() # resetting index\n",
    "df_28_gmm_c05 = df_28_gmm_c05.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Day 35: Filtering Users Accounts Associated With Fraud Clusters #############\n",
    "\n",
    "# The only the accounts labeled as cluster c05 (fraud cluster)\n",
    "df_35_gmm_c05 = df_35[df_35.gmm_cluster_id_day_35 == 5]\n",
    "\n",
    "df_35_gmm_c05 = df_35_gmm_c05.reset_index() # resetting index\n",
    "df_35_gmm_c05 = df_35_gmm_c05.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Day 42: Filtering Users Accounts Associated With Fraud Clusters #############\n",
    "\n",
    "# The only the accounts labeled as cluster c05 (fraud cluster)\n",
    "df_42_gmm_c05 = df_42[df_42.gmm_cluster_id_day_42 == 5]\n",
    "\n",
    "df_42_gmm_c05 = df_42_gmm_c05.reset_index() # resetting index\n",
    "df_42_gmm_c05 = df_42_gmm_c05.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Day 49: Filtering Users Accounts Associated With Fraud Clusters #############\n",
    "\n",
    "# The only the accounts labeled as cluster c01 (fraud cluster)\n",
    "df_49_gmm_c01 = df_49[df_49.gmm_cluster_id_day_49 == 1]\n",
    "\n",
    "df_49_gmm_c01 = df_49_gmm_c01.reset_index() # resetting index\n",
    "df_49_gmm_c01 = df_49_gmm_c01.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Day 56: Filtering Users Accounts Associated With Fraud Clusters #############\n",
    "\n",
    "# The only the accounts labeled as cluster c05 (fraud cluster)\n",
    "df_56_gmm_c05 = df_56[df_56.gmm_cluster_id_day_56 == 5]\n",
    "\n",
    "df_56_gmm_c05 = df_56_gmm_c05.reset_index() # resetting index\n",
    "df_56_gmm_c05 = df_56_gmm_c05.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Day 63: Filtering Users Accounts Associated With Fraud Clusters #############\n",
    "\n",
    "# The only the accounts labeled as cluster c04 (fraud cluster)\n",
    "df_63_gmm_c04 = df_63[df_63.gmm_cluster_id_day_63 == 4]\n",
    "\n",
    "df_63_gmm_c04 = df_63_gmm_c04.reset_index() # resetting index\n",
    "df_63_gmm_c04 = df_63_gmm_c04.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Day 70: Filtering Users Accounts Associated With Fraud Clusters #############\n",
    "\n",
    "# The only the accounts labeled as cluster c07 (fraud cluster)\n",
    "df_70_gmm_c07 = df_70[df_70.gmm_cluster_id_day_70 == 7]\n",
    "\n",
    "df_70_gmm_c07 = df_70_gmm_c07.reset_index() # resetting index\n",
    "df_70_gmm_c07 = df_70_gmm_c07.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Day 77: Filtering Users Accounts Associated With Fraud Clusters #############\n",
    "\n",
    "# The only the accounts labeled as cluster c01 (fraud cluster)\n",
    "df_77_gmm_c01 = df_77[df_77.gmm_cluster_id_day_77 == 1]\n",
    "\n",
    "df_77_gmm_c01 = df_77_gmm_c01.reset_index() # resetting index\n",
    "df_77_gmm_c01 = df_77_gmm_c01.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Day 84: Filtering Users Accounts Associated With Fraud Clusters #############\n",
    "\n",
    "# The only the accounts labeled as cluster c04 (fraud cluster)\n",
    "df_84_gmm_c04 = df_84[df_84.gmm_cluster_id_day_84 == 4]\n",
    "\n",
    "df_84_gmm_c04 = df_84_gmm_c04.reset_index() # resetting index\n",
    "df_84_gmm_c04 = df_84_gmm_c04.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Day 91: Filtering Users Accounts Associated With Fraud Clusters #############\n",
    "\n",
    "# The only the accounts labeled as cluster c01 (fraud cluster)\n",
    "df_91_gmm_c01 = df_91[df_91.gmm_cluster_id_day_91 == 1]\n",
    "\n",
    "df_91_gmm_c01 = df_91_gmm_c01.reset_index() # resetting index\n",
    "df_91_gmm_c01 = df_91_gmm_c01.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network (NN) Classifier\n",
    "### Predicting Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 07: NN Classifier  ##################################################\n",
    "\n",
    "# Load the trained GMM models from disk (pikled file)\n",
<<<<<<< HEAD
    "path_nn_day_07 = '/fraud_detection/src/saved_models/fraud_detection_nn_classifier_day_07_k7_model.sav'\n",
=======
    "path_nn_day_07 = '/fraud_detection_nn_classifier_day_07_k7_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_nn_day_07 = pickle.load(open(path_nn_day_07, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "nn_classifier_id_day_07 = model_nn_day_07.predict(df_07_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_07['nn_classifier_id_day_07'] = nn_classifier_id_day_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_07.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 14: NN Classifier  ##################################################\n",
    "\n",
    "# Load the trained GMM models from disk (pikled file)\n",
<<<<<<< HEAD
    "path_nn_day_14 = '/fraud_detection/src/saved_models/fraud_detection_nn_classifier_day_14_k8_model.sav'\n",
=======
    "path_nn_day_14 = '/fraud_detection_nn_classifier_day_14_k8_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_nn_day_14 = pickle.load(open(path_nn_day_14, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "nn_classifier_id_day_14 = model_nn_day_14.predict(df_14_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_14['nn_classifier_id_day_14'] = nn_classifier_id_day_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_14.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 21: NN Classifier  ##################################################\n",
    "\n",
    "# Load the trained GMM models from disk (pikled file)\n",
<<<<<<< HEAD
    "path_nn_day_21 = '/fraud_detection/src/saved_models/fraud_detection_nn_classifier_day_21_k6_model.sav'\n",
=======
    "path_nn_day_21 = '/fraud_detection_nn_classifier_day_21_k6_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_nn_day_21 = pickle.load(open(path_nn_day_21, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "nn_classifier_id_day_21 = model_nn_day_21.predict(df_21_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_21['nn_classifier_id_day_21'] = nn_classifier_id_day_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_21.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 28: NN Classifier  ##################################################\n",
    "\n",
    "# Load the trained GMM models from disk (pikled file)\n",
<<<<<<< HEAD
    "path_nn_day_28 = '/fraud_detection/src/saved_models/fraud_detection_nn_classifier_day_28_k6_model.sav'\n",
=======
    "path_nn_day_28 = '/fraud_detection_nn_classifier_day_28_k6_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_nn_day_28 = pickle.load(open(path_nn_day_28, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "nn_classifier_id_day_28 = model_nn_day_28.predict(df_28_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_28['nn_classifier_id_day_28'] = nn_classifier_id_day_28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 35: NN Classifier  ##################################################\n",
    "\n",
    "# Load the trained GMM models from disk (pikled file)\n",
<<<<<<< HEAD
    "path_nn_day_35 = '/fraud_detection/src/saved_models/fraud_detection_nn_classifier_day_35_k8_model.sav'\n",
=======
    "path_nn_day_35 = '/fraud_detection_nn_classifier_day_35_k8_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_nn_day_35 = pickle.load(open(path_nn_day_35, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "nn_classifier_id_day_35 = model_nn_day_35.predict(df_35_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_35['nn_classifier_id_day_35'] = nn_classifier_id_day_35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 42: NN Classifier  ##################################################\n",
    "\n",
    "# Load the trained GMM models from disk (pikled file)\n",
<<<<<<< HEAD
    "path_nn_day_42 = '/fraud_detection/src/saved_models/fraud_detection_nn_classifier_day_42_k7_model.sav'\n",
=======
    "path_nn_day_42 = '/fraud_detection_nn_classifier_day_42_k7_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_nn_day_42 = pickle.load(open(path_nn_day_42, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "nn_classifier_id_day_42 = model_nn_day_42.predict(df_42_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_42['nn_classifier_id_day_42'] = nn_classifier_id_day_42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 49: NN Classifier  ##################################################\n",
    "\n",
    "# Load the trained GMM models from disk (pikled file)\n",
<<<<<<< HEAD
    "path_nn_day_49 = '/fraud_detection/src/saved_models/fraud_detection_nn_classifier_day_49_k6_model.sav'\n",
=======
    "path_nn_day_49 = '/fraud_detection_nn_classifier_day_49_k6_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_nn_day_49 = pickle.load(open(path_nn_day_49, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "nn_classifier_id_day_49 = model_nn_day_49.predict(df_49_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_49['nn_classifier_id_day_49'] = nn_classifier_id_day_49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 56: NN Classifier  ##################################################\n",
    "\n",
    "# Load the trained GMM models from disk (pikled file)\n",
<<<<<<< HEAD
    "path_nn_day_56 = '/fraud_detection/src/saved_models/fraud_detection_nn_classifier_day_56_k8_model.sav'\n",
=======
    "path_nn_day_56 = '/fraud_detection_nn_classifier_day_56_k8_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_nn_day_56 = pickle.load(open(path_nn_day_56, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "nn_classifier_id_day_56 = model_nn_day_56.predict(df_56_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_56['nn_classifier_id_day_56'] = nn_classifier_id_day_56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 63: NN Classifier  ##################################################\n",
    "\n",
    "# Load the trained GMM models from disk (pikled file)\n",
<<<<<<< HEAD
    "path_nn_day_63 = '/fraud_detection/src/saved_models/fraud_detection_nn_classifier_day_63_k6_model.sav'\n",
=======
    "path_nn_day_63 = '/fraud_detection_nn_classifier_day_63_k6_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_nn_day_63 = pickle.load(open(path_nn_day_63, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "nn_classifier_id_day_63 = model_nn_day_63.predict(df_63_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_63['nn_classifier_id_day_63'] = nn_classifier_id_day_63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 70: NN Classifier  ##################################################\n",
    "\n",
    "# Load the trained GMM models from disk (pikled file)\n",
<<<<<<< HEAD
    "path_nn_day_70 = '/fraud_detection/src/saved_models/fraud_detection_nn_classifier_day_70_k8_model.sav'\n",
=======
    "path_nn_day_70 = '/fraud_detection_nn_classifier_day_70_k8_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_nn_day_70 = pickle.load(open(path_nn_day_70, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "nn_classifier_id_day_70 = model_nn_day_70.predict(df_70_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_70['nn_classifier_id_day_70'] = nn_classifier_id_day_70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 77: NN Classifier  ##################################################\n",
    "\n",
    "# Load the trained GMM models from disk (pikled file)\n",
<<<<<<< HEAD
    "path_nn_day_77 = '/fraud_detection/src/saved_models/fraud_detection_nn_classifier_day_77_k6_model.sav'\n",
=======
    "path_nn_day_77 = '/fraud_detection_nn_classifier_day_77_k6_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_nn_day_77 = pickle.load(open(path_nn_day_77, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "nn_classifier_id_day_77 = model_nn_day_77.predict(df_77_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_77['nn_classifier_id_day_77'] = nn_classifier_id_day_77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 84: NN Classifier  ##################################################\n",
    "\n",
    "# Load the trained GMM models from disk (pikled file)\n",
<<<<<<< HEAD
    "path_nn_day_84 = '/fraud_detection/src/saved_models/fraud_detection_nn_classifier_day_84_k8_model.sav'\n",
=======
    "path_nn_day_84 = '/fraud_detection_nn_classifier_day_84_k8_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_nn_day_84 = pickle.load(open(path_nn_day_84, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "nn_classifier_id_day_84 = model_nn_day_84.predict(df_84_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_84['nn_classifier_id_day_84'] = nn_classifier_id_day_84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Day 91: NN Classifier  ##################################################\n",
    "\n",
    "# Load the trained GMM models from disk (pikled file)\n",
<<<<<<< HEAD
    "path_nn_day_91 = '/fraud_detection/src/saved_models/fraud_detection_nn_classifier_day_91_k7_model.sav'\n",
=======
    "path_nn_day_91 = '/fraud_detection_nn_classifier_day_91_k7_model.sav'\n",
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
    "model_nn_day_91 = pickle.load(open(path_nn_day_91, 'rb'))\n",
    "\n",
    "# Predicting clustering\n",
    "nn_classifier_id_day_91 = model_nn_day_91.predict(df_91_scaled)\n",
    "\n",
    "# Adding clusters id of each account to the dataframe\n",
    "df_91['nn_classifier_id_day_91'] = nn_classifier_id_day_91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering users accounts associated with the fraud clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only the accounts labeled as cluster c01 (fraud cluster)\n",
    "df_07_nn_c01 = df_07[df_07.nn_classifier_id_day_07 == 1]\n",
    "\n",
    "## Re-indexing\n",
    "df_07_nn_c01 = df_07_nn_c01.reset_index() # resetting index\n",
    "df_07_nn_c01 = df_07_nn_c01.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_07_nn_c01.tail()\n",
    "# df_07_nn_c01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only the accounts labeled as cluster c07 (fraud cluster)\n",
    "df_14_nn_c07 = df_14[df_14.nn_classifier_id_day_14 == 7]\n",
    "\n",
    "## Re-indexing\n",
    "df_14_nn_c07 = df_14_nn_c07.reset_index() # resetting index\n",
    "df_14_nn_c07 = df_14_nn_c07.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_14_nn_c07.tail()\n",
    "# df_14_nn_c07.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only the accounts labeled as cluster c05 (fraud cluster)\n",
    "df_21_nn_c05 = df_21[df_21.nn_classifier_id_day_21 == 5]\n",
    "\n",
    "## Re-indexing\n",
    "df_21_nn_c05 = df_21_nn_c05.reset_index() # resetting index\n",
    "df_21_nn_c05 = df_21_nn_c05.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_21_nn_c05.tail()\n",
    "# df_21_nn_c05.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only the accounts labeled as cluster c05 (fraud cluster)\n",
    "df_28_nn_c05 = df_28[df_28.nn_classifier_id_day_28 == 5]\n",
    " \n",
    "## Re-indexing\n",
    "df_28_nn_c05 = df_28_nn_c05.reset_index() # resetting index\n",
    "df_28_nn_c05 = df_28_nn_c05.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only the accounts labeled as cluster c01 (fraud cluster)\n",
    "df_35_nn_c05 = df_35[df_35.nn_classifier_id_day_35 == 5]\n",
    " \n",
    "## Re-indexing\n",
    "df_35_nn_c05 = df_35_nn_c05.reset_index() # resetting index\n",
    "df_35_nn_c05 = df_35_nn_c05.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only the accounts labeled as cluster c05 (fraud cluster)\n",
    "df_42_nn_c05 = df_42[df_42.nn_classifier_id_day_42 == 5]\n",
    " \n",
    "## Re-indexing\n",
    "df_42_nn_c05 = df_42_nn_c05.reset_index() # resetting index\n",
    "df_42_nn_c05 = df_42_nn_c05.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The only the accounts labeled as cluster c01 (fraud cluster)\n",
    "df_49_nn_c01 = df_49[df_49.nn_classifier_id_day_49 == 1]\n",
    " \n",
    "## Re-indexing\n",
    "df_49_nn_c01 = df_49_nn_c01.reset_index() # resetting index\n",
    "df_49_nn_c01 = df_49_nn_c01.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The only the accounts labeled as cluster c05 (fraud cluster)\n",
    "df_56_nn_c05 = df_56[df_56.nn_classifier_id_day_56 == 5]\n",
    " \n",
    "## Re-indexing\n",
    "df_56_nn_c05 = df_56_nn_c05.reset_index() # resetting index\n",
    "df_56_nn_c05 = df_56_nn_c05.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The only the accounts labeled as cluster c04 (fraud cluster)\n",
    "df_63_nn_c04 = df_63[df_63.nn_classifier_id_day_63 == 4]\n",
    " \n",
    "## Re-indexing\n",
    "df_63_nn_c04 = df_63_nn_c04.reset_index() # resetting index\n",
    "df_63_nn_c04 = df_63_nn_c04.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The only the accounts labeled as cluster c07 (fraud cluster)\n",
    "df_70_nn_c07 = df_70[df_70.nn_classifier_id_day_70 == 7]\n",
    " \n",
    "## Re-indexing\n",
    "df_70_nn_c07 = df_70_nn_c07.reset_index() # resetting index\n",
    "df_70_nn_c07 = df_70_nn_c07.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The only the accounts labeled as cluster c01 (fraud cluster)\n",
    "df_77_nn_c01 = df_77[df_77.nn_classifier_id_day_77 == 1]\n",
    " \n",
    "## Re-indexing\n",
    "df_77_nn_c01 = df_77_nn_c01.reset_index() # resetting index\n",
    "df_77_nn_c01 = df_77_nn_c01.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The only the accounts labeled as cluster c04 (fraud cluster)\n",
    "df_84_nn_c04 = df_84[df_84.nn_classifier_id_day_84 == 4]\n",
    " \n",
    "## Re-indexing\n",
    "df_84_nn_c04 = df_84_nn_c04.reset_index() # resetting index\n",
    "df_84_nn_c04 = df_84_nn_c04.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The only the accounts labeled as cluster c01 (fraud cluster)\n",
    "df_91_nn_c01 = df_91[df_91.nn_classifier_id_day_91 == 1]\n",
    " \n",
    "## Re-indexing\n",
    "df_91_nn_c01 = df_91_nn_c01.reset_index() # resetting index\n",
    "df_91_nn_c01 = df_91_nn_c01.drop(columns=['index'], axis=1) # droping adding columns during the resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging GMM Cluster and NN Class: For Each Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Day 07: Merging GMM and NN Class ##############################################\n",
    "\n",
    "## Droping the cluster and class id lable columns\n",
    "df_07_gmm_c01 = df_07_gmm_c01.drop(columns=['gmm_cluster_id_day_07'], axis=1)\n",
    "df_07_nn_c01_id = pd.DataFrame(df_07_nn_c01['id'])\n",
    "\n",
    "## Merge two dataframes based on the 'id'\n",
    "df_07_c01 = pd.merge(df_07_gmm_c01, df_07_nn_c01_id, how='inner', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_07_c01.iloc[16]['client_count_day_7']\n",
    "# list(df_07_c01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Day 14: Merging GMM and NN Class ##############################################\n",
    "\n",
    "## Droping the cluster and class id lable columns\n",
    "df_14_gmm_c07 = df_14_gmm_c07.drop(columns=['gmm_cluster_id_day_14'], axis=1)\n",
    "df_14_nn_c07_id = pd.DataFrame(df_14_nn_c07['id'])\n",
    "\n",
    "## Merge two dataframes based on the 'id'\n",
    "df_14_c07 = pd.merge(df_14_gmm_c07, df_14_nn_c07_id, how='inner', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Day 21: Merging GMM and NN Class ##############################################\n",
    "\n",
    "## Droping the cluster and class id lable columns\n",
    "df_21_gmm_c05 = df_21_gmm_c05.drop(columns=['gmm_cluster_id_day_21'], axis=1)\n",
    "df_21_nn_c05_id = pd.DataFrame(df_21_nn_c05['id'])\n",
    "\n",
    "## Merge two dataframes based on the 'id'\n",
    "df_21_c05 = pd.merge(df_21_gmm_c05, df_21_nn_c05_id, how='inner', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Day 28: Merging GMM and NN Class ##############################################\n",
    "\n",
    "## Droping the cluster and class id lable columns\n",
    "df_28_gmm_c05 = df_28_gmm_c05.drop(columns=['gmm_cluster_id_day_28'], axis=1)\n",
    "df_28_nn_c05_id = pd.DataFrame(df_28_nn_c05['id'])\n",
    "\n",
    "## Merge two dataframes based on the 'id'\n",
    "df_28_c05 = pd.merge(df_28_gmm_c05, df_28_nn_c05_id, how='inner', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df_28_c05)\n",
    "# df_28_c05.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Day 35: Merging GMM and NN Class ##############################################\n",
    "\n",
    "## Droping the cluster and class id lable columns\n",
    "df_35_gmm_c05 = df_35_gmm_c05.drop(columns=['gmm_cluster_id_day_35'], axis=1)\n",
    "df_35_nn_c05_id = pd.DataFrame(df_35_nn_c05['id'])\n",
    "\n",
    "## Merge two dataframes based on the 'id'\n",
    "df_35_c05 = pd.merge(df_35_gmm_c05, df_35_nn_c05_id, how='inner', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Day 42: Merging GMM and NN Class ##############################################\n",
    "\n",
    "## Droping the cluster and class id lable columns\n",
    "df_42_gmm_c05 = df_42_gmm_c05.drop(columns=['gmm_cluster_id_day_42'], axis=1)\n",
    "df_42_nn_c05_id = pd.DataFrame(df_42_nn_c05['id'])\n",
    "\n",
    "## Merge two dataframes based on the 'id'\n",
    "df_42_c05 = pd.merge(df_42_gmm_c05, df_42_nn_c05_id, how='inner', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Day 49: Merging GMM and NN Class ##############################################\n",
    "\n",
    "## Droping the cluster and class id lable columns\n",
    "df_49_gmm_c01 = df_49_gmm_c01.drop(columns=['gmm_cluster_id_day_49'], axis=1)\n",
    "df_49_nn_c01_id = pd.DataFrame(df_49_nn_c01['id'])\n",
    "\n",
    "## Merge two dataframes based on the 'id'\n",
    "df_49_c01 = pd.merge(df_49_gmm_c01, df_49_nn_c01_id, how='inner', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Day 56: Merging GMM and NN Class ##############################################\n",
    "\n",
    "## Droping the cluster and class id lable columns\n",
    "df_56_gmm_c05 = df_56_gmm_c05.drop(columns=['gmm_cluster_id_day_56'], axis=1)\n",
    "df_56_nn_c05_id = pd.DataFrame(df_56_nn_c05['id'])\n",
    "\n",
    "## Merge two dataframes based on the 'id'\n",
    "df_56_c05 = pd.merge(df_56_gmm_c05, df_56_nn_c05_id, how='inner', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Day 63: Merging GMM and NN Class ##############################################\n",
    "\n",
    "## Droping the cluster and class id lable columns\n",
    "df_63_gmm_c04 = df_63_gmm_c04.drop(columns=['gmm_cluster_id_day_63'], axis=1)\n",
    "df_63_nn_c04_id = pd.DataFrame(df_63_nn_c04['id'])\n",
    "\n",
    "## Merge two dataframes based on the 'id'\n",
    "df_63_c04 = pd.merge(df_63_gmm_c04, df_63_nn_c04_id, how='inner', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Day 70: Merging GMM and NN Class ##############################################\n",
    "\n",
    "## Droping the cluster and class id lable columns\n",
    "df_70_gmm_c07 = df_70_gmm_c07.drop(columns=['gmm_cluster_id_day_70'], axis=1)\n",
    "df_70_nn_c07_id = pd.DataFrame(df_70_nn_c07['id'])\n",
    "\n",
    "## Merge two dataframes based on the 'id'\n",
    "df_70_c07 = pd.merge(df_70_gmm_c07, df_70_nn_c07_id, how='inner', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Day 77: Merging GMM and NN Class ##############################################\n",
    "\n",
    "## Droping the cluster and class id lable columns\n",
    "df_77_gmm_c01 = df_77_gmm_c01.drop(columns=['gmm_cluster_id_day_77'], axis=1)\n",
    "df_77_nn_c01_id = pd.DataFrame(df_77_nn_c01['id'])\n",
    "\n",
    "## Merge two dataframes based on the 'id'\n",
    "df_77_c01 = pd.merge(df_77_gmm_c01, df_77_nn_c01_id, how='inner', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Day 84: Merging GMM and NN Class ##############################################\n",
    "\n",
    "## Droping the cluster and class id lable columns\n",
    "df_84_gmm_c04 = df_84_gmm_c04.drop(columns=['gmm_cluster_id_day_84'], axis=1)\n",
    "df_84_nn_c04_id = pd.DataFrame(df_84_nn_c04['id'])\n",
    "\n",
    "## Merge two dataframes based on the 'id'\n",
    "df_84_c04 = pd.merge(df_84_gmm_c04, df_84_nn_c04_id, how='inner', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Day 91: Merging GMM and NN Class ##############################################\n",
    "\n",
    "## Droping the cluster and class id lable columns\n",
    "df_91_gmm_c01 = df_91_gmm_c01.drop(columns=['gmm_cluster_id_day_91'], axis=1)\n",
    "df_91_nn_c01_id = pd.DataFrame(df_91_nn_c01['id'])\n",
    "\n",
    "## Merge two dataframes based on the 'id'\n",
    "df_91_c01 = pd.merge(df_91_gmm_c01, df_91_nn_c01_id, how='inner', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming Columns in the Weekly FRC DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Day 07: Renaming Columns ########################################################\n",
    "\n",
    "df_07_frc = df_07_c01.rename(columns={\"avg_wc_address_day_7\": \"avg_wc_address\", \n",
    "                                              \"avg_wc_description_day_7\": \"avg_wc_description\",\n",
    "                                              \"avg_wc_notes_day_7\": \"avg_wc_notes\",\n",
    "                                              \"avg_wc_terms_day_7\": \"avg_wc_terms\",\n",
    "                                              \"client_count_day_7\": \"client_count\",\n",
    "                                              \"invoice_count_day_7\": \"invoice_count\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Day 14: Renaming Columns ########################################################\n",
    "\n",
    "df_14_frc = df_14_c07.rename(columns={\"avg_wc_address_day_14\": \"avg_wc_address\", \n",
    "                                              \"avg_wc_description_day_14\": \"avg_wc_description\",\n",
    "                                              \"avg_wc_notes_day_14\": \"avg_wc_notes\",\n",
    "                                              \"avg_wc_terms_day_14\": \"avg_wc_terms\",\n",
    "                                              \"client_count_day_14\": \"client_count\",\n",
    "                                              \"invoice_count_day_14\": \"invoice_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Day 21: Renaming Columns ########################################################\n",
    "\n",
    "df_21_frc = df_21_c05.rename(columns={\"avg_wc_address_day_21\": \"avg_wc_address\", \n",
    "                                              \"avg_wc_description_day_21\": \"avg_wc_description\",\n",
    "                                              \"avg_wc_notes_day_21\": \"avg_wc_notes\",\n",
    "                                              \"avg_wc_terms_day_21\": \"avg_wc_terms\",\n",
    "                                              \"client_count_day_21\": \"client_count\",\n",
    "                                              \"invoice_count_day_21\": \"invoice_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Day 28: Renaming Columns ########################################################\n",
    "\n",
    "df_28_frc = df_28_c05.rename(columns={\"avg_wc_address_day_28\": \"avg_wc_address\", \n",
    "                                              \"avg_wc_description_day_28\": \"avg_wc_description\",\n",
    "                                              \"avg_wc_notes_day_28\": \"avg_wc_notes\",\n",
    "                                              \"avg_wc_terms_day_28\": \"avg_wc_terms\",\n",
    "                                              \"client_count_day_28\": \"client_count\",\n",
    "                                              \"invoice_count_day_28\": \"invoice_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Day 35: Renaming Columns ########################################################\n",
    "\n",
    "df_35_frc = df_35_c05.rename(columns={\"avg_wc_address_day_35\": \"avg_wc_address\", \n",
    "                                              \"avg_wc_description_day_35\": \"avg_wc_description\",\n",
    "                                              \"avg_wc_notes_day_35\": \"avg_wc_notes\",\n",
    "                                              \"avg_wc_terms_day_35\": \"avg_wc_terms\",\n",
    "                                              \"client_count_day_35\": \"client_count\",\n",
    "                                              \"invoice_count_day_35\": \"invoice_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Day 42: Renaming Columns ########################################################\n",
    "\n",
    "df_42_frc = df_42_c05.rename(columns={\"avg_wc_address_day_42\": \"avg_wc_address\", \n",
    "                                              \"avg_wc_description_day_42\": \"avg_wc_description\",\n",
    "                                              \"avg_wc_notes_day_42\": \"avg_wc_notes\",\n",
    "                                              \"avg_wc_terms_day_42\": \"avg_wc_terms\",\n",
    "                                              \"client_count_day_42\": \"client_count\",\n",
    "                                              \"invoice_count_day_42\": \"invoice_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Day 49: Renaming Columns ########################################################\n",
    "\n",
    "df_49_frc = df_49_c01.rename(columns={\"avg_wc_address_day_49\": \"avg_wc_address\", \n",
    "                                              \"avg_wc_description_day_49\": \"avg_wc_description\",\n",
    "                                              \"avg_wc_notes_day_49\": \"avg_wc_notes\",\n",
    "                                              \"avg_wc_terms_day_49\": \"avg_wc_terms\",\n",
    "                                              \"client_count_day_49\": \"client_count\",\n",
    "                                              \"invoice_count_day_49\": \"invoice_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Day 56: Renaming Columns ########################################################\n",
    "\n",
    "df_56_frc = df_56_c05.rename(columns={\"avg_wc_address_day_56\": \"avg_wc_address\", \n",
    "                                              \"avg_wc_description_day_56\": \"avg_wc_description\",\n",
    "                                              \"avg_wc_notes_day_56\": \"avg_wc_notes\",\n",
    "                                              \"avg_wc_terms_day_56\": \"avg_wc_terms\",\n",
    "                                              \"client_count_day_56\": \"client_count\",\n",
    "                                              \"invoice_count_day_56\": \"invoice_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Day 63: Renaming Columns ########################################################\n",
    "\n",
    "df_63_frc = df_63_c04.rename(columns={\"avg_wc_address_day_63\": \"avg_wc_address\", \n",
    "                                              \"avg_wc_description_day_63\": \"avg_wc_description\",\n",
    "                                              \"avg_wc_notes_day_63\": \"avg_wc_notes\",\n",
    "                                              \"avg_wc_terms_day_63\": \"avg_wc_terms\",\n",
    "                                              \"client_count_day_63\": \"client_count\",\n",
    "                                              \"invoice_count_day_63\": \"invoice_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Day 70: Renaming Columns ########################################################\n",
    "\n",
    "df_70_frc = df_70_c07.rename(columns={\"avg_wc_address_day_70\": \"avg_wc_address\", \n",
    "                                              \"avg_wc_description_day_70\": \"avg_wc_description\",\n",
    "                                              \"avg_wc_notes_day_70\": \"avg_wc_notes\",\n",
    "                                              \"avg_wc_terms_day_70\": \"avg_wc_terms\",\n",
    "                                              \"client_count_day_70\": \"client_count\",\n",
    "                                              \"invoice_count_day_70\": \"invoice_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Day 77: Renaming Columns ########################################################\n",
    "\n",
    "df_77_frc = df_77_c01.rename(columns={\"avg_wc_address_day_77\": \"avg_wc_address\", \n",
    "                                              \"avg_wc_description_day_77\": \"avg_wc_description\",\n",
    "                                              \"avg_wc_notes_day_77\": \"avg_wc_notes\",\n",
    "                                              \"avg_wc_terms_day_77\": \"avg_wc_terms\",\n",
    "                                              \"client_count_day_77\": \"client_count\",\n",
    "                                              \"invoice_count_day_77\": \"invoice_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Day 84: Renaming Columns ########################################################\n",
    "\n",
    "df_84_frc = df_84_c04.rename(columns={\"avg_wc_address_day_84\": \"avg_wc_address\", \n",
    "                                              \"avg_wc_description_day_84\": \"avg_wc_description\",\n",
    "                                              \"avg_wc_notes_day_84\": \"avg_wc_notes\",\n",
    "                                              \"avg_wc_terms_day_84\": \"avg_wc_terms\",\n",
    "                                              \"client_count_day_84\": \"client_count\",\n",
    "                                              \"invoice_count_day_84\": \"invoice_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Day 91: Renaming Columns ########################################################\n",
    "\n",
    "df_91_frc = df_91_c01.rename(columns={\"avg_wc_address_day_91\": \"avg_wc_address\", \n",
    "                                              \"avg_wc_description_day_91\": \"avg_wc_description\",\n",
    "                                              \"avg_wc_notes_day_91\": \"avg_wc_notes\",\n",
    "                                              \"avg_wc_terms_day_91\": \"avg_wc_terms\",\n",
    "                                              \"client_count_day_91\": \"client_count\",\n",
    "                                              \"invoice_count_day_91\": \"invoice_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Model Name Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Adding Predicting Column Name ##################################################\n",
    "df_07_frc['model_name'] = 'D07'\n",
    "df_14_frc['model_name'] = 'D14'\n",
    "df_21_frc['model_name'] = 'D21'\n",
    "df_28_frc['model_name'] = 'D28'\n",
    "df_35_frc['model_name'] = 'D35'\n",
    "df_42_frc['model_name'] = 'D42'\n",
    "df_49_frc['model_name'] = 'D49'\n",
    "df_56_frc['model_name'] = 'D56'\n",
    "df_63_frc['model_name'] = 'D63'\n",
    "df_70_frc['model_name'] = 'D70'\n",
    "df_77_frc['model_name'] = 'D77'\n",
    "df_84_frc['model_name'] = 'D84'\n",
    "df_91_frc['model_name'] = 'D91'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging All Weekly Fraud Risk Clusters DataFrames \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging all FRC\n",
    "df_merge_frc = df_07_frc.copy()\n",
    "df_merge_frc = df_merge_frc.append(df_14_frc)\n",
    "df_merge_frc = df_merge_frc.append(df_21_frc)\n",
    "df_merge_frc = df_merge_frc.append(df_28_frc)\n",
    "df_merge_frc = df_merge_frc.append(df_35_frc)\n",
    "df_merge_frc = df_merge_frc.append(df_42_frc)\n",
    "df_merge_frc = df_merge_frc.append(df_49_frc)\n",
    "df_merge_frc = df_merge_frc.append(df_56_frc)\n",
    "df_merge_frc = df_merge_frc.append(df_63_frc)\n",
    "df_merge_frc = df_merge_frc.append(df_70_frc)\n",
    "df_merge_frc = df_merge_frc.append(df_77_frc)\n",
    "df_merge_frc = df_merge_frc.append(df_84_frc)\n",
    "df_merge_frc = df_merge_frc.append(df_91_frc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reindexing the merged dataframe\n",
    "df_merge_frc = df_merge_frc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df_merge_frc)\n",
    "df_merge_frc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_frc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Fraud Risk Score (FRS)\n",
    "Computing Fraud Risk Score (FRS) based on the follwining formula: \n",
    "\n",
    "- 'days_on_platform' = $d$\n",
    "- 'declinedonlinepaymentnotification' = $p$\n",
    "- 'emailinvoice' = $e$\n",
    "- 'invoice_count' = $i$\n",
    "\n",
    "\n",
    "$frs\\_gmm = \\frac{p + e + i}{d}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraud_risk_score(row):\n",
    "    \n",
    "    # selected column values\n",
    "    d = row['model_name']\n",
    "    p = float(row['declinedonlinepaymentnotification'])\n",
    "    e = float(row['emailinvoice'])\n",
    "    i = float(row['invoice_count'])\n",
    "    \n",
    "    # fraud risk score\n",
    "    if d == 'D07':\n",
    "        #print(\"D7\")\n",
    "        frs = (p + e + i) / 7 \n",
    "    elif d == 'D14':\n",
    "        #print(\"D14\")\n",
    "        frs = (p + e + i) / 14\n",
    "    elif d == 'D21':\n",
    "        #print(\"D21\")\n",
    "        frs = (p + e + i) / 21\n",
    "    elif d == 'D28':\n",
    "        #print(\"D28\")\n",
    "        frs = (p + e + i) / 28\n",
    "    elif d == 'D35':\n",
    "        #print(\"D35\")\n",
    "        frs = (p + e + i) / 35\n",
    "    elif d == 'D42':\n",
    "        #print(\"D42\")\n",
    "        frs = (p + e + i) / 42\n",
    "    elif d == 'D49':\n",
    "        #print(\"D49\")\n",
    "        frs = (p + e + i) / 49\n",
    "    else:\n",
    "        #print(\"Exception\")\n",
    "        frs = 0\n",
    "    return frs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating and adding Fraud Risk Score (FRS) column\n",
    "col_list = ['days_on_platform', 'declinedonlinepaymentnotification', 'emailinvoice', 'invoice_count']\n",
    "fraud_risk_score = df_merge_frc.apply(lambda x: fraud_risk_score(x), axis=1)\n",
    "df_merge_frc['fraud_risk_score'] = fraud_risk_score     # adding this fraud risk score as a column in the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_frc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_frc['invoice_count'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Already Labeld Fraud Risk Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Remove Already Labeled Fraud Risk Accounts #################################\n",
    "\n",
    "## Improt labeled fraud risk accounts (labeled by support team)\n",
    "df_labled_fraud = pd.read_csv('/fraud_detection/data/fraud_risk_acc_labeled/all_fraud_status_labeled_by_support.csv', sep=\",\")\n",
<<<<<<< HEAD
    "df_labled_fraud_systemid = pd.DataFrame(df_labled_fraud['id'])\n"
=======
    "df_labled_fraud_systemid = pd.DataFrame(df_labled_fraud['systemid'])\n"
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>systemid</th>\n",
       "      <th>admin_email</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>effective_date</th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>days_on_platform</th>\n",
       "      <th>support_note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>5152105</td>\n",
       "      <td>enquiries@anisdincatering.com</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43</td>\n",
       "      <td>catering services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>5236163</td>\n",
       "      <td>raysonloo.sh@gmail.com</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>selling bags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>5245075</td>\n",
       "      <td>shawndmix@gmail.com</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>trucking services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>5209249</td>\n",
       "      <td>amandaphan1234@gmail.com</td>\n",
       "      <td>2020-04-21</td>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "      <td>cannot verify any information, has not logged ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>5218019</td>\n",
       "      <td>katayres20@gmail.com</td>\n",
       "      <td>2020-04-24</td>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>selling jewellery as an associate of Paparazzi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     systemid                    admin_email signup_date effective_date  \\\n",
       "460   5152105  enquiries@anisdincatering.com  2020-03-30     2020-05-12   \n",
       "461   5236163         raysonloo.sh@gmail.com  2020-05-01     2020-05-12   \n",
       "462   5245075            shawndmix@gmail.com  2020-05-04     2020-05-12   \n",
       "463   5209249       amandaphan1234@gmail.com  2020-04-21     2020-05-12   \n",
       "464   5218019           katayres20@gmail.com  2020-04-24     2020-05-12   \n",
       "\n",
       "     fraud_label  days_on_platform  \\\n",
       "460          0.0                43   \n",
       "461          0.0                11   \n",
       "462          0.0                 8   \n",
       "463          1.0                21   \n",
       "464          0.0                18   \n",
       "\n",
       "                                          support_note  \n",
       "460                                  catering services  \n",
       "461                                       selling bags  \n",
       "462                                  trucking services  \n",
       "463  cannot verify any information, has not logged ...  \n",
       "464     selling jewellery as an associate of Paparazzi  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> bba3c7e185eb01e2826669f1d93a7077a1acd14f
   "source": [
    "df_labled_fraud.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross users accounts, if any account is already labeled then remove it \n",
    "df_merge_frc_and_labeled = pd.merge(df_merge_frc, df_labled_fraud_systemid, how='left', on=['systemid'], indicator=True)\n",
    "df_merge_frc_nolabeled = df_merge_frc_and_labeled[df_merge_frc_and_labeled._merge != 'both']\n",
    "df_merge_frc_nolabeled = df_merge_frc_nolabeled.drop(columns=['_merge'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_frc_nolabeled.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Based on FRS and Re-Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Sorting Based on FRS and Re-Indexing ####################################\n",
    "\n",
    "## Sorting and reindexing reprot data frame based on the 'fraud_risk_score'\n",
    "df_merge_frc_nolabeled.sort_values('fraud_risk_score', axis = 0, ascending = False, inplace = True, na_position ='last') \n",
    "\n",
    "## Reindexing the sorted dataframe\n",
    "df_merge_frc_nolabeled = df_merge_frc_nolabeled.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_frc_nolabeled.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: To Be Labeled (TBL) by Support Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding effective date and model name column\n",
    "df_merge_frc_nolabeled['fraud_label'] = 'TBL'  # To Be Labeled (TBL) by support team\n",
    "df_merge_frc_nolabeled['support_note'] = 'NaN'  # To Be Labeled (TBL) by support team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_frc_nolabeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Path and file name for user accounts for need to be labeled by support team\n",
    "path = \"/fraud_detection/data/fraud_risk_acc_to_be_labeled_all_features/\"\n",
    "\n",
    "# file_name = \"new_fraud_risk_acc_tbl_all_features\"\n",
    "# frc_tbl = path + file_name + \".tsv\"\n",
    "\n",
    "file_name = \"new_fraud_risk_acc_tbl_all_features_\"\n",
    "today = str(date.today())\n",
    "\n",
    "frc_tbl = path + file_name + today + \".tsv\"\n",
    "\n",
    "\n",
    "## Save the user accounts for need to be labeled by support team\n",
    "df_merge_frc_nolabeled.to_csv(frc_tbl, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
