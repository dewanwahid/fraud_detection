{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFD - Weekly Segmented Model X Days - Testing\n",
    "Testing Weekly Segmented Models (WSMs) for X (e.g., 07, 14, etc.) Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "from scipy import stats\n",
    "get_ipython().magic(u'config IPCompleter.greedy=True')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect with the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import closing\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "import simplejson\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "DEFAULT_DB = 'database'\n",
    "DEFAULT_HOST = 'database-host'\n",
    "DEFAULT_PORT = 1234\n",
    "\n",
    "\n",
    "class PsycopgConnector:\n",
    "    '''\n",
    "    A database connector that uses Psycopg to connect to Redshift.\n",
    "\n",
    "    How to play:\n",
    "\n",
    "        psy_conn = PsycopgConnector(username, password)\n",
    "        df = psy_conn.run_query(sql=sql, return_data=True)\n",
    "\n",
    "    NOTE: This class commits queries to redshift if return_data=False.\n",
    "    This means INSERT, DROP, TRUNCATE, etc. all work against the DB.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        username=None,\n",
    "        password=None,\n",
    "        db=DEFAULT_DB,\n",
    "        host=DEFAULT_HOST,\n",
    "        port=DEFAULT_PORT,\n",
    "    ):\n",
    "\n",
    "        self.db = DEFAULT_DB\n",
    "        self.host = DEFAULT_HOST\n",
    "        self.port = DEFAULT_PORT\n",
    "\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "\n",
    "    def _get_connection(self):\n",
    "\n",
    "        self.conn = psycopg2.connect(\n",
    "            dbname=self.db,\n",
    "            user=self.username,\n",
    "            password=self.password,\n",
    "            host=self.host,\n",
    "            port=self.port\n",
    "        )\n",
    "\n",
    "        return self.conn\n",
    "\n",
    "    def run_query(self, sql, return_data=False):\n",
    "\n",
    "        with closing(self._get_connection()) as conn:\n",
    "            with conn, conn.cursor() as cur:\n",
    "                if return_data:\n",
    "                    return pd.read_sql(sql=sql, con=conn)\n",
    "                else:\n",
    "                    cur.execute(sql)\n",
    "                    \n",
    "\n",
    "# Read the database's credentials file \n",
    "with open(\"credential.json.nogit\") as fh:\n",
    "    creds = simplejson.loads(fh.read())\n",
    "    \n",
    "username = creds.get(\"user_name\")\n",
    "password = creds.get(\"password\")\n",
    "\n",
    "pig = PsycopgConnector(username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count fuction\n",
    "import re\n",
    "def words_count (strg):\n",
    "    \n",
    "    #print(strg)\n",
    "    \n",
    "    if strg == '' or pd.isnull(strg):\n",
    "        no_of_words = 0\n",
    "        #print('NaN')\n",
    "    else:\n",
    "        strg_words_list = re.findall(r\"[\\w']+\", strg)\n",
    "        no_of_words = len(strg_words_list)\n",
    "    return no_of_words   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Invoice Data & Extract Avg Word Counts Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.01 Invoice within X days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL for impoorting all invoices created within X days after signup_date\n",
    "sql_invoices_Xdays_all_accounts = '''invoice_creates_in_last_X_day'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_invoices_Xdays_all_accounts = pig.run_query(sql_invoices_Xdays_all_accounts, return_data=True)\n",
    "\n",
    "# Words count in different invoice features with textual data\n",
    "df_invoices_Xdays_all_accounts['avg_wc'] = df_invoices_Xdays_all_accounts.apply(lambda x: words_count(x['invoice_description']), axis=1)\n",
    "                                                                                                                   \n",
    "# Filters the text columns from the dataframe\n",
    "df_invoices_Xdays_all_accounts_fil = df_invoices_Xdays_all_accounts.filter(['features_list'])  \n",
    "                                                                                                                   \n",
    "# Summing (grouping) all invoices for a 'id'\n",
    "df_word_count_Xdays_all_accounts_total = df_invoices_Xdays_all_accounts_fil.groupby('id').mean()  \n",
    "\n",
    "# Final word count table\n",
    "df_word_count_Xdays_all_accounts_final = df_word_count_Xdays_all_accounts_total.filter(['features_list'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Invoice's Important Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Invoice Details Features ###############\n",
    "\n",
    "# SQL query \n",
    "sql_rs_invoices_clients_activities_all_accounts = '''sql query for details invoice feature'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_rs_invoices_clients_activities_all_accounts = pig.run_query(sql_rs_invoices_clients_activities_all_accounts, return_data=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Import and Exract Features from Events Data\n",
    "## 4.1 Event data collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Event Features Extraction ################################\n",
    "\n",
    "#SQL for events \n",
    "sql_events = '''invoice event details'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_events_all_accounts = pig.run_query(sql_events, return_data=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Removing whitespce from the event strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing row if there is 'None' the event cell\n",
    "df_events_all_accounts = df_events_all_accounts[~df_events_all_accounts.astype(str).eq('None').any(1)]\n",
    "\n",
    "# Replace the 'NaN' cell by zero\n",
    "df_events_all_accounts.fillna(0, inplace=True)\n",
    "\n",
    "# Using lambda function to remove the white space in the event string name\n",
    "df_events_all_accounts['event_name'] = df_events_all_accounts.apply(lambda x: x['event'].replace(' ', '').replace('-','').replace('/', ''), axis=1)\n",
    "\n",
    "# Filtered the events columns for day X\n",
    "df_events_all_accounts_day_X = df_events_all_accounts[['id', 'event_count_day_X', 'event_name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Pivote the events (each unique event become a column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pivote the Day X Events (Each Unique Event Become a Column)###\n",
    "\n",
    "# Pivot table based on the unique column value in 'event_name'\n",
    "df_events_all_accounts_day_X = df_events_all_accounts_day_X.pivot_table(values='event_count_day_X', columns='event_name', index='id', aggfunc=np.sum,  fill_value=0)\n",
    "\n",
    "# Drop the old column name\n",
    "df_events_all_accounts_day_X.columns.name = None\n",
    "\n",
    "# Reset the index\n",
    "df_events_all_accounts_day_X = df_events_all_accounts_day_X.reset_index()\n",
    "\n",
    "# Replace 'NaN' with zero\n",
    "df_events_all_accounts_day_X.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Merging Invoice Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging report system and events data for day X period\n",
    "df_rs_events_day_X = pd.merge(df_rs_invoices_clients_activities_all_accounts, df_events_all_accounts_day_X,\n",
    "                             on='id', how='left')\n",
    "\n",
    "# Merging average word count with 'df_rs_events_day_X'\n",
    "df_rs_events_avg_wc_day_X = pd.merge(df_rs_events_day_X, df_word_count_Xdays_all_accounts_final,\n",
    "                                    on='id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Filtering out Platform's test accounts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Filtering Out Test Accounts #############################################################\n",
    "test_emails = pd.read_csv(\"file_path.csv\", sep=\"\\t\")\n",
    "test_email_list = list(test_emails['email'])\n",
    "\n",
    "\n",
    "# Function: Filtering  test account by using admin email\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def email_match(em, email_list):\n",
    "    \n",
    "    L = len(email_list)\n",
    "    match_score = 0\n",
    "    \n",
    "    for i in range(0, L):\n",
    "        if pd.isnull(em):\n",
    "            match_score = 0\n",
    "            break;\n",
    "        else: \n",
    "            match_score =  max(match_score, SequenceMatcher(None,em, email_list[i]).ratio())\n",
    "\n",
    "    return match_score\n",
    "\n",
    "\n",
    "# Filtering final data from the  Test emails\n",
    "df_rs_events_avg_wc_day_X_notest = df_rs_events_avg_wc_day_X[\n",
    "    df_rs_events_avg_wc_day_X.apply(lambda x: email_match(x['admin_email'], test_email_list) < 0.95, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Filtering only important features: Day X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing importing features list\n",
    "important_features = pd.read_csv(\"import_feature_list_path\", sep=\"\\n,\")\n",
    "\n",
    "# Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])\n",
    "\n",
    "# Adding missing important feature column with zero values (if there any!)\n",
    "for i in range(len(imp_features_list)):\n",
    "    if imp_features_list[i] in df_rs_events_avg_wc_day_X_notest.columns:\n",
    "#         print(\"True\")\n",
    "        continue;\n",
    "        \n",
    "    else:\n",
    "        print(\"False: \", imp_features_list[i])\n",
    "        df_rs_events_avg_wc_day_X_notest[imp_features_list[i]] = 0\n",
    "\n",
    "# Filtering only important features \n",
    "df_imp_features_new_accounts_day_X =\\\n",
    "            df_rs_events_avg_wc_day_X_notest[df_rs_events_avg_wc_day_X_notest.columns.intersection(imp_features_list)]\n",
    "\n",
    "df_imp_features_new_accounts_day_X = df_imp_features_new_accounts_day_X.reindex(\n",
    "    sorted(df_imp_features_new_accounts_day_X.columns), axis=1)\n",
    "\n",
    "# Drop rows with nan value\n",
    "df_imp_features_new_accounts_day_X = df_imp_features_new_accounts_day_X.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Filtering inactive users' accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_cols_list = ['feature-list']\n",
    "cols_list = list(df_imp_features_new_accounts_day_X) \n",
    "cols = list(set(cols_list) - set(ex_cols_list))\n",
    "\n",
    "# Function for aggregating selected column values\n",
    "def cell_value_sum (row, cols):\n",
    "    sum = 0\n",
    "    for i in cols:\n",
    "        sum = sum + row[i]\n",
    "    return sum\n",
    "\n",
    "# Fltering out all inactive users accounts\n",
    "df_final_features_new_accounts_day_X =\\\n",
    "        df_imp_features_new_accounts_day_X[df_imp_features_new_accounts_day_X.apply(lambda x: cell_value_sum(x, cols) > 0, axis=1)]\n",
    "\n",
    "df_final_features_new_accounts_day_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Saving the filtered features data for new accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export filtered features data fro new accounts\n",
    "today = str(date.today())\n",
    "path = \"saving-path\" + today + \".tsv\"\n",
    "df_final_features_new_accounts_day_X.to_csv(path, sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
