{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining: X Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "from scipy import stats\n",
    "get_ipython().magic(u'config IPCompleter.greedy=True')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect with the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import closing\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "import simplejson\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "DEFAULT_DB = 'data_depot'\n",
    "DEFAULT_HOST = '-data.c8exzn6geij3.us-east-1.redshift.amazonaws.com'\n",
    "DEFAULT_PORT = 5439\n",
    "\n",
    "\n",
    "class PsycopgConnector:\n",
    "    '''\n",
    "    A database connector that uses Psycopg to connect to Redshift.\n",
    "\n",
    "    How to play:\n",
    "\n",
    "        psy_conn = PsycopgConnector(username, password)\n",
    "        df = psy_conn.run_query(sql=sql, return_data=True)\n",
    "\n",
    "    NOTE: This class commits queries to redshift if return_data=False.\n",
    "    This means INSERT, DROP, TRUNCATE, etc. all work against the DB.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        username=None,\n",
    "        password=None,\n",
    "        db=DEFAULT_DB,\n",
    "        host=DEFAULT_HOST,\n",
    "        port=DEFAULT_PORT,\n",
    "    ):\n",
    "\n",
    "        self.db = DEFAULT_DB\n",
    "        self.host = DEFAULT_HOST\n",
    "        self.port = DEFAULT_PORT\n",
    "\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "\n",
    "    def _get_connection(self):\n",
    "\n",
    "        self.conn = psycopg2.connect(\n",
    "            dbname=self.db,\n",
    "            user=self.username,\n",
    "            password=self.password,\n",
    "            host=self.host,\n",
    "            port=self.port\n",
    "        )\n",
    "\n",
    "        return self.conn\n",
    "\n",
    "    def run_query(self, sql, return_data=False):\n",
    "\n",
    "        with closing(self._get_connection()) as conn:\n",
    "            with conn, conn.cursor() as cur:\n",
    "                if return_data:\n",
    "                    return pd.read_sql(sql=sql, con=conn)\n",
    "                else:\n",
    "                    cur.execute(sql)\n",
    "                    \n",
    "\n",
    "# Read the credentials file \n",
    "with open(\"creds.json.nogit\") as fh:\n",
    "    creds = simplejson.loads(fh.read())\n",
    "    \n",
    "username = creds.get(\"user_name\")\n",
    "password = creds.get(\"password\")\n",
    "\n",
    "pig = PsycopgConnector(username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count fuction\n",
    "import re\n",
    "def words_count (strg):\n",
    "    \n",
    "    #print(strg)\n",
    "    \n",
    "    if strg == '' or pd.isnull(strg):\n",
    "        no_of_words = 0\n",
    "        #print('NaN')\n",
    "    else:\n",
    "        strg_words_list = re.findall(r\"[\\w']+\", strg)\n",
    "        no_of_words = len(strg_words_list)\n",
    "\n",
    "        \n",
    "        #print(strg_words_list)\n",
    "    \n",
    "    return no_of_words \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Invoice Data & Extract Avg Word Counts Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.01 Invoice within X days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL for impoorting all invoices created within X days after signup_date\n",
    "sql_invoices_Xdays_all_accounts = '''invocie created in x days'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_invoices_Xdays_all_accounts = pig.run_query(sql_invoices_Xdays_all_accounts, return_data=True)\n",
    "\n",
    "\n",
    "# Words count in invoice's description, notes, terms, address\n",
    "df_invoices_Xdays_all_accounts['wc_short_text_column_1'] = df_invoices_Xdays_all_accounts.apply(lambda x: words_count(x['description']), axis=1)\n",
    "df_invoices_Xdays_all_accounts['wc_short_text_column_1'] = df_invoices_Xdays_all_accounts.apply(lambda x: words_count(x['notes']), axis=1)\n",
    "df_invoices_Xdays_all_accounts['wc_short_text_column_1'] = df_invoices_Xdays_all_accounts.apply(lambda x: words_count(x['terms']), axis=1)\n",
    "df_invoices_Xdays_all_accounts['wc_short_text_column_1'] = df_invoices_Xdays_all_accounts.apply(lambda x: words_count(x['address']), axis=1)\n",
    "\n",
    "                                                                                                                   \n",
    "# Filters the text columns from the dataframe\n",
    "df_invoices_Xdays_all_accounts_fil = df_invoices_Xdays_all_accounts.filter(['short_text_column_list'])  \n",
    "                                                                                                                   \n",
    "# Summing (grouping) all invoices for a 'id'\n",
    "df_word_count_Xdays_all_accounts_total = df_invoices_Xdays_all_accounts_fil.groupby('id').mean()  \n",
    "\n",
    "# Final word count table\n",
    "df_word_count_Xdays_all_accounts_final = df_word_count_Xdays_all_accounts_total.filter([\n",
    "                                                                            'id',\n",
    "                                                                            'signup_date',\n",
    "                                                                            'avg_wc_des', \n",
    "                                                                            'avg_wc_nt', \n",
    "                                                                            'avg_wc_trm',\n",
    "                                                                            'avg_wc_adr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################# Import RSystems, Periodic Invoices & Client Counts Data ###############\n",
    "\n",
    "# SQL query \n",
    "sql_invoices_clients_activities_all_accounts = ''' SQL QUERIES'''\n",
    "\n",
    "# Import as dataframe \n",
    "df_invoices_clients_activities_all_accounts = pig.run_query(sql_invoices_clients_activities_all_accounts, return_data=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Import and Exract Events \n",
    "## 4.1 Event data collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Event Features Extraction ################################\n",
    "\n",
    "#SQL for events \n",
    "sql_events = '''SQL QUERY'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "# df_events_all_accounts = pd.read_sql_query(sql_events, connect_to_db)\n",
    "df_events_all_accounts = pig.run_query(sql_events, return_data=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Removing whitespce from the event strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing row if there is 'None' the event cell\n",
    "df_events_all_accounts = df_events_all_accounts[~df_events_all_accounts.astype(str).eq('None').any(1)]\n",
    "\n",
    "# Replace the 'NaN' cell by zero\n",
    "df_events_all_accounts.fillna(0, inplace=True)\n",
    "\n",
    "# Using lambda function to remove the white space in the event string name\n",
    "df_events_all_accounts['event_name'] = df_events_all_accounts.apply(lambda x: x['event'].replace(' ', '').replace('-','').replace('/', ''), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered the events columns for day X\n",
    "df_events_all_accounts_day_X = df_events_all_accounts[['id', 'event_count_day_X', 'event_name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Pivote the events (each unique event become a column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table based on the unique column value in 'event_name'\n",
    "df_events_all_accounts_day_X = df_events_all_accounts_day_X.pivot_table(values='event_count_day_X', columns='event_name', index='id', aggfunc=np.sum,  fill_value=0)\n",
    "\n",
    "# Drop the old column name\n",
    "df_events_all_accounts_day_X.columns.name = None\n",
    "\n",
    "# Reset the index\n",
    "df_events_all_accounts_day_X = df_events_all_accounts_day_X.reset_index()\n",
    "\n",
    "# Replace 'NaN' with zero\n",
    "df_events_all_accounts_day_X.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Merging all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging report system and events data for day X period\n",
    "df_events_day_X = pd.merge(df_invoices_clients_activities_all_accounts, df_events_all_accounts_day_X,\n",
    "                             on='id', how='left')\n",
    "\n",
    "# Merging average word count with 'df_events_day_X'\n",
    "df_events_avg_wc_day_X = pd.merge(df_events_day_X, df_word_count_Xdays_all_accounts_final,\n",
    "                                    on='id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Filtering out test accounts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Filtering Out Test Accounts #############################################################\n",
    "\n",
    "# Import test accounts email from CSV file \n",
    "test_emails = pd.read_csv(\"path\", sep=\"\\t\")\n",
    "test_email_list = list(test_emails['email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Filtering  test account by using admin email\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def email_match(em, email_list):\n",
    "    \n",
    "    L = len(email_list)\n",
    "#     print('L', L)\n",
    "#     print('em-before-loop: ', em)\n",
    "    match_score = 0\n",
    "#     x = float(em)\n",
    "    \n",
    "    for i in range(0, L):\n",
    "#         if math.isnan(x):\n",
    "#             match_score = 0\n",
    "#             break;\n",
    "        if pd.isnull(em):\n",
    "            match_score = 0\n",
    "            break;\n",
    "        else: \n",
    "            match_score =  max(match_score, SequenceMatcher(None,em, email_list[i]).ratio())\n",
    "#             print(i, em, email_list[i], match_score)\n",
    "\n",
    "    return match_score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering final data from the  Test emails\n",
    "df_events_avg_wc_day_X_notest = df_events_avg_wc_day_X[\n",
    "    df_events_avg_wc_day_X.apply(lambda x: email_match(x['admin_email'], test_email_list) < 0.95, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Filtering only important features: Day X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing importing features list\n",
    "important_features = pd.read_csv( \"path\", sep=\"\\n,\")\n",
    "\n",
    "# Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imp_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding missing important feature column with zero values (if there any!)\n",
    "for i in range(len(imp_features_list)):\n",
    "    if imp_features_list[i] in df_events_avg_wc_day_X_noÆ’test.columns:\n",
    "#         print(\"True\")\n",
    "        continue;\n",
    "        \n",
    "    else:\n",
    "        print(\"False: \", imp_features_list[i])\n",
    "        df_events_avg_wc_day_X_notest[imp_features_list[i]] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering only important features \n",
    "df_imp_features_new_accounts_day_X =\\\n",
    "            df_events_avg_wc_day_X_notest[df_events_avg_wc_day_X_notest.columns.intersection(imp_features_list)]\n",
    "\n",
    "df_imp_features_new_accounts_day_X = df_imp_features_new_accounts_day_X.reindex(\n",
    "    sorted(df_imp_features_new_accounts_day_X.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN\n",
    "df_imp_features_new_accounts_day_X = df_imp_features_new_accounts_day_X.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Filtering inactive users' accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_cols_list = ['admin_email','days_on_platform', 'effective_date', 'signup_date', 'id']\n",
    "cols_list = list(df_imp_features_new_accounts_day_X) \n",
    "cols = list(set(cols_list) - set(ex_cols_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for aggregating selected column values\n",
    "def cell_value_sum (row, cols):\n",
    "    #print(row)\n",
    "    sum = 0\n",
    "    for i in cols:\n",
    "        #print(i)\n",
    "        #print(i, row[i])\n",
    "        sum = sum + row[i]\n",
    "    \n",
    "    #print('Final sum: ', sum)\n",
    "    return sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fltering out all inactive users accounts\n",
    "df_final_features_new_accounts_day_X =\\\n",
    "        df_imp_features_new_accounts_day_X[df_imp_features_new_accounts_day_X.apply(lambda x: cell_value_sum(x, cols) > 0, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Saving the filtered features data for new accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export filtered features data fro new accounts\n",
    "today = str(date.today())\n",
    "path = \"path\"\n",
    "df_final_features_new_accounts_day_X.to_csv(path, sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
