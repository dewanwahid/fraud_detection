{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Netwok Based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data file path and name\n",
    "day = '91'\n",
    "k = '_k7'\n",
    "path1 = '/Users/dwahid/Documents/GitHub/fraud_detection/data/model_outputs_gmm_for_nn_training/'\n",
    "file_name1 = path1 + 'gmm_clutering_outputs_day_' + day + k + '.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loding data from the GMM outputs \n",
    "df = pd.read_csv(file_name1, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop row with NaN\n",
    "df = df.dropna()\n",
    "\n",
    "# Keep a copy of the original data\n",
    "df_orig = df.copy()\n",
    "\n",
    "# Checking null in data\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 activateexpense\n",
      "1 activateotherincome\n",
      "2 activatepayment\n",
      "3 admindeactivation\n",
      "4 adminonlinepaymentattempt\n",
      "5 adminpayinvoiceonlineinvoice\n",
      "6 adminpayinvoiceonlinelistview\n",
      "7 archiveclient\n",
      "8 archiveexpense\n",
      "9 archiveotherincome\n",
      "10 archiveproject\n",
      "11 archivetask\n",
      "12 autobillpayment\n",
      "13 avg_wc_address_day_91\n",
      "14 avg_wc_description_day_91\n",
      "15 avg_wc_notes_day_91\n",
      "16 avg_wc_terms_day_91\n",
      "17 bulkimportclientscomplete\n",
      "18 client_count_day_91\n",
      "19 clientimportcsvsucceeded\n",
      "20 clientlimitupgradenudge\n",
      "21 createbankaccount\n",
      "22 createbanktransaction\n",
      "23 createbanktransfer\n",
      "24 createcategory\n",
      "25 createcontact\n",
      "26 createcontractor\n",
      "27 createcreditnote\n",
      "28 createdexpense\n",
      "29 createestimate\n",
      "30 createexpense\n",
      "31 createitem\n",
      "32 createotherincome\n",
      "33 createreceipt\n",
      "34 createservice\n",
      "35 creditcardclientaccessgranted\n",
      "36 customemailsignature\n",
      "37 declinedonlinepaymentnotification\n",
      "38 deletebusinesspartner\n",
      "39 deletecollaborator\n",
      "40 deletecreditnote\n",
      "41 deleteestimate\n",
      "42 deleteexpense\n",
      "43 deletehours\n",
      "44 deleteinvoice\n",
      "45 deleteitem\n",
      "46 deleteotherincome\n",
      "47 deleteproject\n",
      "48 deletestaff\n",
      "49 deleteuser\n",
      "50 disconnectbankaccount\n",
      "51 disconnectpaymentgateway\n",
      "52 dualstripepaypalgatewayexperienceinvoicepopupclick\n",
      "53 emailcreditnote\n",
      "54 emailestimate\n",
      "55 emailestimatetoself\n",
      "56 emailinvoice\n",
      "57 emailinvoicesample\n",
      "58 emailreport\n",
      "59 emailsent\n",
      "60 enableautobilling\n",
      "61 enablepaymentgateway\n",
      "62 expenseimportsucceeded\n",
      "63 fbpayenabled\n",
      "64 fbpayfirstinvoicesent\n",
      "65 fbpayupdatedacceptedcreditcards\n",
      "66 hitpaywallclientlimit\n",
      "67 hourslogged\n",
      "68 identitycompletedonboarding\n",
      "69 invoice_count_day_91\n",
      "70 is_freshbooks_account_active\n",
      "71 is_paying\n",
      "72 login\n",
      "73 stripepaymentsuccessful\n",
      "74 successfulonlinepayment\n",
      "75 surveyquestionanswered\n",
      "76 systemid\n",
      "77 updatebusiness\n",
      "78 updatecategory\n",
      "79 updateclient\n",
      "80 updatecompanyprofile\n",
      "81 updatecontractor\n",
      "82 updatecreditnote\n",
      "83 updateestimate\n",
      "84 updateexpense\n",
      "85 updateinvoicesample\n",
      "86 updateitem\n",
      "87 updateservice\n",
      "88 cluster_id\n"
     ]
    }
   ],
   "source": [
    "## Get the column index\n",
    "col_names = list(df)\n",
    "L = len(col_names)\n",
    "\n",
    "for i in range(0, L):\n",
    "    print i, col_names[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized all features columns except the 'systemid'\n",
    "column_names_to_normalize = ['systemid', 'cluster_id']\n",
    "column_names_to_normalize = [x for x in list(df) if x not in column_names_to_normalize ]\n",
    "\n",
    "# Normalized all features columns except the 'systemid'\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x = df[column_names_to_normalize].values\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = df.index)\n",
    "df[column_names_to_normalize] = df_temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data: Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['systemid', 'cluster_id'],axis=1)\n",
    "y = df['cluster_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(13, 13, 13), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(13, 13, 13), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(13, 13, 13), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "       verbose=False, warm_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Saving the Train Model #################################################\n",
    "\n",
    "## Saving pickled model file path and name\n",
    "path2 = '/Users/dwahid/Documents/GitHub/fraud_detection/src/saved_models/'\n",
    "file_name2 = path2 + 'fraud_detection_nn_classifier_day_' + day + k + '_model.sav'\n",
    "\n",
    "# save the model to disk  \n",
    "pickle.dump(mlp, open(file_name2, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35914     0     0     0    23     0     1]\n",
      " [    0  4233     0     0     4     2     0]\n",
      " [    0     0 50714     0    65     1     0]\n",
      " [    0     0     0   263     0     2    47]\n",
      " [   12     0   162     0 14514     0     0]\n",
      " [    0    14     0     1     0   104     0]\n",
      " [    0     0     0     9     0     0  3603]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     35938\n",
      "           1       1.00      1.00      1.00      4239\n",
      "           2       1.00      1.00      1.00     50780\n",
      "           3       0.96      0.84      0.90       312\n",
      "           4       0.99      0.99      0.99     14688\n",
      "           5       0.95      0.87      0.91       119\n",
      "           6       0.99      1.00      0.99      3612\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    109688\n",
      "   macro avg       0.98      0.96      0.97    109688\n",
      "weighted avg       1.00      1.00      1.00    109688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
