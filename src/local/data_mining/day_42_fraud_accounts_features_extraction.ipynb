{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Accounts (Identified): Quring and Features Extraction (Day 42)\n",
    "\n",
    "We use this script to quering and extracting features for accounts signup in last 42 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "from scipy import stats\n",
    "get_ipython().magic(u'config IPCompleter.greedy=True')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect with the Redshift Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import closing\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "import simplejson\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "DEFAULT_DB = 'data_depot'\n",
    "DEFAULT_HOST = 'freshbooks-data.c8exzn6geij3.us-east-1.redshift.amazonaws.com'\n",
    "DEFAULT_PORT = 5439\n",
    "\n",
    "\n",
    "class PsycopgConnector:\n",
    "    '''\n",
    "    A database connector that uses Psycopg to connect to Redshift.\n",
    "\n",
    "    How to play:\n",
    "\n",
    "        psy_conn = PsycopgConnector(username, password)\n",
    "        df = psy_conn.run_query(sql=sql, return_data=True)\n",
    "\n",
    "    NOTE: This class commits queries to redshift if return_data=False.\n",
    "    This means INSERT, DROP, TRUNCATE, etc. all work against the DB.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        username=None,\n",
    "        password=None,\n",
    "        db=DEFAULT_DB,\n",
    "        host=DEFAULT_HOST,\n",
    "        port=DEFAULT_PORT,\n",
    "    ):\n",
    "\n",
    "        self.db = DEFAULT_DB\n",
    "        self.host = DEFAULT_HOST\n",
    "        self.port = DEFAULT_PORT\n",
    "\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "\n",
    "    def _get_connection(self):\n",
    "\n",
    "        self.conn = psycopg2.connect(\n",
    "            dbname=self.db,\n",
    "            user=self.username,\n",
    "            password=self.password,\n",
    "            host=self.host,\n",
    "            port=self.port\n",
    "        )\n",
    "\n",
    "        return self.conn\n",
    "\n",
    "    def run_query(self, sql, return_data=False):\n",
    "\n",
    "        with closing(self._get_connection()) as conn:\n",
    "            with conn, conn.cursor() as cur:\n",
    "                if return_data:\n",
    "                    return pd.read_sql(sql=sql, con=conn)\n",
    "                else:\n",
    "                    cur.execute(sql)\n",
    "                    \n",
    "\n",
    "# Read the Redshift's credentials file \n",
    "with open(\"redshift_creds.json.nogit\") as fh:\n",
    "    creds = simplejson.loads(fh.read())\n",
    "    \n",
    "username = creds.get(\"user_name\")\n",
    "password = creds.get(\"password\")\n",
    "\n",
    "pig = PsycopgConnector(username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing connection\n",
    "sql_test = '''SELECT * FROM report_systems LIMIT 5'''\n",
    "df_test = pig.run_query(sql_test, return_data=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count fuction\n",
    "import re\n",
    "def words_count (strg):\n",
    "    \n",
    "    #print(strg)\n",
    "    \n",
    "    if strg == '' or pd.isnull(strg):\n",
    "        no_of_words = 0\n",
    "        #print('NaN')\n",
    "    else:\n",
    "        strg_words_list = re.findall(r\"[\\w']+\", strg)\n",
    "        no_of_words = len(strg_words_list)\n",
    "\n",
    "        \n",
    "        #print(strg_words_list)\n",
    "    \n",
    "    return no_of_words \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Invoice Data & Extract Avg Word Counts Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.01 Invoice within 42 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL for impoorting all invoices created within 42 days after signup_date\n",
    "sql_invoices_42days_all_accounts = '''WITH invoices_in_a_period AS (\n",
    "    SELECT\n",
    "            systemid,\n",
    "            signup_date\n",
    "    FROM report_systems rs\n",
    "    JOIN ( SELECT systemid FROM data_science.fraud_accounts_final ) USING (systemid)\n",
    "), invoice_created_at AS (\n",
    "    SELECT\n",
    "           pic.systemid,\n",
    "           pic.signup_date,\n",
    "           inv.invoiceid,\n",
    "           inv.create_date,\n",
    "           inv.description,\n",
    "           inv.notes,\n",
    "           inv.terms,\n",
    "           inv.address,\n",
    "           DATEDIFF(days, pic.signup_date, inv.created_at) AS days_to_invoice_creation\n",
    "    FROM invoices_in_a_period AS pic\n",
    "    LEFT JOIN coalesced_live_shards.invoice_stable as inv USING (systemid)\n",
    "    WHERE ((days_to_invoice_creation BETWEEN 0 AND 42) OR days_to_invoice_creation IS NULL)\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM invoice_created_at;'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "# df_invoices_42days_all_accounts = pd.read_sql_query(sql_invoices_42days_all_accounts, connect_to_db)\n",
    "df_invoices_42days_all_accounts = pig.run_query(sql_invoices_42days_all_accounts, return_data=True)\n",
    "\n",
    "\n",
    "# Words count in invoice's description, notes, terms, address\n",
    "df_invoices_42days_all_accounts['avg_wc_description_day_42'] = df_invoices_42days_all_accounts.apply(lambda x: words_count(x['description']), axis=1)\n",
    "df_invoices_42days_all_accounts['avg_wc_notes_day_42'] = df_invoices_42days_all_accounts.apply(lambda x: words_count(x['notes']), axis=1)\n",
    "df_invoices_42days_all_accounts['avg_wc_terms_day_42'] = df_invoices_42days_all_accounts.apply(lambda x: words_count(x['terms']), axis=1)\n",
    "df_invoices_42days_all_accounts['avg_wc_address_day_42'] = df_invoices_42days_all_accounts.apply(lambda x: words_count(x['address']), axis=1)\n",
    "\n",
    "                                                                                                                   \n",
    "# Filters the text columns from the dataframe\n",
    "df_invoices_42days_all_accounts_fil = df_invoices_42days_all_accounts.filter(['systemid', \n",
    "                                                                            'invoiceid', \n",
    "                                                                            'signup_date',\n",
    "                                                                            'create_date', \n",
    "                                                                            'created_at',\n",
    "                                                                            'days_to_invoice_creation', \n",
    "                                                                            'avg_wc_description_day_42', \n",
    "                                                                            'avg_wc_notes_day_42', \n",
    "                                                                            'avg_wc_terms_day_42',\n",
    "                                                                            'avg_wc_address_day_42'])  \n",
    "                                                                                                                   \n",
    "# Summing (grouping) all invoices for a 'systemid'\n",
    "df_word_count_42days_all_accounts_total = df_invoices_42days_all_accounts_fil.groupby('systemid').mean()  \n",
    "\n",
    "# Final word count table\n",
    "df_word_count_42days_all_accounts_final = df_word_count_42days_all_accounts_total.filter([\n",
    "                                                                            'systemid',\n",
    "                                                                            'signup_date',\n",
    "                                                                            'avg_wc_description_day_42', \n",
    "                                                                            'avg_wc_notes_day_42', \n",
    "                                                                            'avg_wc_terms_day_42',\n",
    "                                                                            'avg_wc_address_day_42'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_count_42days_all_accounts_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_count_42days_all_accounts_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Report Systems Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################# Import RSystems, Periodic Invoices & Client Counts Data ###############\n",
    "\n",
    "# SQL query \n",
    "sql_rs_invoices_clients_activities_all_accounts = '''WITH periodic_report_system_activities AS (\n",
    "    SELECT\n",
    "        systemid,\n",
    "        signup_date,\n",
    "        admin_email,\n",
    "        is_sales_managed,\n",
    "        is_freshbooks_account_active,\n",
    "        freshbooks_account_status,\n",
    "        is_paying\n",
    "    FROM report_systems rs\n",
    "    JOIN ( SELECT systemid FROM data_science.fraud_accounts_final ) USING (systemid)\n",
    "), invoice_create_date AS (\n",
    "    SELECT\n",
    "           pic.systemid,\n",
    "           inv.invoiceid,\n",
    "           inv.create_date,\n",
    "           inv.created_at,\n",
    "           DATEDIFF(days, pic.signup_date, inv.created_at) AS days_to_invoice_creation\n",
    "    FROM periodic_report_system_activities AS pic\n",
    "    LEFT JOIN coalesced_live_shards.invoice_stable as inv USING (systemid)\n",
    "), invoice_grouping AS (\n",
    "    SELECT\n",
    "           systemid,\n",
    "           COUNT(invoiceid) as invoice_count,\n",
    "           SUM(CASE WHEN days_to_invoice_creation BETWEEN 0 AND 42 THEN 1 ELSE 0 END) AS invoice_count_day_42\n",
    "    FROM invoice_create_date\n",
    "    GROUP BY systemid\n",
    "), client_crate_date AS (\n",
    "     SELECT\n",
    "            pic.systemid,\n",
    "            usr.userid,\n",
    "            usr.signup_date,\n",
    "            DATEDIFF(days, pic.signup_date, usr.signup_date) AS days_to_client_creation\n",
    "    FROM periodic_report_system_activities  AS pic\n",
    "    LEFT JOIN coalesced_live_shards.\"user\" as usr USING (systemid)\n",
    "), client_grouping AS (\n",
    "    SELECT\n",
    "           systemid,\n",
    "           count(userid) AS client_count,\n",
    "           SUM(CASE WHEN days_to_client_creation BETWEEN 0 AND 42 THEN 1 ELSE 0 END) AS client_count_day_42\n",
    "\n",
    "    FROM  client_crate_date\n",
    "    GROUP BY systemid\n",
    ")\n",
    "\n",
    "SELECT\n",
    "       systemid,\n",
    "       signup_date,\n",
    "       current_date as effective_date,\n",
    "       DATEDIFF(days, signup_date, current_date) as days_on_platform,\n",
    "       admin_email,\n",
    "       is_sales_managed,\n",
    "       is_freshbooks_account_active,\n",
    "       is_paying,\n",
    "       inv_gr.invoice_count_day_42,\n",
    "       cl_gr.client_count_day_42\n",
    "FROM periodic_report_system_activities\n",
    "LEFT JOIN invoice_grouping as inv_gr USING (systemid)\n",
    "LEFT JOIN client_grouping AS cl_gr USING (systemid);\n",
    "'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "df_rs_invoices_clients_activities_all_accounts = pig.run_query(sql_rs_invoices_clients_activities_all_accounts, return_data=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking \n",
    "df_rs_invoices_clients_activities_all_accounts.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rs_invoices_clients_activities_all_accounts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Import and Exract Features from Events Data\n",
    "## 4.1 Event data collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Event Features Extraction ################################\n",
    "\n",
    "#SQL for events \n",
    "sql_events = '''WITH selected_accounts_events AS (\n",
    "    SELECT systemid,\n",
    "           signup_date,\n",
    "           signup_datetime\n",
    "    FROM report_systems\n",
    "    JOIN ( SELECT systemid FROM data_science.fraud_accounts_final ) USING (systemid)\n",
    "), events_activities AS (\n",
    "    SELECT sae.systemid,\n",
    "           signup_date,\n",
    "           dd.date,\n",
    "           datediff(days, signup_date, dd.date) as days_to_event,\n",
    "           lower(e.event) as event,\n",
    "           ec.count\n",
    "    FROM selected_accounts_events AS sae\n",
    "    LEFT JOIN event_counts AS ec USING (systemid)\n",
    "    LEFT JOIN d_date AS dd USING (date_key)\n",
    "    LEFT JOIN d_event e on ec.event_key = e.event_key\n",
    "), event_groupings AS (\n",
    "    SELECT distinct  ea.systemid,\n",
    "                    ea.signup_date,\n",
    "                    ea.date,\n",
    "                    ea.event,\n",
    "                    ea.count,\n",
    "                    (CASE WHEN days_to_event BETWEEN 0 AND 42 THEN ea.count END) AS day_42_event\n",
    "    FROM events_activities AS ea\n",
    ")\n",
    "SELECT systemid,\n",
    "       event,\n",
    "       sum(day_42_event) AS event_count_day_42\n",
    "From event_groupings\n",
    "GROUP BY systemid, signup_date, event\n",
    "ORDER BY systemid, event_count_day_42 DESC;'''\n",
    "\n",
    "# Import as dataframe from redshift\n",
    "# df_events_all_accounts = pd.read_sql_query(sql_events, connect_to_db)\n",
    "df_events_all_accounts = pig.run_query(sql_events, return_data=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking\n",
    "df_events_all_accounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_all_accounts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Removing whitespce from the event strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing row if there is 'None' the event cell\n",
    "df_events_all_accounts = df_events_all_accounts[~df_events_all_accounts.astype(str).eq('None').any(1)]\n",
    "\n",
    "# Replace the 'NaN' cell by zero\n",
    "df_events_all_accounts.fillna(0, inplace=True)\n",
    "\n",
    "# Using lambda function to remove the white space in the event string name\n",
    "df_events_all_accounts['event_name'] = df_events_all_accounts.apply(lambda x: x['event'].replace(' ', '').replace('-','').replace('/', ''), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking\n",
    "df_events_all_accounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df_events_all_accounts['event_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered the events columns for day 42\n",
    "df_events_all_accounts_day_42 = df_events_all_accounts[['systemid', 'event_count_day_42', 'event_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_all_accounts_day_42.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_all_accounts_day_42.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Pivote the events (each unique event become a column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pivote the Day 42 Events (Each Unique Event Become a Column)###\n",
    "\n",
    "# Pivot table based on the unique column value in 'event_name'\n",
    "df_events_all_accounts_day_42 = df_events_all_accounts_day_42.pivot_table(values='event_count_day_42', columns='event_name', index='systemid', aggfunc=np.sum,  fill_value=0)\n",
    "\n",
    "# Drop the old column name\n",
    "df_events_all_accounts_day_42.columns.name = None\n",
    "\n",
    "# Reset the index\n",
    "df_events_all_accounts_day_42 = df_events_all_accounts_day_42.reset_index()\n",
    "\n",
    "# Replace 'NaN' with zero\n",
    "df_events_all_accounts_day_42.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking\n",
    "df_events_all_accounts_day_42.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_all_accounts_day_42.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Merging all data: Report system, average word count and event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging report system and events data for day 42 period\n",
    "df_rs_events_day_42 = pd.merge(df_rs_invoices_clients_activities_all_accounts, df_events_all_accounts_day_42,\n",
    "                             on='systemid', how='left')\n",
    "\n",
    "# Merging average word count with 'df_rs_events_day_42'\n",
    "df_rs_events_avg_wc_day_42 = pd.merge(df_rs_events_day_42, df_word_count_42days_all_accounts_final,\n",
    "                                    on='systemid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_rs_events_avg_wc_day_42.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rs_events_avg_wc_day_42['signup_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Filtering out FreshBooks test accounts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Filtering Out FreshBooks Test Accounts #############################################################\n",
    "\n",
    "# Import Freshbooks test accounts email from CSV file (non-freshbooks email)\n",
    "fb_test_emails = pd.read_csv(\n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/data/freshbooks_test_email/non-fb-testing-emails.tsv\", \n",
    "                                      sep=\"\\t\")\n",
    "fb_test_email_list = list(fb_test_emails['email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fb_test_email_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Filtering FB test account by using admin email\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def email_match(em, email_list):\n",
    "    \n",
    "    L = len(email_list)\n",
    "#     print('L', L)\n",
    "#     print('em-before-loop: ', em)\n",
    "    match_score = 0\n",
    "#     x = float(em)\n",
    "    \n",
    "    for i in range(0, L):\n",
    "#         if math.isnan(x):\n",
    "#             match_score = 0\n",
    "#             break;\n",
    "        if pd.isnull(em):\n",
    "            match_score = 0\n",
    "            break;\n",
    "        else: \n",
    "            match_score =  max(match_score, SequenceMatcher(None,em, email_list[i]).ratio())\n",
    "#             print(i, em, email_list[i], match_score)\n",
    "\n",
    "    return match_score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering final data from the FreshBooks Test emails\n",
    "df_rs_events_avg_wc_day_42_noFBtest = df_rs_events_avg_wc_day_42[\n",
    "    df_rs_events_avg_wc_day_42.apply(lambda x: email_match(x['admin_email'], fb_test_email_list) < 0.95, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rs_events_avg_wc_day_42_noFBtest['signup_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df_rs_events_avg_wc_day_42_noFBtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Filtering only important features: Day 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing importing features list\n",
    "important_features = pd.read_csv( \n",
    "    \"/Users/dwahid/Documents/GitHub/fraud_detection/src/important_features/important_features_day_42_new_accounts.tsv\", sep=\"\\n,\")\n",
    "\n",
    "# Get the important feature as a list\n",
    "imp_features_list = list(important_features['important_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imp_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding missing important feature column with zero values (if there any!)\n",
    "for i in range(len(imp_features_list)):\n",
    "    if imp_features_list[i] in df_rs_events_avg_wc_day_42_noFBtest.columns:\n",
    "#         print(\"True\")\n",
    "        continue;\n",
    "        \n",
    "    else:\n",
    "        print(\"False: \", imp_features_list[i])\n",
    "        df_rs_events_avg_wc_day_42_noFBtest[imp_features_list[i]] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rs_events_avg_wc_day_42_noFBtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df_rs_events_avg_wc_day_42_noFBtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rs_events_avg_wc_day_42_noFBtest['signup_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering only important features \n",
    "df_imp_features_new_accounts_day_42 =\\\n",
    "            df_rs_events_avg_wc_day_42_noFBtest[df_rs_events_avg_wc_day_42_noFBtest.columns.intersection(imp_features_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp_features_new_accounts_day_42.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp_features_new_accounts_day_42 = df_imp_features_new_accounts_day_42.reindex(\n",
    "    sorted(df_imp_features_new_accounts_day_42.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df_final_features_new_accounts_day_42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_imp_features_new_accounts_day_42['signup_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with nan value\n",
    "df_imp_features_new_accounts_day_42 = df_imp_features_new_accounts_day_42.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Filtering inactive users' accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_cols_list = ['admin_email','days_on_platform', 'effective_date', 'signup_date', 'systemid']\n",
    "cols_list = list(df_imp_features_new_accounts_day_42) \n",
    "cols = list(set(cols_list) - set(ex_cols_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for aggregating selected column values\n",
    "def cell_value_sum (row, cols):\n",
    "    #print(row)\n",
    "    sum = 0\n",
    "    for i in cols:\n",
    "        #print(i)\n",
    "        #print(i, row[i])\n",
    "        sum = sum + row[i]\n",
    "    \n",
    "    #print('Final sum: ', sum)\n",
    "    return sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fltering out all inactive users accounts\n",
    "df_final_features_new_accounts_day_42 =\\\n",
    "        df_imp_features_new_accounts_day_42[df_imp_features_new_accounts_day_42.apply(lambda x: cell_value_sum(x, cols) > 0, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_features_new_accounts_day_42.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final_features_new_accounts_day_342['signup_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Saving the filtered features data for new accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export filtered features data fro new accounts\n",
    "today = str(date.today())\n",
    "path = \"/Users/dwahid/Documents/GitHub/fraud_detection/data/analyzing_fraud_accounts/fraud_accounts_features_day_42.tsv\"\n",
    "df_final_features_new_accounts_day_42.to_csv(path, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_features_new_accounts_day_42.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
